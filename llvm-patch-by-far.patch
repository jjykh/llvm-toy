Index: include/llvm/ADT/Triple.h
===================================================================
--- include/llvm/ADT/Triple.h	(revision 365284)
+++ include/llvm/ADT/Triple.h	(working copy)
@@ -211,6 +211,7 @@
     Cygnus,
     CoreCLR,
     Simulator,  // Simulator variants of other systems, e.g., Apple's iOS
+    V8,
     LastEnvironmentType = Simulator
   };
   enum ObjectFormatType {
Index: include/llvm/CodeGen/MachineRegisterInfo.h
===================================================================
--- include/llvm/CodeGen/MachineRegisterInfo.h	(revision 365284)
+++ include/llvm/CodeGen/MachineRegisterInfo.h	(working copy)
@@ -933,6 +933,10 @@
   /// corresponding live-in physical register.
   unsigned getLiveInVirtReg(unsigned PReg) const;
 
+  /// updateVirtRegIfLivein - If a VReg is a live-in virtual register, update
+  /// it to VNewReg.
+  void updateVirtRegIfLivein(unsigned VReg, unsigned VNewReg);
+
   /// EmitLiveInCopies - Emit copies to initialize livein virtual registers
   /// into the given entry block.
   void EmitLiveInCopies(MachineBasicBlock *EntryMBB,
Index: include/llvm/CodeGen/TargetRegisterInfo.h
===================================================================
--- include/llvm/CodeGen/TargetRegisterInfo.h	(revision 365284)
+++ include/llvm/CodeGen/TargetRegisterInfo.h	(working copy)
@@ -968,6 +968,13 @@
     return StringRef(getName(Reg));
   }
 
+  using VirtRegToFixSlotMap = std::vector<std::pair<unsigned, int>>;
+
+  virtual VirtRegToFixSlotMap
+  getHoistToFixStackSlotMap(MachineFunction &) const {
+    return VirtRegToFixSlotMap();
+  }
+
   //===--------------------------------------------------------------------===//
   /// Subtarget Hooks
 
Index: include/llvm/IR/CallingConv.h
===================================================================
--- include/llvm/IR/CallingConv.h	(revision 365284)
+++ include/llvm/IR/CallingConv.h	(working copy)
@@ -220,8 +220,16 @@
     /// shader if tessellation is in use, or otherwise the vertex shader.
     AMDGPU_ES = 96,
 
+    // Calling convention for v8
+    V8CC = 97,
+
+    // Calling convention for v8 store barrier stub
+    V8SBCC = 98,
+    // Calling convention for v8 C call with fp saved
+    V8FPSave = 99,
+
     // Calling convention between AArch64 Advanced SIMD functions
-    AArch64_VectorCall = 97,
+    AArch64_VectorCall = 100,
 
     /// The highest possible calling convention ID. Must be some 2^k - 1.
     MaxID = 1023
Index: include/llvm/IR/Intrinsics.td
===================================================================
--- include/llvm/IR/Intrinsics.td	(revision 365284)
+++ include/llvm/IR/Intrinsics.td	(working copy)
@@ -907,6 +907,11 @@
 def int_experimental_gc_relocate : Intrinsic<[llvm_any_ty],
                                 [llvm_token_ty, llvm_i32_ty, llvm_i32_ty],
                                 [IntrReadMem]>;
+def int_experimental_gc_exception : Intrinsic<[llvm_any_ty], [llvm_token_ty],
+                                             [IntrReadMem]>;
+def int_experimental_gc_exception_data : Intrinsic<[llvm_any_ty],
+                                             [llvm_token_ty],
+                                             [IntrReadMem]>;
 
 //===------------------------ Coroutine Intrinsics ---------------===//
 // These are documented in docs/Coroutines.rst
Index: include/llvm/IR/Statepoint.h
===================================================================
--- include/llvm/IR/Statepoint.h	(revision 365284)
+++ include/llvm/IR/Statepoint.h	(working copy)
@@ -331,7 +331,9 @@
 public:
   static bool classof(const IntrinsicInst *I) {
     return I->getIntrinsicID() == Intrinsic::experimental_gc_relocate ||
-      I->getIntrinsicID() == Intrinsic::experimental_gc_result;
+           I->getIntrinsicID() == Intrinsic::experimental_gc_result ||
+           I->getIntrinsicID() == Intrinsic::experimental_gc_exception ||
+           I->getIntrinsicID() == Intrinsic::experimental_gc_exception_data;
   }
 
   static bool classof(const Value *V) {
@@ -417,6 +419,28 @@
   }
 };
 
+class GCExceptionInst : public GCProjectionInst {
+public:
+  static bool classof(const IntrinsicInst *I) {
+    return I->getIntrinsicID() == Intrinsic::experimental_gc_exception;
+  }
+
+  static bool classof(const Value *V) {
+    return isa<IntrinsicInst>(V) && classof(cast<IntrinsicInst>(V));
+  }
+};
+
+class GCExceptionDataInst : public GCProjectionInst {
+public:
+  static bool classof(const IntrinsicInst *I) {
+    return I->getIntrinsicID() == Intrinsic::experimental_gc_exception_data;
+  }
+
+  static bool classof(const Value *V) {
+    return isa<IntrinsicInst>(V) && classof(cast<IntrinsicInst>(V));
+  }
+};
+
 template <typename FunTy, typename InstructionTy, typename ValueTy,
           typename CallSiteTy>
 std::vector<const GCRelocateInst *>
Index: include/llvm/Support/TargetOpcodes.def
===================================================================
--- include/llvm/Support/TargetOpcodes.def	(revision 365284)
+++ include/llvm/Support/TargetOpcodes.def	(working copy)
@@ -120,6 +120,7 @@
 /// rewrite calls to runtimes with more efficient code sequences.
 /// This also implies a stack map.
 HANDLE_TARGET_OPCODE(PATCHPOINT)
+HANDLE_TARGET_OPCODE(TCPATCHPOINT)
 
 /// This pseudo-instruction loads the stack guard value. Targets which need
 /// to prevent the stack guard value or address from being spilled to the
@@ -133,6 +134,13 @@
 /// collectors and deoptimizations in either the callee or caller.
 HANDLE_TARGET_OPCODE(STATEPOINT)
 
+/// Define a new register from old register, used by gc relocate intrinsic.
+/// It is intended to cause a value to split in MIR.
+HANDLE_TARGET_OPCODE(RELOCATE_DEF)
+
+/// Restore SP from FP, used by V8 functions.
+HANDLE_TARGET_OPCODE(RESTORESP)
+
 /// Instruction that records the offset of a local stack allocation passed to
 /// llvm.localescape. It has two arguments: the symbol for the label and the
 /// frame index of the local stack allocation.
Index: include/llvm/Target/Target.td
===================================================================
--- include/llvm/Target/Target.td	(revision 365284)
+++ include/llvm/Target/Target.td	(working copy)
@@ -1068,6 +1068,17 @@
   let mayLoad = 1;
   let usesCustomInserter = 1;
 }
+def TCPATCHPOINT : StandardPseudoInstruction {
+  let OutOperandList = (outs);
+  let InOperandList = (ins i64imm:$id, i32imm:$nbytes, unknown:$callee,
+                       i32imm:$nargs, i32imm:$cc, variable_ops);
+  let hasSideEffects = 1;
+  let isCall = 1;
+  let mayLoad = 1;
+  let usesCustomInserter = 1;
+  let isReturn = 1;
+  let isTerminator = 1;
+}
 def STATEPOINT : StandardPseudoInstruction {
   let OutOperandList = (outs);
   let InOperandList = (ins variable_ops);
@@ -1077,6 +1088,20 @@
   let hasSideEffects = 1;
   let isCall = 1;
 }
+def RELOCATE_DEF: StandardPseudoInstruction {
+  let OutOperandList = (outs unknown:$dst);
+  let InOperandList = (ins unknown:$src);
+  let AsmString = "";
+  let hasSideEffects = 0;
+  let isAsCheapAsAMove = 1;
+}
+def RESTORESP: StandardPseudoInstruction {
+  let OutOperandList = (outs);
+  let InOperandList = (ins);
+  let AsmString = "";
+  let hasSideEffects = 0;
+  let mayLoad = 1;
+}
 def LOAD_STACK_GUARD : StandardPseudoInstruction {
   let OutOperandList = (outs ptr_rc:$dst);
   let InOperandList = (ins);
Index: include/llvm/Transforms/IPO/SampleProfile.h
===================================================================
--- include/llvm/Transforms/IPO/SampleProfile.h	(revision 365284)
+++ include/llvm/Transforms/IPO/SampleProfile.h	(working copy)
@@ -31,6 +31,7 @@
         IsThinLTOPreLink(IsThinLTOPreLink) {}
 
   PreservedAnalyses run(Module &M, ModuleAnalysisManager &AM);
+  static std::string SampleProfileFileFromOption();
 
 private:
   std::string ProfileFileName;
Index: include/llvm-c/Core.h
===================================================================
--- include/llvm-c/Core.h	(revision 365284)
+++ include/llvm-c/Core.h	(working copy)
@@ -245,7 +245,9 @@
   LLVMAMDGPUHSCallConv      = 93,
   LLVMMSP430BUILTINCallConv = 94,
   LLVMAMDGPULSCallConv      = 95,
-  LLVMAMDGPUESCallConv      = 96
+  LLVMAMDGPUESCallConv      = 96,
+  LLVMV8CallConv            = 97,
+  LLVMV8SBCallConv          = 98
 } LLVMCallConv;
 
 typedef enum {
Index: include/llvm-c/Target.h
===================================================================
--- include/llvm-c/Target.h	(revision 365284)
+++ include/llvm-c/Target.h	(working copy)
@@ -211,6 +211,9 @@
 void LLVMAddTargetLibraryInfo(LLVMTargetLibraryInfoRef TLI,
                               LLVMPassManagerRef PM);
 
+/** Create empty TargetLibraryInfoImpl, with all functions disabled */
+LLVMTargetLibraryInfoRef LLVMCreateEmptyTargetLibraryInfo();
+
 /** Converts target data to a target layout string. The string must be disposed
     with LLVMDisposeMessage.
     See the constructor llvm::DataLayout::DataLayout. */
Index: lib/Analysis/BranchProbabilityInfo.cpp
===================================================================
--- lib/Analysis/BranchProbabilityInfo.cpp	(revision 365284)
+++ lib/Analysis/BranchProbabilityInfo.cpp	(working copy)
@@ -302,7 +302,13 @@
   assert(WeightSum <= UINT32_MAX &&
          "Expected weights to scale down to 32 bits");
 
-  if (WeightSum == 0 || ReachableIdxs.size() == 0) {
+  bool NoUnreachableHeuristic = false;
+  CallingConv::ID CC = BB->getParent()->getCallingConv();
+  if ((CC == CallingConv::V8CC) || (CC == CallingConv::V8SBCC))
+    NoUnreachableHeuristic = true;
+
+  if (!NoUnreachableHeuristic &&
+      (WeightSum == 0 || ReachableIdxs.size() == 0)) {
     for (unsigned i = 0, e = TI->getNumSuccessors(); i != e; ++i)
       Weights[i] = 1;
     WeightSum = TI->getNumSuccessors();
@@ -315,7 +321,8 @@
 
   // Examine the metadata against unreachable heuristic.
   // If the unreachable heuristic is more strong then we use it for this edge.
-  if (UnreachableIdxs.size() > 0 && ReachableIdxs.size() > 0) {
+  if (!NoUnreachableHeuristic &&
+      (UnreachableIdxs.size() > 0 && ReachableIdxs.size() > 0)) {
     auto ToDistribute = BranchProbability::getZero();
     auto UnreachableProb = UR_TAKEN_PROB;
     for (auto i : UnreachableIdxs)
Index: lib/Analysis/ScalarEvolution.cpp
===================================================================
--- lib/Analysis/ScalarEvolution.cpp	(revision 365284)
+++ lib/Analysis/ScalarEvolution.cpp	(working copy)
@@ -6455,6 +6455,7 @@
     }
     return getSignExtendExpr(getSCEV(U->getOperand(0)), U->getType());
 
+  case Instruction::AddrSpaceCast:
   case Instruction::BitCast:
     // BitCasts are no-op casts so we just eliminate the cast.
     if (isSCEVable(U->getType()) && isSCEVable(U->getOperand(0)->getType()))
Index: lib/Analysis/ScalarEvolutionExpander.cpp
===================================================================
--- lib/Analysis/ScalarEvolutionExpander.cpp	(revision 365284)
+++ lib/Analysis/ScalarEvolutionExpander.cpp	(working copy)
@@ -112,6 +112,7 @@
 Value *SCEVExpander::InsertNoopCastOfTo(Value *V, Type *Ty) {
   Instruction::CastOps Op = CastInst::getCastOpcode(V, false, Ty, false);
   assert((Op == Instruction::BitCast ||
+          Op == Instruction::AddrSpaceCast ||
           Op == Instruction::PtrToInt ||
           Op == Instruction::IntToPtr) &&
          "InsertNoopCastOfTo cannot perform non-noop casts!");
Index: lib/AsmParser/LLLexer.cpp
===================================================================
--- lib/AsmParser/LLLexer.cpp	(revision 365284)
+++ lib/AsmParser/LLLexer.cpp	(working copy)
@@ -607,6 +607,8 @@
   KEYWORD(webkit_jscc);
   KEYWORD(swiftcc);
   KEYWORD(anyregcc);
+  KEYWORD(v8cc);
+  KEYWORD(v8sbcc);
   KEYWORD(preserve_mostcc);
   KEYWORD(preserve_allcc);
   KEYWORD(ghccc);
Index: lib/AsmParser/LLParser.cpp
===================================================================
--- lib/AsmParser/LLParser.cpp	(revision 365284)
+++ lib/AsmParser/LLParser.cpp	(working copy)
@@ -1931,6 +1931,12 @@
   case lltok::kw_win64cc:        CC = CallingConv::Win64; break;
   case lltok::kw_webkit_jscc:    CC = CallingConv::WebKit_JS; break;
   case lltok::kw_anyregcc:       CC = CallingConv::AnyReg; break;
+  case lltok::kw_v8cc:
+    CC = CallingConv::V8CC;
+    break;
+  case lltok::kw_v8sbcc:
+    CC = CallingConv::V8SBCC;
+    break;
   case lltok::kw_preserve_mostcc:CC = CallingConv::PreserveMost; break;
   case lltok::kw_preserve_allcc: CC = CallingConv::PreserveAll; break;
   case lltok::kw_ghccc:          CC = CallingConv::GHC; break;
Index: lib/AsmParser/LLToken.h
===================================================================
--- lib/AsmParser/LLToken.h	(revision 365284)
+++ lib/AsmParser/LLToken.h	(working copy)
@@ -151,6 +151,8 @@
   kw_win64cc,
   kw_webkit_jscc,
   kw_anyregcc,
+  kw_v8cc,
+  kw_v8sbcc,
   kw_swiftcc,
   kw_preserve_mostcc,
   kw_preserve_allcc,
Index: lib/CodeGen/InlineSpiller.cpp
===================================================================
--- lib/CodeGen/InlineSpiller.cpp	(revision 365284)
+++ lib/CodeGen/InlineSpiller.cpp	(working copy)
@@ -57,6 +57,7 @@
 #include <cassert>
 #include <iterator>
 #include <tuple>
+#include <unordered_set>
 #include <utility>
 #include <vector>
 
@@ -213,6 +214,10 @@
   bool isSibling(unsigned Reg);
   bool hoistSpillInsideBB(LiveInterval &SpillLI, MachineInstr &CopyMI);
   void eliminateRedundantSpills(LiveInterval &LI, VNInfo *VNI);
+  /// Fold all the regs that related to InputReg and live through a
+  /// statepoint.
+  void foldStatePoints(unsigned Reg);
+  void removeRegFromStatePoint(MachineInstr *MI, unsigned Reg);
 
   void markValueUsed(LiveInterval*, VNInfo*);
   bool reMaterializeFor(LiveInterval &, MachineInstr &MI);
@@ -552,6 +557,11 @@
     return false;
   }
 
+  if (MI.getOpcode() == TargetOpcode::STATEPOINT) {
+    removeRegFromStatePoint(&MI, VirtReg.reg);
+    return true;
+  }
+
   // If the instruction also writes VirtReg.reg, it had better not require the
   // same register for uses and defs.
   if (RI.Tied) {
@@ -924,11 +934,85 @@
     HSpiller.addToMergeableSpills(*std::next(MI), StackSlot, Original);
 }
 
+static bool CheckIdxInsideLI(SlotIndex Idx, const LiveInterval &LI) {
+  auto Segment = LI.FindSegmentContaining(Idx.getPrevSlot());
+  if (LI.end() == Segment) {
+    return false;
+  }
+  return true;
+}
+
+void InlineSpiller::foldStatePoints(unsigned InputReg) {
+  SmallVector<unsigned, 8> WorkList;
+  std::unordered_set<unsigned> VisitedSet;
+  // Don't add Reg to WorkList, spillAroundUses will handle it.
+  for (MachineRegisterInfo::reg_bundle_iterator
+           RegI = MRI.reg_bundle_begin(InputReg),
+           E = MRI.reg_bundle_end();
+       RegI != E;) {
+    MachineInstr &MI = *RegI++;
+    unsigned SibReg = isFullCopyOf(MI, InputReg);
+    if (SibReg && isSibling(SibReg) && SibReg != InputReg) {
+      WorkList.push_back(SibReg);
+      VisitedSet.emplace(SibReg);
+    }
+  }
+  VisitedSet.emplace(InputReg);
+  while (!WorkList.empty()) {
+    unsigned Reg = WorkList.pop_back_val();
+    // Fold the use of state point.
+    for (MachineRegisterInfo::use_instr_nodbg_iterator
+             UI = MRI.use_instr_nodbg_begin(Reg),
+             E = MRI.use_instr_nodbg_end();
+         UI != E;) {
+      MachineInstr &MI = *UI++;
+      if (unsigned DstReg = isFullCopyOf(MI, Reg)) {
+        if (isSibling(DstReg)) {
+          auto pair = VisitedSet.emplace(DstReg);
+          if (pair.second)
+            WorkList.push_back(DstReg);
+        }
+        continue;
+      }
+      // Handle STATEPOINT, PATCHPOINT
+      switch (MI.getOpcode()) {
+      case TargetOpcode::STATEPOINT:
+      case TargetOpcode::PATCHPOINT: {
+        // Ignore if the input reg's live range does not contain
+        // the MI.
+        SlotIndex Idx = LIS.getInstructionIndex(MI).getRegSlot();
+        // Check both to ensure the case that reloads for STATEPOINT.
+        if (!CheckIdxInsideLI(Idx, *StackInt)) {
+          continue;
+        }
+        // Analyze instruction.
+        SmallVector<std::pair<MachineInstr *, unsigned>, 8> Ops;
+        MIBundleOperands(MI).analyzeVirtReg(Reg, &Ops);
+        foldMemoryOperand(Ops);
+      } break;
+      default:
+        break;
+      }
+    }
+  }
+}
+
+void InlineSpiller::removeRegFromStatePoint(MachineInstr *MI, unsigned Reg) {
+  for (unsigned i = MI->getNumOperands(); i; --i) {
+    MachineOperand &MO = MI->getOperand(i - 1);
+    if (!MO.isReg())
+      continue;
+    if (MO.getReg() == Reg)
+      MI->RemoveOperand(i - 1);
+  }
+}
+
 /// spillAroundUses - insert spill code around each use of Reg.
 void InlineSpiller::spillAroundUses(unsigned Reg) {
   LLVM_DEBUG(dbgs() << "spillAroundUses " << printReg(Reg) << '\n');
   LiveInterval &OldLI = LIS.getInterval(Reg);
 
+  foldStatePoints(Reg);
   // Iterate over instructions using Reg.
   for (MachineRegisterInfo::reg_bundle_iterator
        RegI = MRI.reg_bundle_begin(Reg), E = MRI.reg_bundle_end();
Index: lib/CodeGen/LocalStackSlotAllocation.cpp
===================================================================
--- lib/CodeGen/LocalStackSlotAllocation.cpp	(revision 365284)
+++ lib/CodeGen/LocalStackSlotAllocation.cpp	(working copy)
@@ -299,6 +299,7 @@
       // range, so they don't need any updates.
       if (MI.isDebugInstr() || MI.getOpcode() == TargetOpcode::STATEPOINT ||
           MI.getOpcode() == TargetOpcode::STACKMAP ||
+          MI.getOpcode() == TargetOpcode::TCPATCHPOINT ||
           MI.getOpcode() == TargetOpcode::PATCHPOINT)
         continue;
 
Index: lib/CodeGen/MachineInstr.cpp
===================================================================
--- lib/CodeGen/MachineInstr.cpp	(revision 365284)
+++ lib/CodeGen/MachineInstr.cpp	(working copy)
@@ -1304,11 +1304,18 @@
 
   const MachineFrameInfo &MFI = getParent()->getParent()->getFrameInfo();
 
+  bool IsJSFunction = false;
+  CallingConv::ID CC = getParent()->getParent()->getFunction().getCallingConv();
+  if (CC == CallingConv::V8CC || CC == CallingConv::V8SBCC)
+    IsJSFunction = true;
+
   for (MachineMemOperand *MMO : memoperands()) {
     if (MMO->isVolatile()) return false;
     if (MMO->isStore()) return false;
     if (MMO->isInvariant() && MMO->isDereferenceable())
       continue;
+    if (IsJSFunction && MMO->isInvariant())
+      continue;
 
     // A load from a constant PseudoSourceValue is invariant.
     if (const PseudoSourceValue *PSV = MMO->getPseudoValue())
Index: lib/CodeGen/MachineRegisterInfo.cpp
===================================================================
--- lib/CodeGen/MachineRegisterInfo.cpp	(revision 365284)
+++ lib/CodeGen/MachineRegisterInfo.cpp	(working copy)
@@ -458,6 +458,14 @@
   return 0;
 }
 
+void MachineRegisterInfo::updateVirtRegIfLivein(unsigned VReg,
+                                                unsigned VNewReg) {
+  for (auto I = LiveIns.begin(), E = LiveIns.end(); I != E; ++I)
+    if (I->second == VReg) {
+      I->second = VNewReg;
+    }
+}
+
 /// EmitLiveInCopies - Emit copies to initialize livein virtual registers
 /// into the given entry block.
 void
Index: lib/CodeGen/MachineVerifier.cpp
===================================================================
--- lib/CodeGen/MachineVerifier.cpp	(revision 365284)
+++ lib/CodeGen/MachineVerifier.cpp	(working copy)
@@ -1203,6 +1203,8 @@
   unsigned NumDefs = MCID.getNumDefs();
   if (MCID.getOpcode() == TargetOpcode::PATCHPOINT)
     NumDefs = (MONum == 0 && MO->isReg()) ? NumDefs : 0;
+  else if (MCID.getOpcode() == TargetOpcode::TCPATCHPOINT)
+    NumDefs = 0;
 
   // The first MCID.NumDefs operands must be explicit register defines
   if (MONum < NumDefs) {
Index: lib/CodeGen/RegAllocBase.cpp
===================================================================
--- lib/CodeGen/RegAllocBase.cpp	(revision 365284)
+++ lib/CodeGen/RegAllocBase.cpp	(working copy)
@@ -30,6 +30,7 @@
 #include "llvm/Support/Timer.h"
 #include "llvm/Support/raw_ostream.h"
 #include <cassert>
+#include <unordered_map>
 
 using namespace llvm;
 
@@ -166,4 +167,39 @@
     DeadInst->eraseFromParent();
   }
   DeadRemats.clear();
+  tryHoistToStackSlot();
 }
+
+void RegAllocBase::tryHoistToStackSlot() {
+  MachineFunction &MF = VRM->getMachineFunction();
+  auto RegMapping = TRI->getHoistToFixStackSlotMap(MF);
+  if (RegMapping.empty())
+    return;
+  std::unordered_map<int, int> SlotMapping;
+  for (auto &Entry : RegMapping) {
+    int Slot = VRM->getStackSlot(Entry.first);
+    if (Slot == VirtRegMap::NO_STACK_SLOT)
+      continue;
+    SlotMapping.emplace(Slot, Entry.second);
+  }
+  if (SlotMapping.empty())
+    return;
+  for (MachineBasicBlock &MBB : MF) {
+    for (MachineInstr &MI : MBB)
+      for (unsigned i = 0, ee = MI.getNumOperands(); i != ee; ++i) {
+        MachineOperand &MO = MI.getOperand(i);
+        if (!MO.isFI())
+          continue;
+        int OldFI = MO.getIndex();
+        if (OldFI < 0)
+          continue;
+        auto FoundFI = SlotMapping.find(OldFI);
+        if (FoundFI == SlotMapping.end())
+          continue;
+        int NewFI = FoundFI->second;
+        if (NewFI == OldFI)
+          continue;
+        MO.setIndex(NewFI);
+      }
+  }
+}
Index: lib/CodeGen/RegAllocBase.h
===================================================================
--- lib/CodeGen/RegAllocBase.h	(revision 365284)
+++ lib/CodeGen/RegAllocBase.h	(working copy)
@@ -118,6 +118,7 @@
 
 private:
   void seedLiveRegs();
+  void tryHoistToStackSlot();
 };
 
 } // end namespace llvm
Index: lib/CodeGen/RegisterCoalescer.cpp
===================================================================
--- lib/CodeGen/RegisterCoalescer.cpp	(revision 365284)
+++ lib/CodeGen/RegisterCoalescer.cpp	(working copy)
@@ -36,6 +36,7 @@
 #include "llvm/CodeGen/Passes.h"
 #include "llvm/CodeGen/RegisterClassInfo.h"
 #include "llvm/CodeGen/SlotIndexes.h"
+#include "llvm/CodeGen/StackMaps.h"
 #include "llvm/CodeGen/TargetInstrInfo.h"
 #include "llvm/CodeGen/TargetOpcodes.h"
 #include "llvm/CodeGen/TargetRegisterInfo.h"
@@ -57,6 +58,7 @@
 #include <tuple>
 #include <utility>
 #include <vector>
+#include <unordered_set>
 
 using namespace llvm;
 
@@ -165,6 +167,13 @@
     /// Join compatible live intervals
     void joinAllIntervals();
 
+    /// Remove the redundant move immediate from statepoints.
+    void removeMoveImmediateFromPatchpoint();
+
+    void removeMoveImmediateFromPatchpoint(MachineInstr *MI);
+
+    void foldRelocateDef();
+
     /// Coalesce copies in the specified MBB, putting
     /// copies that cannot yet be coalesced into WorkList.
     void copyCoalesceInMBB(MachineBasicBlock *MBB);
@@ -1929,6 +1938,8 @@
   // Update regalloc hint.
   TRI->updateRegAllocHint(CP.getSrcReg(), CP.getDstReg(), *MF);
 
+  MRI->updateVirtRegIfLivein(CP.getSrcReg(), CP.getDstReg());
+
   LLVM_DEBUG({
     dbgs() << "\tSuccess: " << printReg(CP.getSrcReg(), TRI, CP.getSrcIdx())
            << " -> " << printReg(CP.getDstReg(), TRI, CP.getDstIdx()) << '\n';
@@ -3615,6 +3626,122 @@
   lateLiveIntervalUpdate();
 }
 
+void RegisterCoalescer::removeMoveImmediateFromPatchpoint() {
+  SmallVector<MachineInstr *, 8> WorkList;
+  for (MachineFunction::iterator I = MF->begin(), E = MF->end(); I != E; ++I) {
+    MachineBasicBlock *MBB = &*I;
+    for (MachineInstr &MI : *MBB) {
+      switch (MI.getOpcode()) {
+      case TargetOpcode::STACKMAP:
+      case TargetOpcode::PATCHPOINT:
+      case TargetOpcode::TCPATCHPOINT:
+      case TargetOpcode::STATEPOINT:
+        WorkList.emplace_back(&MI);
+        break;
+      }
+    }
+  }
+  for (MachineInstr *MI : WorkList)
+    removeMoveImmediateFromPatchpoint(MI);
+}
+
+static const MachineInstr *getSingleDef(const MachineRegisterInfo &MRI,
+                                        unsigned Reg) {
+  auto defs_iterator = MRI.def_begin(Reg);
+  auto defs_end = MRI.def_end();
+  int count = 0;
+  MachineInstr *MI = nullptr;
+  for (; defs_iterator != defs_end; ++defs_iterator, ++count) {
+    if (count != 0)
+      return nullptr;
+    MI = defs_iterator->getParent();
+  }
+  return MI;
+}
+
+static bool shouldFoldAsConstant(const MachineOperand &MO,
+                                 const MachineRegisterInfo &MRI, int64_t *imm) {
+  if (!MO.isReg())
+    return false;
+  const MachineOperand *Target = &MO;
+  std::unordered_set<unsigned> Visited;
+  while (true) {
+    auto InsertResult = Visited.emplace(Target->getReg());
+    // Avoid loop.
+    if (!InsertResult.second)
+      return false;
+    const MachineInstr *MI = getSingleDef(MRI, Target->getReg());
+    if (!MI)
+      return false;
+    if (MI->isFullCopy() &&
+        TargetRegisterInfo::isVirtualRegister(MI->getOperand(1).getReg())) {
+      Target = &MI->getOperand(1);
+      continue;
+    }
+    if (!MI->isMoveImmediate())
+      return false;
+    const MachineOperand &ImmOperand = MI->getOperand(1);
+    assert(ImmOperand.isImm());
+    *imm = ImmOperand.getImm();
+    return true;
+  }
+}
+
+void RegisterCoalescer::removeMoveImmediateFromPatchpoint(
+    MachineInstr *PatchPoint) {
+  unsigned StartIdx = 0;
+  switch (PatchPoint->getOpcode()) {
+  case TargetOpcode::STACKMAP: {
+    StartIdx = StackMapOpers(PatchPoint).getVarIdx();
+    break;
+  }
+  case TargetOpcode::TCPATCHPOINT:
+  case TargetOpcode::PATCHPOINT: {
+    StartIdx = PatchPointOpers(PatchPoint).getVarIdx();
+    break;
+  }
+  case TargetOpcode::STATEPOINT: {
+    StartIdx = StatepointOpers(PatchPoint).getVarIdx();
+    break;
+  }
+  default:
+    llvm_unreachable("unexpected stackmap opcode");
+  }
+
+  MachineInstr *NewMI = MF->CreateMachineInstr(
+      TII->get(PatchPoint->getOpcode()), PatchPoint->getDebugLoc(), true);
+  MachineInstrBuilder MIB(*MF, NewMI);
+
+  // No need to fold return, the meta data, and function arguments
+  for (unsigned i = 0; i < StartIdx; ++i)
+    MIB.add(PatchPoint->getOperand(i));
+  for (unsigned i = StartIdx; i < PatchPoint->getNumOperands(); ++i) {
+    MachineOperand &MO = PatchPoint->getOperand(i);
+    int64_t Imm;
+    if (!shouldFoldAsConstant(MO, *MRI, &Imm)) {
+      MIB.add(MO);
+      continue;
+    }
+    MIB.addImm(StackMaps::ConstantOp);
+    MIB.addImm(Imm);
+  }
+  MachineBasicBlock *MBB = PatchPoint->getParent();
+  LIS->ReplaceMachineInstrInMaps(*PatchPoint, *NewMI);
+  MachineBasicBlock::iterator Pos = PatchPoint;
+  MBB->insert(Pos, NewMI);
+  MBB->erase(PatchPoint);
+}
+
+void RegisterCoalescer::foldRelocateDef() {
+  for (MachineFunction::iterator I = MF->begin(), E = MF->end(); I != E; ++I) {
+    MachineBasicBlock *MBB = &*I;
+    for (MachineInstr &MI : *MBB) {
+      if (MI.getOpcode() == TargetOpcode::RELOCATE_DEF)
+        MI.setDesc(TII->get(TargetOpcode::COPY));
+    }
+  }
+}
+
 void RegisterCoalescer::releaseMemory() {
   ErasedInstrs.clear();
   WorkList.clear();
@@ -3649,6 +3776,8 @@
 
   RegClassInfo.runOnMachineFunction(fn);
 
+  removeMoveImmediateFromPatchpoint();
+  foldRelocateDef();
   // Join (coalesce) intervals if requested.
   if (EnableJoining)
     joinAllIntervals();
@@ -3690,6 +3819,35 @@
     }
   }
 
+  // Fold all the reg with single def with others are phi-def.
+  for (unsigned i = 0, e = MRI->getNumVirtRegs(); i != e; ++i) {
+    unsigned Reg = TargetRegisterInfo::index2VirtReg(i);
+    if (MRI->reg_nodbg_empty(Reg))
+      continue;
+    if (!LIS->hasInterval(Reg))
+      continue;
+    LiveInterval &Interval = LIS->getInterval(Reg);
+    size_t defnum = 0;
+    VNInfo *LastDef = nullptr;
+    for (VNInfo *info : Interval.valnos) {
+      if (!info->isUnused() && !info->isPHIDef()) {
+        defnum++;
+        LastDef = info;
+      }
+    }
+    if (defnum == 1) {
+      LLVM_DEBUG(dbgs() << "Found one def reg: " << Interval << "\n");
+      for (auto &s : Interval) {
+        s.valno = LastDef;
+      }
+      for (VNInfo *info : Interval.valnos) {
+        if (info != LastDef && !info->isUnused())
+          info->markUnused();
+      }
+      Interval.RenumberValues();
+    }
+  }
+
   LLVM_DEBUG(dump());
   if (VerifyCoalescing)
     MF->verify(this, "After register coalescing");
Index: lib/CodeGen/SelectionDAG/InstrEmitter.cpp
===================================================================
--- lib/CodeGen/SelectionDAG/InstrEmitter.cpp	(revision 365284)
+++ lib/CodeGen/SelectionDAG/InstrEmitter.cpp	(working copy)
@@ -832,7 +832,8 @@
   const MCPhysReg *ScratchRegs = nullptr;
 
   // Handle STACKMAP and PATCHPOINT specially and then use the generic code.
-  if (Opc == TargetOpcode::STACKMAP || Opc == TargetOpcode::PATCHPOINT) {
+  if (Opc == TargetOpcode::STACKMAP || Opc == TargetOpcode::PATCHPOINT ||
+      Opc == TargetOpcode::TCPATCHPOINT) {
     // Stackmaps do not have arguments and do not preserve their calling
     // convention. However, to simplify runtime support, they clobber the same
     // scratch registers as AnyRegCC.
Index: lib/CodeGen/SelectionDAG/ScheduleDAGSDNodes.cpp
===================================================================
--- lib/CodeGen/SelectionDAG/ScheduleDAGSDNodes.cpp	(revision 365284)
+++ lib/CodeGen/SelectionDAG/ScheduleDAGSDNodes.cpp	(working copy)
@@ -538,6 +538,10 @@
     NodeNumDefs = 0;
     return;
   }
+  if (POpc == TargetOpcode::TCPATCHPOINT) {
+    NodeNumDefs = 0;
+    return;
+  }
   if (POpc == TargetOpcode::PATCHPOINT &&
       Node->getValueType(0) == MVT::Other) {
     // PATCHPOINT is defined to have one result, but it might really have none
Index: lib/CodeGen/SelectionDAG/SelectionDAGBuilder.cpp
===================================================================
--- lib/CodeGen/SelectionDAG/SelectionDAGBuilder.cpp	(revision 365284)
+++ lib/CodeGen/SelectionDAG/SelectionDAGBuilder.cpp	(working copy)
@@ -6155,6 +6155,12 @@
   case Intrinsic::experimental_gc_relocate:
     visitGCRelocate(cast<GCRelocateInst>(I));
     return nullptr;
+  case Intrinsic::experimental_gc_exception:
+    visitGCException(cast<GCExceptionInst>(I));
+    return nullptr;
+  case Intrinsic::experimental_gc_exception_data:
+    visitGCExceptionData(cast<GCExceptionDataInst>(I));
+    return nullptr;
   case Intrinsic::instrprof_increment:
     llvm_unreachable("instrprof failed to lower an increment");
   case Intrinsic::instrprof_value_profile:
@@ -8187,6 +8193,32 @@
   FuncInfo.MF->getFrameInfo().setHasStackMap();
 }
 
+static bool isPatchpointInTailCallPosition(ImmutableCallSite CS) {
+  const Instruction *I = CS.getInstruction();
+  const BasicBlock *ExitBB = I->getParent();
+  const Instruction *Term = ExitBB->getTerminator();
+  const ReturnInst *Ret = dyn_cast<ReturnInst>(Term);
+  // If Term is not a ReturnInst, then it must be a UnreachableInst.
+  if (!Ret && !isa<UnreachableInst>(Term))
+    return false;
+  // Copy from isInTailCallPosition.
+  // If I will have a chain, make sure no other instruction that will have a
+  // chain interposes between I and the return.
+  if (I->mayHaveSideEffects() || I->mayReadFromMemory() ||
+      !isSafeToSpeculativelyExecute(I))
+    for (BasicBlock::const_iterator BBI = std::prev(ExitBB->end(), 2);; --BBI) {
+      if (&*BBI == I)
+        break;
+      // Debug info intrinsics do not get in the way of tail call optimization.
+      if (isa<DbgInfoIntrinsic>(BBI))
+        continue;
+      if (BBI->mayHaveSideEffects() || BBI->mayReadFromMemory() ||
+          !isSafeToSpeculativelyExecute(&*BBI))
+        return false;
+    }
+  return true;
+}
+
 /// Lower llvm.experimental.patchpoint directly to its target opcode.
 void SelectionDAGBuilder::visitPatchpoint(ImmutableCallSite CS,
                                           const BasicBlock *EHPadBB) {
@@ -8230,17 +8262,25 @@
   TargetLowering::CallLoweringInfo CLI(DAG);
   populateCallLoweringInfo(CLI, CS, NumMetaOpers, NumCallArgs, Callee, ReturnTy,
                            true);
+  CLI.IsTailCall = isPatchpointInTailCallPosition(CS);
+  assert((!CLI.IsTailCall || !HasDef) &&
+         "TailCall should not has a return type");
   std::pair<SDValue, SDValue> Result = lowerInvokable(CLI, EHPadBB);
+  SDNode *Call;
+  assert(CLI.IsTailCall || !HasTailCall);
+  if (!CLI.IsTailCall) {
+    SDNode *CallEnd = Result.second.getNode();
+    if (HasDef && (CallEnd->getOpcode() == ISD::CopyFromReg))
+      CallEnd = CallEnd->getOperand(0).getNode();
 
-  SDNode *CallEnd = Result.second.getNode();
-  if (HasDef && (CallEnd->getOpcode() == ISD::CopyFromReg))
-    CallEnd = CallEnd->getOperand(0).getNode();
-
-  /// Get a call instruction from the call sequence chain.
-  /// Tail calls are not allowed.
-  assert(CallEnd->getOpcode() == ISD::CALLSEQ_END &&
-         "Expected a callseq node.");
-  SDNode *Call = CallEnd->getOperand(0).getNode();
+    /// Get a call instruction from the call sequence chain.
+    /// Tail calls are not allowed.
+    assert(CallEnd->getOpcode() == ISD::CALLSEQ_END &&
+           "Expected a callseq node.");
+    Call = CallEnd->getOperand(0).getNode();
+  } else {
+    Call = CLI.Chain.getNode();
+  }
   bool HasGlue = Call->getGluedNode();
 
   // Replace the target specific call node with the patchable intrinsic.
@@ -8260,8 +8300,9 @@
 
   // Adjust <numArgs> to account for any arguments that have been passed on the
   // stack instead.
-  // Call Node: Chain, Target, {Args}, RegMask, [Glue]
-  unsigned NumCallRegArgs = Call->getNumOperands() - (HasGlue ? 4 : 3);
+  // Call Node: Chain, Target, {Args}, [RegMask], [Glue]
+  unsigned NumCallRegArgs =
+      Call->getNumOperands() + (CLI.IsTailCall ? 1 : 0) - (HasGlue ? 4 : 3);
   NumCallRegArgs = IsAnyRegCC ? NumArgs : NumCallRegArgs;
   Ops.push_back(DAG.getTargetConstant(NumCallRegArgs, dl, MVT::i32));
 
@@ -8276,6 +8317,8 @@
 
   // Push the arguments from the call instruction up to the register mask.
   SDNode::op_iterator e = HasGlue ? Call->op_end()-2 : Call->op_end()-1;
+  if (CLI.IsTailCall)
+    e += 1;
   Ops.append(Call->op_begin() + 2, e);
 
   // Push live variables for the stack map.
@@ -8282,10 +8325,12 @@
   addStackMapLiveVars(CS, NumMetaOpers + NumArgs, dl, Ops, *this);
 
   // Push the register mask info.
-  if (HasGlue)
-    Ops.push_back(*(Call->op_end()-2));
-  else
-    Ops.push_back(*(Call->op_end()-1));
+  if (NumCallRegArgs) {
+    if (HasGlue)
+      Ops.push_back(*(Call->op_end()-2));
+    else
+      Ops.push_back(*(Call->op_end()-1));
+  }
 
   // Push the chain (this is originally the first operand of the call, but
   // becomes now the last or second to last operand).
@@ -8311,8 +8356,9 @@
     NodeTys = DAG.getVTList(MVT::Other, MVT::Glue);
 
   // Replace the target specific call node with a PATCHPOINT node.
-  MachineSDNode *MN = DAG.getMachineNode(TargetOpcode::PATCHPOINT,
-                                         dl, NodeTys, Ops);
+  MachineSDNode *MN = DAG.getMachineNode(
+      CLI.IsTailCall ? TargetOpcode::TCPATCHPOINT : TargetOpcode::PATCHPOINT,
+      dl, NodeTys, Ops);
 
   // Update the NodeMap.
   if (HasDef) {
Index: lib/CodeGen/SelectionDAG/SelectionDAGBuilder.h
===================================================================
--- lib/CodeGen/SelectionDAG/SelectionDAGBuilder.h	(revision 365284)
+++ lib/CodeGen/SelectionDAG/SelectionDAGBuilder.h	(working copy)
@@ -947,6 +947,8 @@
   // These two are implemented in StatepointLowering.cpp
   void visitGCRelocate(const GCRelocateInst &Relocate);
   void visitGCResult(const GCResultInst &I);
+  void visitGCException(const GCExceptionInst &I);
+  void visitGCExceptionData(const GCExceptionDataInst &I);
 
   void visitVectorReduce(const CallInst &I, unsigned Intrinsic);
 
Index: lib/CodeGen/SelectionDAG/StatepointLowering.cpp
===================================================================
--- lib/CodeGen/SelectionDAG/StatepointLowering.cpp	(revision 365284)
+++ lib/CodeGen/SelectionDAG/StatepointLowering.cpp	(working copy)
@@ -541,14 +541,15 @@
   // arrays interwoven with each (lowered) base pointer immediately followed by
   // it's (lowered) derived pointer.  i.e
   // (base[0], ptr[0], base[1], ptr[1], ...)
+  bool LiveInOnly = SI.CLI.CallConv == CallingConv::V8CC;
   for (unsigned i = 0; i < SI.Bases.size(); ++i) {
     const Value *Base = SI.Bases[i];
-    lowerIncomingStatepointValue(Builder.getValue(Base), /*LiveInOnly*/ false,
-                                 Ops, Builder);
+    lowerIncomingStatepointValue(Builder.getValue(Base), LiveInOnly, Ops,
+                                 Builder);
 
     const Value *Ptr = SI.Ptrs[i];
-    lowerIncomingStatepointValue(Builder.getValue(Ptr), /*LiveInOnly*/ false,
-                                 Ops, Builder);
+    lowerIncomingStatepointValue(Builder.getValue(Ptr), LiveInOnly, Ops,
+                                 Builder);
   }
 
   // If there are any explicit spill slots passed to the statepoint, record
@@ -827,6 +828,11 @@
 
     unsigned AS = ISP.getCalledValue()->getType()->getPointerAddressSpace();
     ActualCallee = DAG.getConstant(0, getCurSDLoc(), TLI.getPointerTy(DL, AS));
+    if (auto *ConstCallee =
+            dyn_cast<ConstantSDNode>(getValue(ISP.getCalledValue())))
+      ActualCallee =
+          DAG.getIntPtrConstant(ConstCallee->getZExtValue(), getCurSDLoc(),
+                                /*isTarget=*/true);
   } else {
     ActualCallee = getValue(ISP.getCalledValue());
   }
@@ -956,6 +962,42 @@
   }
 }
 
+void SelectionDAGBuilder::visitGCException(const GCExceptionInst &CI) {
+  const TargetLowering &TLI = DAG.getTargetLoweringInfo();
+  const Constant *PersonalityFn = FuncInfo.Fn->getPersonalityFn();
+  if (TLI.getExceptionPointerRegister(PersonalityFn) == 0 &&
+      TLI.getExceptionSelectorRegister(PersonalityFn) == 0)
+    return;
+  assert(FuncInfo.MBB->isEHPad() && "Call to landingpad not in landing pad!");
+  SDValue Op;
+  SDLoc dl = getCurSDLoc();
+  Type *RetTy = CI.getFunctionType()->getReturnType();
+  Op = DAG.getZExtOrTrunc(
+      DAG.getCopyFromReg(DAG.getEntryNode(), dl,
+                         FuncInfo.ExceptionPointerVirtReg,
+                         TLI.getPointerTy(DAG.getDataLayout())),
+      dl, TLI.getValueType(DAG.getDataLayout(), RetTy));
+  setValue(&CI, Op);
+}
+
+void SelectionDAGBuilder::visitGCExceptionData(const GCExceptionDataInst &CI) {
+  const TargetLowering &TLI = DAG.getTargetLoweringInfo();
+  const Constant *PersonalityFn = FuncInfo.Fn->getPersonalityFn();
+  if (TLI.getExceptionPointerRegister(PersonalityFn) == 0 &&
+      TLI.getExceptionSelectorRegister(PersonalityFn) == 0)
+    return;
+  assert(FuncInfo.MBB->isEHPad() && "Call to landingpad not in landing pad!");
+  SDValue Op;
+  SDLoc dl = getCurSDLoc();
+  Type *RetTy = CI.getFunctionType()->getReturnType();
+  Op = DAG.getZExtOrTrunc(
+      DAG.getCopyFromReg(DAG.getEntryNode(), dl,
+                         FuncInfo.ExceptionSelectorVirtReg,
+                         TLI.getPointerTy(DAG.getDataLayout())),
+      dl, TLI.getValueType(DAG.getDataLayout(), RetTy));
+  setValue(&CI, Op);
+}
+
 void SelectionDAGBuilder::visitGCRelocate(const GCRelocateInst &Relocate) {
 #ifndef NDEBUG
   // Consistency check
@@ -981,7 +1023,14 @@
   // We didn't need to spill these special cases (constants and allocas).
   // See the handling in spillIncomingValueForStatepoint for detail.
   if (!DerivedPtrLocation) {
-    setValue(&Relocate, SD);
+    SDVTList NodeTys = DAG.getVTList(SD.getValueType());
+
+    SmallVector<SDValue, 1> Ops;
+    Ops.emplace_back(SD);
+    MachineSDNode *RelocateMCNode = DAG.getMachineNode(
+        TargetOpcode::RELOCATE_DEF, getCurSDLoc(), NodeTys, Ops);
+
+    setValue(&Relocate, SDValue(RelocateMCNode, 0));
     return;
   }
 
Index: lib/CodeGen/SelectionDAG/TargetLowering.cpp
===================================================================
--- lib/CodeGen/SelectionDAG/TargetLowering.cpp	(revision 365284)
+++ lib/CodeGen/SelectionDAG/TargetLowering.cpp	(working copy)
@@ -89,6 +89,9 @@
     // (We look for a CopyFromReg reading a virtual register that is used
     //  for the function live-in value of register Reg)
     SDValue Value = OutVals[I];
+    // Ignore the undefined input.
+    if (Value->getOpcode() == ISD::UNDEF)
+      continue;
     if (Value->getOpcode() != ISD::CopyFromReg)
       return false;
     unsigned ArgReg = cast<RegisterSDNode>(Value->getOperand(1))->getReg();
Index: lib/CodeGen/ShrinkWrap.cpp
===================================================================
--- lib/CodeGen/ShrinkWrap.cpp	(revision 365284)
+++ lib/CodeGen/ShrinkWrap.cpp	(working copy)
@@ -170,9 +170,13 @@
 
       TFI->determineCalleeSaves(*MachineFunc, SavedRegs, RS);
 
+      const TargetRegisterInfo *TRI =
+          MachineFunc->getSubtarget().getRegisterInfo();
       for (int Reg = SavedRegs.find_first(); Reg != -1;
-           Reg = SavedRegs.find_next(Reg))
-        CurrentCSRs.insert((unsigned)Reg);
+           Reg = SavedRegs.find_next(Reg)) {
+        for (MCRegAliasIterator AI(Reg, TRI, true); AI.isValid(); ++AI)
+          CurrentCSRs.insert(*AI);
+      }
     }
     return CurrentCSRs;
   }
@@ -264,6 +268,8 @@
     LLVM_DEBUG(dbgs() << "Frame instruction: " << MI << '\n');
     return true;
   }
+  if (MI.isReturn())
+    return false;
   for (const MachineOperand &MO : MI.operands()) {
     bool UseOrDefCSR = false;
     if (MO.isReg()) {
@@ -280,8 +286,8 @@
       // separately. An SP mentioned by a call instruction, we can ignore,
       // though, as it's harmless and we do not want to effectively disable tail
       // calls by forcing the restore point to post-dominate them.
-      UseOrDefCSR = (!MI.isCall() && PhysReg == SP) ||
-                    RCI.getLastCalleeSavedAlias(PhysReg);
+      UseOrDefCSR =
+          (!MI.isCall() && PhysReg == SP) || getCurrentCSRs(RS).count(PhysReg);
     } else if (MO.isRegMask()) {
       // Check if this regmask clobbers any of the CSRs.
       for (unsigned Reg : getCurrentCSRs(RS)) {
Index: lib/CodeGen/StackMaps.cpp
===================================================================
--- lib/CodeGen/StackMaps.cpp	(revision 365284)
+++ lib/CodeGen/StackMaps.cpp	(working copy)
@@ -30,6 +30,7 @@
 #include "llvm/Support/ErrorHandling.h"
 #include "llvm/Support/MathExtras.h"
 #include "llvm/Support/raw_ostream.h"
+#include "llvm/Target/TargetMachine.h"
 #include <algorithm>
 #include <cassert>
 #include <cstdint>
@@ -371,7 +372,9 @@
 }
 
 void StackMaps::recordPatchPoint(const MachineInstr &MI) {
-  assert(MI.getOpcode() == TargetOpcode::PATCHPOINT && "expected patchpoint");
+  assert(((MI.getOpcode() == TargetOpcode::PATCHPOINT) ||
+          (MI.getOpcode() == TargetOpcode::TCPATCHPOINT)) &&
+         "expected patchpoint");
 
   PatchPointOpers opers(&MI);
   const int64_t ID = opers.getID();
@@ -443,7 +446,7 @@
     LLVM_DEBUG(dbgs() << WSMP << "function addr: " << FR.first
                       << " frame size: " << FR.second.StackSize
                       << " callsite count: " << FR.second.RecordCount << '\n');
-    OS.EmitSymbolValue(FR.first, 8);
+    OS.EmitSymbolValue(FR.first, AP.TM.getProgramPointerSize());
     OS.EmitIntValue(FR.second.StackSize, 8);
     OS.EmitIntValue(FR.second.RecordCount, 8);
   }
Index: lib/CodeGen/TargetInstrInfo.cpp
===================================================================
--- lib/CodeGen/TargetInstrInfo.cpp	(revision 365284)
+++ lib/CodeGen/TargetInstrInfo.cpp	(working copy)
@@ -468,6 +468,7 @@
     StartIdx = StackMapOpers(&MI).getVarIdx();
     break;
   }
+  case TargetOpcode::TCPATCHPOINT:
   case TargetOpcode::PATCHPOINT: {
     // For PatchPoint, the call args are not foldable (even if reported in the
     // stackmap e.g. via anyregcc).
@@ -562,6 +563,7 @@
 
   if (MI.getOpcode() == TargetOpcode::STACKMAP ||
       MI.getOpcode() == TargetOpcode::PATCHPOINT ||
+      MI.getOpcode() == TargetOpcode::TCPATCHPOINT ||
       MI.getOpcode() == TargetOpcode::STATEPOINT) {
     // Fold stackmap/patchpoint.
     NewMI = foldPatchpoint(MF, MI, Ops, FI, *this);
@@ -626,6 +628,7 @@
   int FrameIndex = 0;
 
   if ((MI.getOpcode() == TargetOpcode::STACKMAP ||
+       MI.getOpcode() == TargetOpcode::TCPATCHPOINT ||
        MI.getOpcode() == TargetOpcode::PATCHPOINT ||
        MI.getOpcode() == TargetOpcode::STATEPOINT) &&
       isLoadFromStackSlot(LoadMI, FrameIndex)) {
Index: lib/CodeGen/TargetPassConfig.cpp
===================================================================
--- lib/CodeGen/TargetPassConfig.cpp	(revision 365284)
+++ lib/CodeGen/TargetPassConfig.cpp	(working copy)
@@ -954,6 +954,7 @@
   if (getOptLevel() != CodeGenOpt::None)
     addBlockPlacement();
 
+  addPass(&StackMapLivenessID, false);
   addPreEmitPass();
 
   if (TM->Options.EnableIPRA)
@@ -963,7 +964,6 @@
 
   addPass(&FuncletLayoutID, false);
 
-  addPass(&StackMapLivenessID, false);
   addPass(&LiveDebugValuesID, false);
 
   // Insert before XRay Instrumentation.
Index: lib/ExecutionEngine/ExecutionEngineBindings.cpp
===================================================================
--- lib/ExecutionEngine/ExecutionEngineBindings.cpp	(revision 365284)
+++ lib/ExecutionEngine/ExecutionEngineBindings.cpp	(working copy)
@@ -200,6 +200,7 @@
          .setErrorStr(&Error)
          .setOptLevel((CodeGenOpt::Level)options.OptLevel)
          .setTargetOptions(targetOptions);
+  builder.setRelocationModel(Reloc::PIC_);
   bool JIT;
   if (Optional<CodeModel::Model> CM = unwrap(options.CodeModel, JIT))
     builder.setCodeModel(*CM);
Index: lib/ExecutionEngine/RuntimeDyld/RuntimeDyld.cpp
===================================================================
--- lib/ExecutionEngine/RuntimeDyld/RuntimeDyld.cpp	(revision 365284)
+++ lib/ExecutionEngine/RuntimeDyld/RuntimeDyld.cpp	(working copy)
@@ -1201,7 +1201,9 @@
   // they share a single memory manager.  This can become a problem when page
   // permissions are applied.
   Dyld = nullptr;
-  ProcessAllSections = false;
+  // UC_BUILD
+  // ProcessAllSections = false;
+  ProcessAllSections = true;
   Checker = nullptr;
 }
 
Index: lib/IR/AsmWriter.cpp
===================================================================
--- lib/IR/AsmWriter.cpp	(revision 365284)
+++ lib/IR/AsmWriter.cpp	(working copy)
@@ -349,6 +349,12 @@
   case CallingConv::Cold:          Out << "coldcc"; break;
   case CallingConv::WebKit_JS:     Out << "webkit_jscc"; break;
   case CallingConv::AnyReg:        Out << "anyregcc"; break;
+  case CallingConv::V8CC:
+    Out << "v8cc";
+    break;
+  case CallingConv::V8SBCC:
+    Out << "v8sbcc";
+    break;
   case CallingConv::PreserveMost:  Out << "preserve_mostcc"; break;
   case CallingConv::PreserveAll:   Out << "preserve_allcc"; break;
   case CallingConv::CXX_FAST_TLS:  Out << "cxx_fast_tlscc"; break;
Index: lib/IR/Verifier.cpp
===================================================================
--- lib/IR/Verifier.cpp	(revision 365284)
+++ lib/IR/Verifier.cpp	(working copy)
@@ -2013,7 +2013,9 @@
     Assert(UserCall, "illegal use of statepoint token", Call, U);
     if (!UserCall)
       continue;
-    Assert(isa<GCRelocateInst>(UserCall) || isa<GCResultInst>(UserCall),
+    Assert(isa<GCRelocateInst>(UserCall) || isa<GCResultInst>(UserCall) ||
+               isa<GCExceptionInst>(UserCall) ||
+               isa<GCExceptionDataInst>(UserCall),
            "gc.result or gc.relocate are the only value uses "
            "of a gc.statepoint",
            Call, U);
@@ -2023,6 +2025,13 @@
     } else if (isa<GCRelocateInst>(Call)) {
       Assert(UserCall->getArgOperand(0) == &Call,
              "gc.relocate connected to wrong gc.statepoint", Call, UserCall);
+    } else if (isa<GCExceptionInst>(UserCall)) {
+      Assert(UserCall->getArgOperand(0) == &Call,
+             "gc.exception connected to wrong gc.statepoint", Call, UserCall);
+    } else if (isa<GCExceptionDataInst>(UserCall)) {
+      Assert(UserCall->getArgOperand(0) == &Call,
+             "gc.exception_data connected to wrong gc.statepoint", Call,
+             UserCall);
     }
   }
 
Index: lib/MC/MCAsmStreamer.cpp
===================================================================
--- lib/MC/MCAsmStreamer.cpp	(revision 365284)
+++ lib/MC/MCAsmStreamer.cpp	(working copy)
@@ -941,7 +941,7 @@
   if (!Directive) {
     int64_t IntValue;
     if (!Value->evaluateAsAbsolute(IntValue))
-      report_fatal_error("Don't know how to emit this value.");
+      IntValue = -1;
 
     // We couldn't handle the requested integer size so we fallback by breaking
     // the request down into several, smaller, integers.
Index: lib/Support/ARMTargetParser.cpp
===================================================================
--- lib/Support/ARMTargetParser.cpp	(revision 365284)
+++ lib/Support/ARMTargetParser.cpp	(working copy)
@@ -567,6 +567,8 @@
   case Triple::EABIHF:
   case Triple::EABI:
     return "aapcs";
+  case Triple::V8:
+    return "v8";
   default:
     if (TT.isOSNetBSD())
       return "apcs-gnu";
Index: lib/Support/Triple.cpp
===================================================================
--- lib/Support/Triple.cpp	(revision 365284)
+++ lib/Support/Triple.cpp	(working copy)
@@ -235,6 +235,7 @@
   case Cygnus: return "cygnus";
   case CoreCLR: return "coreclr";
   case Simulator: return "simulator";
+  case V8: return "v8";
   }
 
   llvm_unreachable("Invalid EnvironmentType!");
@@ -530,6 +531,7 @@
     .StartsWith("cygnus", Triple::Cygnus)
     .StartsWith("coreclr", Triple::CoreCLR)
     .StartsWith("simulator", Triple::Simulator)
+    .StartsWith("v8", Triple::V8)
     .Default(Triple::UnknownEnvironment);
 }
 
Index: lib/Target/ARM/ARMAsmPrinter.cpp
===================================================================
--- lib/Target/ARM/ARMAsmPrinter.cpp	(revision 365284)
+++ lib/Target/ARM/ARMAsmPrinter.cpp	(working copy)
@@ -55,7 +55,7 @@
 ARMAsmPrinter::ARMAsmPrinter(TargetMachine &TM,
                              std::unique_ptr<MCStreamer> Streamer)
     : AsmPrinter(TM, std::move(Streamer)), AFI(nullptr), MCP(nullptr),
-      InConstantPool(false), OptimizationGoals(-1) {}
+      InConstantPool(false), OptimizationGoals(-1), SM(*this) {}
 
 void ARMAsmPrinter::EmitFunctionBodyEnd() {
   // Make sure to terminate any constant pools that were at the end
@@ -559,6 +559,7 @@
       OutStreamer->AddBlankLine();
     }
 
+    SM.serializeToStackMapSection();
     // Funny Darwin hack: This flag tells the linker that no global symbols
     // contain code that falls through to other global symbols (e.g. the obvious
     // implementation of multiple entry points).  If this doesn't occur, the
@@ -578,6 +579,15 @@
   OptimizationGoals = -1;
 
   ATS.finishAttributeSection();
+  if (TT.isOSBinFormatCOFF()) {
+    SM.serializeToStackMapSection();
+    return;
+  }
+
+  if (TT.isOSBinFormatELF()) {
+    SM.serializeToStackMapSection();
+    return;
+  }
 }
 
 //===----------------------------------------------------------------------===//
@@ -2064,6 +2074,13 @@
   case ARM::PATCHABLE_TAIL_CALL:
     LowerPATCHABLE_TAIL_CALL(*MI);
     return;
+  case TargetOpcode::STACKMAP:
+    return LowerSTACKMAP(*OutStreamer, SM, *MI);
+  case TargetOpcode::PATCHPOINT:
+  case TargetOpcode::TCPATCHPOINT:
+    return LowerPATCHPOINT(*OutStreamer, SM, *MI);
+  case TargetOpcode::STATEPOINT:
+    return LowerSTATEPOINT(*OutStreamer, SM, *MI);
   }
 
   MCInst TmpInst;
@@ -2072,6 +2089,107 @@
   EmitToStreamer(*OutStreamer, TmpInst);
 }
 
+static unsigned roundUpTo4ByteAligned(unsigned n) {
+  unsigned mask = 3;
+  unsigned rev = ~3;
+  n = (n & rev) + (((n & mask) + mask) & rev);
+  return n;
+}
+
+void ARMAsmPrinter::LowerSTACKMAP(MCStreamer &OutStreamer, StackMaps &SM,
+                                  const MachineInstr &MI) {
+  assert(!AFI->isThumbFunction());
+  unsigned NumNOPBytes =
+      roundUpTo4ByteAligned(StackMapOpers(&MI).getNumPatchBytes());
+
+  SM.recordStackMap(MI);
+  assert(NumNOPBytes % 4 == 0 && "Invalid number of NOP bytes requested!");
+
+  // Scan ahead to trim the shadow.
+  const MachineBasicBlock &MBB = *MI.getParent();
+  MachineBasicBlock::const_iterator MII(MI);
+  ++MII;
+  while (NumNOPBytes > 0) {
+    if (MII == MBB.end() || MII->isCall() ||
+        MII->getOpcode() == ARM::DBG_VALUE ||
+        MII->getOpcode() == TargetOpcode::PATCHPOINT ||
+        MII->getOpcode() == TargetOpcode::TCPATCHPOINT ||
+        MII->getOpcode() == TargetOpcode::STACKMAP)
+      break;
+    ++MII;
+    NumNOPBytes -= 4;
+  }
+
+  // Emit nops.
+  MCInst Noop;
+  Subtarget->getInstrInfo()->getNoop(Noop);
+  for (unsigned i = 0; i < NumNOPBytes; i += 4)
+    EmitToStreamer(OutStreamer, Noop);
+}
+
+// Lower a patchpoint of the form:
+// [<def>], <id>, <numBytes>, <target>, <numArgs>
+void ARMAsmPrinter::LowerPATCHPOINT(MCStreamer &OutStreamer, StackMaps &SM,
+                                    const MachineInstr &MI) {
+  assert(!AFI->isThumbFunction());
+  SM.recordPatchPoint(MI);
+
+  PatchPointOpers Opers(&MI);
+
+  int64_t CallTarget = Opers.getCallTarget().getImm();
+  unsigned EncodedBytes = 0;
+  if (CallTarget) {
+    assert((CallTarget & 0xFFFFFFFFLL) == CallTarget &&
+           "High 32 bits of call target should be zero.");
+    unsigned ScratchReg = MI.getOperand(Opers.getNextScratchIdx()).getReg();
+    EncodedBytes = 16;
+    EmitToStreamer(OutStreamer, MCInstBuilder(ARM::MOVTi16)
+                                    .addReg(ScratchReg)
+                                    .addReg(ScratchReg)
+                                    .addImm((CallTarget >> 16) & 0xFFFF)
+                                    .addImm(ARMCC::AL)
+                                    .addImm(0));
+    EmitToStreamer(OutStreamer, MCInstBuilder(ARM::MOVi16)
+                                    .addReg(ScratchReg)
+                                    .addImm(CallTarget & 0xFFFF)
+                                    .addImm(ARMCC::AL)
+                                    .addImm(0));
+    EmitToStreamer(OutStreamer, MCInstBuilder(ARM::BLX).addReg(ScratchReg));
+  }
+  // Emit padding.
+  unsigned NumBytes = roundUpTo4ByteAligned(Opers.getNumPatchBytes());
+  assert(NumBytes >= EncodedBytes &&
+         "Patchpoint can't request size less than the length of a call.");
+  assert((NumBytes - EncodedBytes) % 4 == 0 &&
+         "Invalid number of NOP bytes requested!");
+  MCInst Noop;
+  Subtarget->getInstrInfo()->getNoop(Noop);
+  for (unsigned i = EncodedBytes; i < NumBytes; i += 4)
+    EmitToStreamer(OutStreamer, Noop);
+}
+
+void ARMAsmPrinter::LowerSTATEPOINT(MCStreamer &OutStreamer, StackMaps &SM,
+                                    const MachineInstr &MI) {
+  assert(!AFI->isThumbFunction());
+  SM.recordStatepoint(MI);
+
+  StatepointOpers SOpers(&MI);
+  MCInst Noop;
+  Subtarget->getInstrInfo()->getNoop(Noop);
+  if (unsigned PatchBytes = SOpers.getNumPatchBytes()) {
+    unsigned NumBytes = roundUpTo4ByteAligned(PatchBytes);
+    unsigned EncodedBytes = 0;
+    assert(NumBytes >= EncodedBytes &&
+           "Patchpoint can't request size less than the length of a call.");
+    assert((NumBytes - EncodedBytes) % 4 == 0 &&
+           "Invalid number of NOP bytes requested!");
+    MCInst Noop;
+    Subtarget->getInstrInfo()->getNoop(Noop);
+    for (unsigned i = EncodedBytes; i < NumBytes; i += 4)
+      EmitToStreamer(OutStreamer, Noop);
+  }
+}
+
 //===----------------------------------------------------------------------===//
 // Target Registry Stuff
 //===----------------------------------------------------------------------===//
Index: lib/Target/ARM/ARMAsmPrinter.h
===================================================================
--- lib/Target/ARM/ARMAsmPrinter.h	(revision 365284)
+++ lib/Target/ARM/ARMAsmPrinter.h	(working copy)
@@ -12,6 +12,7 @@
 
 #include "ARMSubtarget.h"
 #include "llvm/CodeGen/AsmPrinter.h"
+#include "llvm/CodeGen/StackMaps.h"
 #include "llvm/Target/TargetMachine.h"
 
 namespace llvm {
@@ -64,8 +65,10 @@
   /// Set of globals in PromotedGlobals that we've emitted labels for.
   /// We need to emit labels even for promoted globals so that DWARF
   /// debug info can link properly.
-  SmallPtrSet<const GlobalVariable*,2> EmittedPromotedGlobalLabels;
+  SmallPtrSet<const GlobalVariable *, 2> EmittedPromotedGlobalLabels;
 
+  StackMaps SM;
+
 public:
   explicit ARMAsmPrinter(TargetMachine &TM,
                          std::unique_ptr<MCStreamer> Streamer);
@@ -131,6 +134,15 @@
   bool emitPseudoExpansionLowering(MCStreamer &OutStreamer,
                                    const MachineInstr *MI);
 
+  void LowerSTACKMAP(MCStreamer &OutStreamer, StackMaps &SM,
+                     const MachineInstr &MI);
+
+  void LowerPATCHPOINT(MCStreamer &OutStreamer, StackMaps &SM,
+                       const MachineInstr &MI);
+
+  void LowerSTATEPOINT(MCStreamer &OutStreamer, StackMaps &SM,
+                       const MachineInstr &MI);
+
 public:
   unsigned getISAEncoding() override {
     // ARM/Darwin adds ISA to the DWARF info for each function.
Index: lib/Target/ARM/ARMBaseInstrInfo.cpp
===================================================================
--- lib/Target/ARM/ARMBaseInstrInfo.cpp	(revision 365284)
+++ lib/Target/ARM/ARMBaseInstrInfo.cpp	(working copy)
@@ -25,6 +25,7 @@
 #include "llvm/ADT/SmallSet.h"
 #include "llvm/ADT/SmallVector.h"
 #include "llvm/ADT/Triple.h"
+#include "llvm/Analysis/ValueTracking.h"
 #include "llvm/CodeGen/LiveVariables.h"
 #include "llvm/CodeGen/MachineBasicBlock.h"
 #include "llvm/CodeGen/MachineConstantPool.h"
@@ -721,6 +722,10 @@
     return 0;
   case TargetOpcode::BUNDLE:
     return getInstBundleLength(MI);
+  case TargetOpcode::TCPATCHPOINT:
+  case TargetOpcode::PATCHPOINT:
+  case TargetOpcode::STATEPOINT:
+    return MI.getOperand(1).getImm();
   case ARM::MOVi16_ga_pcrel:
   case ARM::MOVTi16_ga_pcrel:
   case ARM::t2MOVi16_ga_pcrel:
@@ -1462,6 +1467,72 @@
   BB->erase(MI);
 }
 
+void ARMBaseInstrInfo::expandRESTORESP(MachineBasicBlock::iterator MI) const {
+  MachineBasicBlock *BB = MI->getParent();
+  bool ShouldExpand = true;
+  MachineBasicBlock::iterator it = MI;
+  ++it;
+  MachineBasicBlock::iterator end = MI->getParent()->end();
+  for (; it != end; ++it) {
+    bool IsDef = false, IsUse = false;
+    if (it->isCall()) {
+      break;
+    }
+    for (unsigned i = 0, e = it->getNumOperands(); i != e; ++i) {
+      const MachineOperand &MO = it->getOperand(i);
+      if (!MO.isReg())
+        continue;
+      unsigned MOReg = MO.getReg();
+      if (MOReg != ARM::SP)
+        continue;
+      // Define new sp
+      if (MO.isDef())
+        IsDef = true;
+      else
+        IsUse = true;
+    }
+    // Should expand now.
+    if (IsUse) {
+      break;
+    }
+    if (IsDef) {
+      ShouldExpand = false;
+      break;
+    }
+  }
+  if (ShouldExpand) {
+    MachineFunction *MF = BB->getParent();
+    DebugLoc dl = MI->getDebugLoc();
+
+    ARMFunctionInfo *AFI = MF->getInfo<ARMFunctionInfo>();
+    BuildMI(*BB, MI, dl, get(ARM::SUBri), ARM::SP)
+        .addReg(ARM::R11)
+        .addImm(AFI->getFramePtrSpillOffset())
+        .add(predOps(ARMCC::AL))
+        .add(condCodeOp());
+  }
+  BB->erase(MI);
+}
+
+int ARMBaseInstrInfo::getSPAdjust(const MachineInstr &MI) const {
+
+  const MachineFunction *MF = MI.getParent()->getParent();
+  const ARMFunctionInfo *AFI = MF->getInfo<ARMFunctionInfo>();
+  if (!AFI->isJSFunction() && !AFI->isJSStub())
+    return ARMGenInstrInfo::getSPAdjust(MI);
+  if (MI.isCall()) {
+    if (AFI->isJSStub() || AFI->isJSFunction())
+      return -AFI->popLastSPAdjust();
+  }
+  unsigned FrameSetupOpcode = getCallFrameSetupOpcode();
+  if (MI.getOpcode() == FrameSetupOpcode) {
+    int SPAdj = getFrameSize(MI);
+    AFI->pushLastSPAdjust(SPAdj);
+    return SPAdj;
+  }
+  return 0;
+}
+
 bool ARMBaseInstrInfo::expandPostRAPseudo(MachineInstr &MI) const {
   if (MI.getOpcode() == TargetOpcode::LOAD_STACK_GUARD) {
     assert(getSubtarget().getTargetTriple().isOSBinFormatMachO() &&
@@ -1476,6 +1547,11 @@
     return true;
   }
 
+  if (MI.getOpcode() == TargetOpcode::RESTORESP) {
+    expandRESTORESP(MI);
+    return true;
+  }
+  
   // This hook gets to expand COPY instructions before they become
   // copyPhysReg() calls.  Look for VMOVS instructions that can legally be
   // widened to VMOVD.  We prefer the VMOVD when possible because it may be
@@ -1892,6 +1968,9 @@
 isProfitableToIfCvt(MachineBasicBlock &MBB,
                     unsigned NumCycles, unsigned ExtraPredCycles,
                     BranchProbability Probability) const {
+  CallingConv::ID CallingConv = MBB.getParent()->getFunction().getCallingConv();
+  if (CallingConv == CallingConv::V8CC || CallingConv == CallingConv::V8SBCC)
+    return false;
   if (!NumCycles)
     return false;
 
Index: lib/Target/ARM/ARMBaseInstrInfo.h
===================================================================
--- lib/Target/ARM/ARMBaseInstrInfo.h	(revision 365284)
+++ lib/Target/ARM/ARMBaseInstrInfo.h	(working copy)
@@ -398,6 +398,8 @@
   virtual void expandLoadStackGuard(MachineBasicBlock::iterator MI) const = 0;
 
   void expandMEMCPY(MachineBasicBlock::iterator) const;
+  void expandRESTORESP(MachineBasicBlock::iterator) const;
+  int getSPAdjust(const MachineInstr &MI) const final override;
 
 private:
   /// Modeling special VFP / NEON fp MLA / MLS hazards.
Index: lib/Target/ARM/ARMBaseRegisterInfo.cpp
===================================================================
--- lib/Target/ARM/ARMBaseRegisterInfo.cpp	(revision 365284)
+++ lib/Target/ARM/ARMBaseRegisterInfo.cpp	(working copy)
@@ -90,6 +90,10 @@
       // exception handling.
       return CSR_GenericInt_SaveList;
     }
+  } else if (F.getCallingConv() == CallingConv::V8CC) {
+    return CSR_V8CC_SaveList;
+  } else if (F.getCallingConv() == CallingConv::V8SBCC) {
+    return CSR_V8CC_SaveList;
   }
 
   if (STI.getTargetLowering()->supportSwiftError() &&
@@ -124,6 +128,12 @@
   if (CC == CallingConv::GHC)
     // This is academic because all GHC calls are (supposed to be) tail calls
     return CSR_NoRegs_RegMask;
+  if (CC == CallingConv::V8CC)
+    return CSR_V8CC_RegMask;
+  if (CC == CallingConv::V8SBCC)
+    return CSR_V8SBCC_RegMask;
+  if (CC == CallingConv::V8FPSave)
+    return CSR_V8FPSave_RegMask;
 
   if (STI.getTargetLowering()->supportSwiftError() &&
       MF.getFunction().getAttributes().hasAttrSomewhere(Attribute::SwiftError))
@@ -205,6 +215,19 @@
       if (Reserved.test(*SI))
         markSuperRegs(Reserved, Reg);
 
+  const Function &F = MF.getFunction();
+  if (F.getCallingConv() == CallingConv::V8SBCC) {
+    markSuperRegs(Reserved, ARM::R5);
+    markSuperRegs(Reserved, ARM::R6);
+    markSuperRegs(Reserved, ARM::R7);
+    markSuperRegs(Reserved, ARM::R8);
+    markSuperRegs(Reserved, ARM::R9);
+    markSuperRegs(Reserved, ARM::R10);
+    markSuperRegs(Reserved, ARM::R11);
+  } else if (F.getCallingConv() == CallingConv::V8CC) {
+    markSuperRegs(Reserved, ARM::R10);
+    markSuperRegs(Reserved, ARM::R11);
+  }
   assert(checkAllSuperRegsMarked(Reserved));
   return Reserved;
 }
@@ -369,6 +392,8 @@
   const ARMFunctionInfo *AFI = MF.getInfo<ARMFunctionInfo>();
   const ARMFrameLowering *TFI = getFrameLowering(MF);
 
+  if (MF.getFunction().getCallingConv() == CallingConv::V8CC)
+    return false;
   // When outgoing call frames are so large that we adjust the stack pointer
   // around the call, we can no longer use the stack pointer to reach the
   // emergency spill slot.
@@ -403,6 +428,8 @@
   // 2. There are VLAs in the function and the base pointer is disabled.
   if (!TargetRegisterInfo::canRealignStack(MF))
     return false;
+  if (MF.getFunction().getCallingConv() == CallingConv::V8CC)
+    return true;
   // Stack realignment requires a frame pointer.  If we already started
   // register allocation with frame pointer elimination, it is too late now.
   if (!MRI->canReserveReg(getFramePointerReg(MF.getSubtarget<ARMSubtarget>())))
@@ -862,3 +889,18 @@
   }
   return false;
 }
+
+TargetRegisterInfo::VirtRegToFixSlotMap
+ARMBaseRegisterInfo::getHoistToFixStackSlotMap(MachineFunction &MF) const {
+  const MachineRegisterInfo &MRI = MF.getRegInfo();
+  const ARMFunctionInfo *AFI = MF.getInfo<ARMFunctionInfo>();
+  MachineFrameInfo &MFI = MF.getFrameInfo();
+  VirtRegToFixSlotMap result;
+  for (auto &livein : MRI.liveins()) {
+    if (AFI->isWASM() && (livein.first == ARM::R3)) {
+      result.emplace_back(livein.second,
+                          MFI.CreateFixedSpillStackObject(4, -16));
+    }
+  }
+  return result;
+}
Index: lib/Target/ARM/ARMBaseRegisterInfo.h
===================================================================
--- lib/Target/ARM/ARMBaseRegisterInfo.h	(revision 365284)
+++ lib/Target/ARM/ARMBaseRegisterInfo.h	(working copy)
@@ -210,6 +210,8 @@
                       unsigned DstSubReg,
                       const TargetRegisterClass *NewRC,
                       LiveIntervals &LIS) const override;
+
+  VirtRegToFixSlotMap getHoistToFixStackSlotMap(MachineFunction &) const final;
 };
 
 } // end namespace llvm
Index: lib/Target/ARM/ARMCallingConv.td
===================================================================
--- lib/Target/ARM/ARMCallingConv.td	(revision 365284)
+++ lib/Target/ARM/ARMCallingConv.td	(working copy)
@@ -239,6 +239,60 @@
 ]>;
 
 //===----------------------------------------------------------------------===//
+// V8 Calling Conventions
+//===----------------------------------------------------------------------===//
+
+def CC_ARM_V8: CallingConv<[
+  CCIfType<[i1, i8, i16], CCPromoteToType<i32>>,
+
+  // i64/f64 is passed in even pairs of GPRs
+  // i64 is 8-aligned i32 here, so we may need to eat R1 as a pad register
+  // (and the same is true for f64 if VFP is not enabled)
+  CCIfType<[i32], CCIfAlign<"8", CCAssignToRegWithShadow<[R0, R2], [R0, R1]>>>,
+  CCIfType<[i32], CCIf<"ArgFlags.getOrigAlign() != 8",
+                       CCAssignToReg<[R0, R1, R2, R3, R4, R5, R6, R7, R8, R9, R10, R11]>>>,
+
+  CCIfType<[i32], CCAssignToStackWithShadow<4, 4, [R0, R1, R2, R3, R4, R5, R6, R7, R8, R9, R10, R11]>>,
+  CCIfType<[f32], CCAssignToReg<[S0, S1, S2, S3, S4, S5, S6, S7, S8,
+                                 S9, S10, S11, S12, S13, S14, S15]>>,
+  CCIfType<[f64], CCAssignToReg<[D0, D1, D2, D3, D4, D5, D6, D7]>>
+]>;
+
+def RetCC_ARM_V8: CallingConv<[
+  CCIfType<[i1, i8, i16], CCPromoteToType<i32>>,
+  CCIfType<[i32], CCAssignToReg<[R0, R1, R2, R3]>>,
+  CCIfType<[i64], CCAssignToRegWithShadow<[R0, R2], [R1, R3]>>,
+  CCIfType<[f64], CCAssignToReg<[D0, D1, D2, D3, D4, D5, D6, D7]>>,
+  CCIfType<[f32], CCAssignToReg<[S0, S1, S2, S3, S4, S5, S6, S7, S8,
+                                 S9, S10, S11, S12, S13, S14, S15]>>
+]>;
+
+def CC_ARM_V8SB: CallingConv<[
+  CCIfType<[i1, i8, i16], CCPromoteToType<i32>>,
+
+  // i64/f64 is passed in even pairs of GPRs
+  // i64 is 8-aligned i32 here, so we may need to eat R1 as a pad register
+  // (and the same is true for f64 if VFP is not enabled)
+  CCIfType<[i32], CCIfAlign<"8", CCAssignToRegWithShadow<[R0, R2], [R0, R1]>>>,
+  CCIfType<[i32], CCIf<"ArgFlags.getOrigAlign() != 8",
+                       CCAssignToReg<[R0, R1, R2, R3, R4, R10, R11, R12]>>>,
+
+  CCIfType<[i32], CCIfAlign<"8", CCAssignToStackWithShadow<4, 8, [R0, R1, R2, R3, R4, R10, R11, R12]>>>,
+  CCIfType<[i32], CCAssignToStackWithShadow<4, 4, [R0, R1, R2, R3, R4, R10, R11, R12]>>,
+  CCIfType<[f32], CCAssignToStackWithShadow<4, 4, [Q0, Q1, Q2, Q3]>>,
+  CCIfType<[f64], CCAssignToStackWithShadow<8, 8, [Q0, Q1, Q2, Q3]>>,
+  CCIfType<[v2f64], CCIfAlign<"16",
+           CCAssignToStackWithShadow<16, 16, [Q0, Q1, Q2, Q3]>>>,
+  CCIfType<[v2f64], CCAssignToStackWithShadow<16, 8, [Q0, Q1, Q2, Q3]>>
+]>;
+
+def RetCC_ARM_V8SB: CallingConv<[
+  CCIfType<[i1, i8, i16], CCPromoteToType<i32>>,
+  CCIfType<[i32], CCAssignToReg<[R0, R1, R2, R3]>>,
+  CCIfType<[i64], CCAssignToRegWithShadow<[R0, R2], [R1, R3]>>
+]>;
+
+//===----------------------------------------------------------------------===//
 // Callee-saved register lists.
 //===----------------------------------------------------------------------===//
 
@@ -248,6 +302,13 @@
 def CSR_AAPCS : CalleeSavedRegs<(add LR, R11, R10, R9, R8, R7, R6, R5, R4,
                                      (sequence "D%u", 15, 8))>;
 
+def CSR_V8CC : CalleeSavedRegs<(add LR, R11, R10)>;
+
+def CSR_V8FPSave: CalleeSavedRegs<(add LR, R11, R10, R9, R8, R7, R6, R5, R4,
+                                     (sequence "D%u", 31, 0))>;
+
+def CSR_V8SBCC : CalleeSavedRegs<(add LR, R11, R10, R9, R8, R7, R6, R5, (sequence "D%u", 31, 0))>;
+
 // R8 is used to pass swifterror, remove it from CSR.
 def CSR_AAPCS_SwiftError : CalleeSavedRegs<(sub CSR_AAPCS, R8)>;
 
Index: lib/Target/ARM/ARMConstantIslandPass.cpp
===================================================================
--- lib/Target/ARM/ARMConstantIslandPass.cpp	(revision 365284)
+++ lib/Target/ARM/ARMConstantIslandPass.cpp	(working copy)
@@ -241,6 +241,7 @@
     CPEntry *findConstPoolEntry(unsigned CPI, const MachineInstr *CPEMI);
     unsigned getCPELogAlign(const MachineInstr *CPEMI);
     void scanFunctionJumpTables();
+    bool replaceInlineAsm();
     void initializeFunctionInfo(const std::vector<MachineInstr*> &CPEMIs);
     MachineBasicBlock *splitBlockBeforeInstr(MachineInstr *MI);
     void updateForInsertedWaterBlock(MachineBasicBlock *NewBB);
@@ -367,7 +368,7 @@
 
   // Try to reorder and otherwise adjust the block layout to make good use
   // of the TB[BH] instructions.
-  bool MadeChange = false;
+  bool MadeChange = replaceInlineAsm();
   if (GenerateTBB && AdjustJumpTableBlocks) {
     scanFunctionJumpTables();
     MadeChange |= reorderThumb2JumpTables();
@@ -678,6 +679,49 @@
   }
 }
 
+bool ARMConstantIslands::replaceInlineAsm() {
+  bool MadeChange = false;
+  std::vector<MachineInstr *> toRemove;
+  for (MachineBasicBlock &MBB : *MF) {
+    for (MachineInstr &MI : MBB) {
+      if (MI.getOpcode() == TargetOpcode::INLINEASM) {
+        // handle const pool load
+        if (!strcmp(MI.getOperand(0).getSymbolName(), "ldr $0, =${1:c}")) {
+          MachineFunction *MF = MBB.getParent();
+          MachineConstantPool *ConstantPool = MF->getConstantPool();
+          Type *Int32Ty = Type::getInt32Ty(MF->getFunction().getContext());
+          const Constant *C =
+              ConstantInt::get(Int32Ty, MI.getOperand(5).getImm());
+
+          // MachineConstantPool wants an explicit alignment.
+          unsigned Align = MF->getDataLayout().getPrefTypeAlignment(Int32Ty);
+          if (Align == 0)
+            Align = MF->getDataLayout().getTypeAllocSize(C->getType());
+          unsigned Idx = ConstantPool->getConstantPoolIndex(C, Align);
+
+          if (isThumb)
+            BuildMI(MBB, MI, MI.getDebugLoc(), TII->get(ARM::tLDRpci))
+                .addReg(MI.getOperand(3).getReg(), RegState::Define)
+                .addConstantPoolIndex(Idx)
+                .add(predOps(ARMCC::AL));
+          else
+            BuildMI(MBB, MI, MI.getDebugLoc(), TII->get(ARM::LDRcp))
+                .addReg(MI.getOperand(3).getReg(), RegState::Define)
+                .addConstantPoolIndex(Idx)
+                .addImm(0)
+                .add(predOps(ARMCC::AL));
+          toRemove.push_back(&MI);
+          MadeChange = true;
+        }
+      }
+    }
+  }
+  for (MachineInstr *MI : toRemove) {
+    MI->eraseFromParent();
+  }
+  return MadeChange;
+}
+
 /// initializeFunctionInfo - Do the initial scan of the function, building up
 /// information about the sizes of each block, the location of all the water,
 /// and finding all of the constant pool users.
Index: lib/Target/ARM/ARMFastISel.cpp
===================================================================
--- lib/Target/ARM/ARMFastISel.cpp	(revision 365284)
+++ lib/Target/ARM/ARMFastISel.cpp	(working copy)
@@ -1890,6 +1890,10 @@
       report_fatal_error("Can't return in GHC call convention");
     else
       return CC_ARM_APCS_GHC;
+  case CallingConv::V8CC:
+    return (Return ? RetCC_ARM_V8 : CC_ARM_V8);
+  case CallingConv::V8SBCC:
+    return (Return ? RetCC_ARM_V8SB : CC_ARM_V8SB);
   }
 }
 
Index: lib/Target/ARM/ARMFrameLowering.cpp
===================================================================
--- lib/Target/ARM/ARMFrameLowering.cpp	(revision 365284)
+++ lib/Target/ARM/ARMFrameLowering.cpp	(working copy)
@@ -122,6 +122,9 @@
 /// included as part of the stack frame.
 bool ARMFrameLowering::hasReservedCallFrame(const MachineFunction &MF) const {
   const MachineFrameInfo &MFI = MF.getFrameInfo();
+  const ARMFunctionInfo *AFI = MF.getInfo<ARMFunctionInfo>();
+  if (AFI->isJSStub() || AFI->isJSFunction())
+    return false;
   unsigned CFSize = MFI.getMaxCallFrameSize();
   // It's not always a good idea to include the call frame as part of the
   // stack frame. ARM (especially Thumb) has small immediate offset to
@@ -351,9 +354,13 @@
 
 void ARMFrameLowering::emitPrologue(MachineFunction &MF,
                                     MachineBasicBlock &MBB) const {
+  ARMFunctionInfo *AFI = MF.getInfo<ARMFunctionInfo>();
+  if (AFI->isJSStub() || AFI->isJSFunction()) {
+    emitJSPrologue(MF, MBB);
+    return;
+  }
   MachineBasicBlock::iterator MBBI = MBB.begin();
   MachineFrameInfo  &MFI = MF.getFrameInfo();
-  ARMFunctionInfo *AFI = MF.getInfo<ARMFunctionInfo>();
   MachineModuleInfo &MMI = MF.getMMI();
   MCContext &Context = MMI.getContext();
   const TargetMachine &TM = MF.getTarget();
@@ -765,8 +772,12 @@
 
 void ARMFrameLowering::emitEpilogue(MachineFunction &MF,
                                     MachineBasicBlock &MBB) const {
+  ARMFunctionInfo *AFI = MF.getInfo<ARMFunctionInfo>();
+  if (AFI->isJSFunction() || AFI->isJSStub()) {
+    emitJSEpilogue(MF, MBB);
+    return;
+  }
   MachineFrameInfo &MFI = MF.getFrameInfo();
-  ARMFunctionInfo *AFI = MF.getInfo<ARMFunctionInfo>();
   const TargetRegisterInfo *RegInfo = MF.getSubtarget().getRegisterInfo();
   const ARMBaseInstrInfo &TII =
       *static_cast<const ARMBaseInstrInfo *>(MF.getSubtarget().getInstrInfo());
@@ -906,6 +917,9 @@
     if (isFixed) {
       FrameReg = RegInfo->getFrameRegister(MF);
       Offset = FPOffset;
+    } else if (hasMovingSP &&
+               MF.getFunction().getCallingConv() == CallingConv::V8CC) {
+      return Offset;
     } else if (hasMovingSP) {
       assert(RegInfo->hasBasePointer(MF) &&
              "VLAs and dynamic stack alignment, but missing base pointer!");
@@ -951,6 +965,9 @@
       // Otherwise, use SP or FP, whichever is closer to the stack slot.
       FrameReg = RegInfo->getFrameRegister(MF);
       return FPOffset;
+    } else if (AFI->isJSStub() || AFI->isJSFunction()) {
+      FrameReg = RegInfo->getFrameRegister(MF);
+      return FPOffset;
     }
   }
   // Use the base pointer if we have one.
@@ -1055,7 +1072,9 @@
   if (MBB.end() != MI) {
     DL = MI->getDebugLoc();
     unsigned RetOpcode = MI->getOpcode();
-    isTailCall = (RetOpcode == ARM::TCRETURNdi || RetOpcode == ARM::TCRETURNri);
+    isTailCall =
+        (RetOpcode == ARM::TCRETURNdi || RetOpcode == ARM::TCRETURNri ||
+         RetOpcode == TargetOpcode::TCPATCHPOINT);
     isInterrupt =
         RetOpcode == ARM::SUBS_PC_LR || RetOpcode == ARM::t2SUBS_PC_LR;
     isTrap =
@@ -1414,6 +1433,47 @@
   std::prev(MI)->addRegisterKilled(ARM::R4, TRI);
 }
 
+static inline bool isV8Area1Register(unsigned Reg, bool isIOS) {
+  using namespace ARM;
+
+  switch (Reg) {
+  case R0:
+  case R1:
+  case R7:
+  case R11:
+  case LR:
+  case SP:
+  case PC:
+    return true;
+  default:
+    return false;
+  }
+}
+
+static inline bool isV8Area2Register(unsigned Reg, bool isIOS) {
+  using namespace ARM;
+  switch (Reg) {
+  case R3:
+  case R12:
+    return true;
+  default:
+    return false;
+  }
+}
+
+static inline bool isV8Area3Register(unsigned Reg, bool isIOS) {
+  using namespace ARM;
+  switch (Reg) {
+  case R5:
+  case R6:
+  case R8:
+  case R9:
+    return true;
+  default:
+    return false;
+  }
+}
+
 bool ARMFrameLowering::spillCalleeSavedRegisters(MachineBasicBlock &MBB,
                                         MachineBasicBlock::iterator MI,
                                         const std::vector<CalleeSavedInfo> &CSI,
@@ -1429,10 +1489,19 @@
     ARM::t2STR_PRE : ARM::STR_PRE_IMM;
   unsigned FltOpc = ARM::VSTMDDB_UPD;
   unsigned NumAlignedDPRCS2Regs = AFI->getNumAlignedDPRCS2Regs();
-  emitPushInst(MBB, MI, CSI, PushOpc, PushOneOpc, false, &isARMArea1Register, 0,
-               MachineInstr::FrameSetup);
-  emitPushInst(MBB, MI, CSI, PushOpc, PushOneOpc, false, &isARMArea2Register, 0,
-               MachineInstr::FrameSetup);
+  if (AFI->isJSStub() || AFI->isJSFunction()) {
+    emitPushInst(MBB, MI, CSI, PushOpc, PushOneOpc, false, &isV8Area1Register,
+                 0, MachineInstr::FrameSetup);
+    emitPushInst(MBB, MI, CSI, PushOpc, PushOneOpc, false, &isV8Area2Register,
+                 0, MachineInstr::FrameSetup);
+    emitPushInst(MBB, MI, CSI, PushOpc, PushOneOpc, false, &isV8Area3Register,
+                 0, MachineInstr::FrameSetup);
+  } else {
+    emitPushInst(MBB, MI, CSI, PushOpc, PushOneOpc, false, &isARMArea1Register,
+                 0, MachineInstr::FrameSetup);
+    emitPushInst(MBB, MI, CSI, PushOpc, PushOneOpc, false, &isARMArea2Register,
+                 0, MachineInstr::FrameSetup);
+  }
   emitPushInst(MBB, MI, CSI, FltOpc, 0, true, &isARMArea3Register,
                NumAlignedDPRCS2Regs, MachineInstr::FrameSetup);
 
@@ -1461,16 +1530,23 @@
   // registers. Do that here instead.
   if (NumAlignedDPRCS2Regs)
     emitAlignedDPRCS2Restores(MBB, MI, NumAlignedDPRCS2Regs, CSI, TRI);
-
   unsigned PopOpc = AFI->isThumbFunction() ? ARM::t2LDMIA_UPD : ARM::LDMIA_UPD;
   unsigned LdrOpc = AFI->isThumbFunction() ? ARM::t2LDR_POST :ARM::LDR_POST_IMM;
   unsigned FltOpc = ARM::VLDMDIA_UPD;
   emitPopInst(MBB, MI, CSI, FltOpc, 0, isVarArg, true, &isARMArea3Register,
               NumAlignedDPRCS2Regs);
-  emitPopInst(MBB, MI, CSI, PopOpc, LdrOpc, isVarArg, false,
-              &isARMArea2Register, 0);
-  emitPopInst(MBB, MI, CSI, PopOpc, LdrOpc, isVarArg, false,
-              &isARMArea1Register, 0);
+  if (AFI->isJSStub() || AFI->isJSFunction()) {
+    emitPopInst(MBB, MI, CSI, PopOpc, LdrOpc, isVarArg, false,
+                &isV8Area3Register, 0);
+    // Just need one more pop as place holder.
+    emitPopInst(MBB, MI, CSI, PopOpc, LdrOpc, isVarArg, false,
+                &isV8Area1Register, 0);
+  } else {
+    emitPopInst(MBB, MI, CSI, PopOpc, LdrOpc, isVarArg, false,
+                &isARMArea2Register, 0);
+    emitPopInst(MBB, MI, CSI, PopOpc, LdrOpc, isVarArg, false,
+                &isARMArea1Register, 0);
+  }
 
   return true;
 }
@@ -1611,6 +1687,7 @@
   MachineRegisterInfo &MRI = MF.getRegInfo();
   const TargetRegisterInfo *TRI = MF.getSubtarget().getRegisterInfo();
   (void)TRI;  // Silence unused warning in non-assert builds.
+
   unsigned FramePtr = RegInfo->getFrameRegister(MF);
 
   // Spill R4 if Thumb2 function requires stack realignment - it will be used as
@@ -1724,6 +1801,21 @@
     }
   }
 
+  if (!CanEliminateFrame) {
+    if (MF.getFunction().getCallingConv() == CallingConv::V8CC) {
+      SavedRegs.set(ARM::R11);
+      SavedRegs.set(ARM::LR);
+    }
+
+    if (AFI->isJSFunction()) {
+      SavedRegs.set(ARM::R0);
+      SavedRegs.set(ARM::R1);
+      SavedRegs.set(ARM::R7);
+    } else if (AFI->isWASM()) {
+      SavedRegs.set(ARM::R3);
+    }
+  }
+
   bool ForceLRSpill = false;
   if (!LRSpilled && AFI->isThumb1OnlyFunction()) {
     unsigned FnSize = GetFunctionSizeInBytes(MF, TII);
@@ -1783,7 +1875,10 @@
     AFI->setHasStackFrame(true);
 
     if (HasFP) {
-      SavedRegs.set(FramePtr);
+      if (!SavedRegs.test(FramePtr)) {
+        SavedRegs.set(FramePtr);
+        NumGPRSpills++;
+      }
       // If the frame pointer is required by the ABI, also spill LR so that we
       // emit a complete frame record.
       if (MF.getTarget().Options.DisableFramePointerElim(MF) && !LRSpilled) {
@@ -1797,7 +1892,6 @@
       auto FPPos = llvm::find(UnspilledCS1GPRs, FramePtr);
       if (FPPos != UnspilledCS1GPRs.end())
         UnspilledCS1GPRs.erase(FPPos);
-      NumGPRSpills++;
       if (FramePtr == ARM::R7)
         CS1Spilled = true;
     }
@@ -2043,12 +2137,13 @@
     DebugLoc dl = Old.getDebugLoc();
     unsigned Amount = TII.getFrameSize(Old);
     if (Amount != 0) {
+      ARMFunctionInfo *AFI = MF.getInfo<ARMFunctionInfo>();
       // We need to keep the stack aligned properly.  To do this, we round the
       // amount of space needed for the outgoing arguments up to the next
       // alignment boundary.
-      Amount = alignSPAdjust(Amount);
+      if (!AFI->isJSFunction() && !AFI->isJSStub())
+        Amount = alignSPAdjust(Amount);
 
-      ARMFunctionInfo *AFI = MF.getInfo<ARMFunctionInfo>();
       assert(!AFI->isThumb1OnlyFunction() &&
              "This eliminateCallFramePseudoInstr does not support Thumb1!");
       bool isARM = !AFI->isThumbFunction();
@@ -2063,7 +2158,7 @@
       if (Opc == ARM::ADJCALLSTACKDOWN || Opc == ARM::tADJCALLSTACKDOWN) {
         emitSPUpdate(isARM, MBB, I, dl, TII, -Amount, MachineInstr::NoFlags,
                      Pred, PredReg);
-      } else {
+      } else if (!AFI->isJSFunction() && !AFI->isJSStub()) {
         assert(Opc == ARM::ADJCALLSTACKUP || Opc == ARM::tADJCALLSTACKUP);
         emitSPUpdate(isARM, MBB, I, dl, TII, Amount, MachineInstr::NoFlags,
                      Pred, PredReg);
@@ -2511,3 +2606,469 @@
   MF.verify();
 #endif
 }
+
+bool ARMFrameLowering::assignCalleeSavedSpillSlots(
+    MachineFunction &MF, const TargetRegisterInfo *TRI,
+    std::vector<CalleeSavedInfo> &CSI) const {
+  ARMFunctionInfo *AFI = MF.getInfo<ARMFunctionInfo>();
+  // default handle.
+  if (!AFI->isJSStub() && !AFI->isJSFunction())
+    return false;
+  MachineFrameInfo &MFI = MF.getFrameInfo();
+  // No need to do anything.
+  if (CSI.empty())
+    return true;
+  int FixedOffset;
+  if (AFI->isJSStub()) {
+    MFI.CreateFixedSpillStackObject(4, -12);
+    FixedOffset = -12;
+    if (AFI->isWASM()) {
+      // wasm instance for v8.
+      CSI.emplace_back(ARM::R3);
+      CSI.back().setFrameIdx(MFI.CreateFixedSpillStackObject(4, -16));
+      FixedOffset = -16;
+    }
+  } else {
+    int CPFI = MFI.CreateFixedSpillStackObject(4, -12);  // cp
+    int FunFI = MFI.CreateFixedSpillStackObject(4, -16); // function
+    int ArgFI = MFI.CreateFixedSpillStackObject(4, -20); // arg count.
+    CSI.emplace_back(ARM::R7);
+    CSI.back().setFrameIdx(CPFI);
+    CSI.emplace_back(ARM::R1);
+    CSI.back().setFrameIdx(FunFI);
+    CSI.emplace_back(ARM::R0);
+    CSI.back().setFrameIdx(ArgFI);
+    FixedOffset = -20;
+  }
+
+  for (auto &i : CSI) {
+    if (i.getReg() == ARM::LR) {
+      i.setFrameIdx(MFI.CreateFixedSpillStackObject(4, -4));
+    } else if (i.getReg() == ARM::R11) {
+      i.setFrameIdx(MFI.CreateFixedSpillStackObject(4, -8));
+    } else if (i.getReg() >= ARM::R5 && i.getReg() < ARM::R9) {
+      FixedOffset -= 4;
+      i.setFrameIdx(MFI.CreateFixedSpillStackObject(4, FixedOffset));
+    }
+  }
+
+  unsigned Align = STI.getFrameLowering()->getStackAlignment();
+  FixedOffset &= ~(Align - 1);
+  for (auto &i : CSI) {
+    if (i.getReg() >= ARM::D0 && i.getReg() <= ARM::D31) {
+      FixedOffset -= 8;
+      i.setFrameIdx(MFI.CreateFixedSpillStackObject(8, FixedOffset));
+    }
+  }
+  return true;
+}
+
+void ARMFrameLowering::emitJSPrologue(MachineFunction &MF,
+                                      MachineBasicBlock &MBB) const {
+  ARMFunctionInfo *AFI = MF.getInfo<ARMFunctionInfo>();
+  MachineBasicBlock::iterator MBBI = MBB.begin();
+  MachineFrameInfo &MFI = MF.getFrameInfo();
+  MachineModuleInfo &MMI = MF.getMMI();
+  MCContext &Context = MMI.getContext();
+  const MCRegisterInfo *MRI = Context.getRegisterInfo();
+  const ARMBaseInstrInfo &TII = *STI.getInstrInfo();
+  bool isARM = !AFI->isThumbFunction();
+  unsigned Align = STI.getFrameLowering()->getStackAlignment();
+  unsigned NumBytes = MFI.getStackSize();
+  const std::vector<CalleeSavedInfo> &CSI = MFI.getCalleeSavedInfo();
+  const ARMBaseRegisterInfo *RegInfo = STI.getRegisterInfo();
+  // Debug location must be unknown since the first debug location is used
+  // to determine the end of the prologue.
+  DebugLoc dl;
+
+  unsigned FramePtr = ARM::R11;
+  // Determine the sizes of each callee-save spill areas and record which frame
+  // belongs to which callee-save spill areas.
+  unsigned GPRCS1Size = 0, GPRCS2Size = 0, GPRCS3Size = 0, DPRCSSize = 0;
+  int FramePtrSpillFI = 0;
+  int D8SpillFI = 0;
+
+  if (AFI->isJSStub()) {
+    if (MF.getRegInfo().isLiveIn(ARM::R9)) {
+      MachineBasicBlock &MBB = MF.front();
+      BuildMI(MBB, MBB.instr_begin(), dl, TII.get(ARM::MOVr), ARM::R9)
+          .addReg(ARM::R11)
+          .add(predOps(ARMCC::AL))
+          .add(condCodeOp());
+    }
+    // Push LR as marker.
+    GPRCS1Size = 4;
+  }
+
+  StackAdjustingInsts DefCFAOffsetCandidates;
+  bool HasFP = hasFP(MF);
+
+  if (!AFI->hasStackFrame()) {
+    if (NumBytes != 0) {
+      emitSPUpdate(isARM, MBB, MBBI, dl, TII, -NumBytes,
+                   MachineInstr::FrameSetup);
+      DefCFAOffsetCandidates.addInst(std::prev(MBBI), NumBytes, true);
+    }
+    DefCFAOffsetCandidates.emitDefCFAOffsets(MBB, dl, TII, HasFP);
+    return;
+  }
+
+  // Determine spill area sizes.
+  for (unsigned i = 0, e = CSI.size(); i != e; ++i) {
+    unsigned Reg = CSI[i].getReg();
+    int FI = CSI[i].getFrameIdx();
+    switch (Reg) {
+    case ARM::R3:
+      GPRCS2Size += 4;
+      break;
+    case ARM::R0:
+    case ARM::R1:
+    case ARM::R7:
+    case ARM::R11:
+    case ARM::LR:
+      if (Reg == ARM::R11)
+        FramePtrSpillFI = FI;
+      GPRCS1Size += 4;
+      break;
+    case ARM::R5:
+    case ARM::R6:
+    case ARM::R8:
+    case ARM::R9:
+      GPRCS3Size += 4;
+      break;
+    default:
+      // This is a DPR. Exclude the aligned DPRCS2 spills.
+      if (Reg == ARM::D8)
+        D8SpillFI = FI;
+      if ((Reg <= ARM::D31) && (Reg >= ARM::D0))
+        DPRCSSize += 8;
+    }
+  }
+
+  // Move past area 1.
+  MachineBasicBlock::iterator LastPush = MBB.end(), GPRCS1Push, GPRCS2Push,
+                              GPRCS3Push;
+  if (GPRCS1Size > 0) {
+    GPRCS1Push = LastPush = MBBI++;
+    DefCFAOffsetCandidates.addInst(LastPush, GPRCS1Size, true);
+  }
+
+  // Determine starting offsets of spill areas.
+  unsigned GPRCS1Offset = NumBytes - GPRCS1Size;
+  unsigned GPRCS2Offset = GPRCS1Offset - GPRCS2Size;
+  unsigned GPRCS3Offset = GPRCS2Offset - GPRCS3Size;
+  unsigned DPRAlign = DPRCSSize ? std::min(8U, Align) : 4U;
+  unsigned DPRGapSize = (GPRCS1Size + GPRCS2Size + GPRCS3Size) % DPRAlign;
+  unsigned DPRCSOffset = GPRCS3Offset - DPRGapSize - DPRCSSize;
+  int FramePtrOffsetInPush = 0;
+  if (FramePtrSpillFI) {
+    int FPOffset = MFI.getObjectOffset(FramePtrSpillFI);
+    assert(getMaxFPOffset(MF.getFunction(), *AFI) <= FPOffset &&
+           "Max FP estimation is wrong");
+    FramePtrOffsetInPush = FPOffset;
+    AFI->setFramePtrSpillOffset(MFI.getObjectOffset(FramePtrSpillFI) +
+                                NumBytes);
+  }
+  AFI->setGPRCalleeSavedArea1Offset(GPRCS1Offset);
+  AFI->setGPRCalleeSavedArea2Offset(GPRCS2Offset);
+  AFI->setDPRCalleeSavedAreaOffset(DPRCSOffset);
+
+  // Move past area 2.
+  if (GPRCS2Size > 0) {
+    GPRCS2Push = LastPush = MBBI++;
+    DefCFAOffsetCandidates.addInst(LastPush, GPRCS2Size);
+  }
+
+  if (GPRCS3Size > 0) {
+    GPRCS3Push = LastPush = MBBI++;
+    DefCFAOffsetCandidates.addInst(LastPush, GPRCS3Size);
+  }
+
+  // Prolog/epilog inserter assumes we correctly align DPRs on the stack, so our
+  // .cfi_offset operations will reflect that.
+  if (DPRGapSize) {
+    assert(DPRGapSize == 4 && "unexpected alignment requirements for DPRs");
+    emitSPUpdate(isARM, MBB, MBBI, dl, TII, -DPRGapSize,
+                 MachineInstr::FrameSetup);
+    DefCFAOffsetCandidates.addInst(std::prev(MBBI), DPRGapSize);
+  }
+
+  // Move past area 3.
+  if (DPRCSSize > 0) {
+    // Since vpush register list cannot have gaps, there may be multiple vpush
+    // instructions in the prologue.
+    while (MBBI != MBB.end() && MBBI->getOpcode() == ARM::VSTMDDB_UPD) {
+      DefCFAOffsetCandidates.addInst(MBBI, sizeOfSPAdjustment(*MBBI));
+      LastPush = MBBI++;
+    }
+  }
+
+  // Move past the aligned DPRCS2 area.
+  if (AFI->getNumAlignedDPRCS2Regs() > 0) {
+    MBBI = skipAlignedDPRCS2Spills(MBBI, AFI->getNumAlignedDPRCS2Regs());
+    // The code inserted by emitAlignedDPRCS2Spills realigns the stack, and
+    // leaves the stack pointer pointing to the DPRCS2 area.
+    //
+    // Adjust NumBytes to represent the stack slots below the DPRCS2 area.
+    NumBytes += MFI.getObjectOffset(D8SpillFI);
+  } else
+    NumBytes = DPRCSOffset;
+
+  if (NumBytes) {
+    emitSPUpdate(isARM, MBB, MBBI, dl, TII, -NumBytes,
+                 MachineInstr::FrameSetup);
+    DefCFAOffsetCandidates.addInst(std::prev(MBBI), NumBytes);
+
+    if (HasFP && isARM)
+      // Restore from fp only in ARM mode: e.g. sub sp, r7, #24
+      // Note it's not safe to do this in Thumb2 mode because it would have
+      // taken two instructions:
+      // mov sp, r7
+      // sub sp, #24
+      // If an interrupt is taken between the two instructions, then sp is in
+      // an inconsistent state (pointing to the middle of callee-saved area).
+      // The interrupt handler can end up clobbering the registers.
+      AFI->setShouldRestoreSPFromFP(true);
+  }
+
+  // Set FP to point to the stack slot that contains the previous FP.
+  // For iOS, FP is R7, which has now been stored in spill area 1.
+  // Otherwise, if this is not iOS, all the callee-saved registers go
+  // into spill area 1, including the FP in R11.  In either case, it
+  // is in area one and the adjustment needs to take place just after
+  // that push.
+  MachineBasicBlock::iterator AfterPush = std::next(GPRCS1Push);
+  unsigned PushSize = sizeOfSPAdjustment(*GPRCS1Push);
+  emitRegPlusImmediate(!AFI->isThumbFunction(), MBB, AfterPush, dl, TII,
+                       FramePtr, ARM::SP, PushSize + FramePtrOffsetInPush,
+                       MachineInstr::FrameSetup);
+  if (FramePtrOffsetInPush + PushSize != 0) {
+    unsigned CFIIndex = MF.addFrameInst(MCCFIInstruction::createDefCfa(
+        nullptr, MRI->getDwarfRegNum(FramePtr, true), FramePtrOffsetInPush));
+    BuildMI(MBB, AfterPush, dl, TII.get(TargetOpcode::CFI_INSTRUCTION))
+        .addCFIIndex(CFIIndex)
+        .setMIFlags(MachineInstr::FrameSetup);
+  } else {
+    unsigned CFIIndex = MF.addFrameInst(MCCFIInstruction::createDefCfaRegister(
+        nullptr, MRI->getDwarfRegNum(FramePtr, true)));
+    BuildMI(MBB, AfterPush, dl, TII.get(TargetOpcode::CFI_INSTRUCTION))
+        .addCFIIndex(CFIIndex)
+        .setMIFlags(MachineInstr::FrameSetup);
+  }
+  // Push marker
+  if (AFI->isJSStub()) {
+    int Marker;
+    MF.getFunction()
+        .getFnAttribute("js-stub-call")
+        .getValueAsString()
+        .getAsInteger(10, Marker);
+    BuildMI(MBB, AfterPush, dl, TII.get(ARM::MOVi), ARM::LR)
+        .addImm(Marker)
+        .add(predOps(ARMCC::AL))
+        .add(condCodeOp());
+    BuildMI(MBB, AfterPush, dl, TII.get(ARM::STR_PRE_IMM), ARM::SP)
+        .addReg(ARM::LR, RegState::Kill)
+        .addReg(ARM::SP)
+        .setMIFlags(MachineInstr::NoFlags)
+        .addImm(-4)
+        .add(predOps(ARMCC::AL));
+  }
+
+  // Now that the prologue's actual instructions are finalised, we can insert
+  // the necessary DWARF cf instructions to describe the situation. Start by
+  // recording where each register ended up:
+  if (GPRCS1Size > 0) {
+    MachineBasicBlock::iterator Pos = std::next(GPRCS1Push);
+    int CFIIndex;
+    for (const auto &Entry : CSI) {
+      unsigned Reg = Entry.getReg();
+      int FI = Entry.getFrameIdx();
+      switch (Reg) {
+      case ARM::R0:
+      case ARM::R1:
+      case ARM::R7:
+      case ARM::LR:
+        CFIIndex = MF.addFrameInst(MCCFIInstruction::createOffset(
+            nullptr, MRI->getDwarfRegNum(Reg, true), MFI.getObjectOffset(FI)));
+        BuildMI(MBB, Pos, dl, TII.get(TargetOpcode::CFI_INSTRUCTION))
+            .addCFIIndex(CFIIndex)
+            .setMIFlags(MachineInstr::FrameSetup);
+        break;
+      }
+    }
+  }
+
+  if (GPRCS2Size > 0) {
+    MachineBasicBlock::iterator Pos = std::next(GPRCS2Push);
+    for (const auto &Entry : CSI) {
+      unsigned Reg = Entry.getReg();
+      int FI = Entry.getFrameIdx();
+      switch (Reg) {
+      case ARM::R3:
+        unsigned DwarfReg = MRI->getDwarfRegNum(Reg, true);
+        unsigned Offset = MFI.getObjectOffset(FI);
+        unsigned CFIIndex = MF.addFrameInst(
+            MCCFIInstruction::createOffset(nullptr, DwarfReg, Offset));
+        BuildMI(MBB, Pos, dl, TII.get(TargetOpcode::CFI_INSTRUCTION))
+            .addCFIIndex(CFIIndex)
+            .setMIFlags(MachineInstr::FrameSetup);
+        break;
+      }
+    }
+  }
+
+  if (GPRCS3Size > 0) {
+    MachineBasicBlock::iterator Pos = std::next(GPRCS3Push);
+    for (const auto &Entry : CSI) {
+      unsigned Reg = Entry.getReg();
+      int FI = Entry.getFrameIdx();
+      switch (Reg) {
+      case ARM::R5:
+      case ARM::R6:
+      case ARM::R8:
+      case ARM::R9:
+        unsigned DwarfReg = MRI->getDwarfRegNum(Reg, true);
+        unsigned Offset = MFI.getObjectOffset(FI);
+        unsigned CFIIndex = MF.addFrameInst(
+            MCCFIInstruction::createOffset(nullptr, DwarfReg, Offset));
+        BuildMI(MBB, Pos, dl, TII.get(TargetOpcode::CFI_INSTRUCTION))
+            .addCFIIndex(CFIIndex)
+            .setMIFlags(MachineInstr::FrameSetup);
+        break;
+      }
+    }
+  }
+
+  if (DPRCSSize > 0) {
+    // Since vpush register list cannot have gaps, there may be multiple vpush
+    // instructions in the prologue.
+    MachineBasicBlock::iterator Pos = std::next(LastPush);
+    for (const auto &Entry : CSI) {
+      unsigned Reg = Entry.getReg();
+      int FI = Entry.getFrameIdx();
+      if ((Reg >= ARM::D0 && Reg <= ARM::D31) &&
+          (Reg < ARM::D8 || Reg >= ARM::D8 + AFI->getNumAlignedDPRCS2Regs())) {
+        unsigned DwarfReg = MRI->getDwarfRegNum(Reg, true);
+        unsigned Offset = MFI.getObjectOffset(FI);
+        unsigned CFIIndex = MF.addFrameInst(
+            MCCFIInstruction::createOffset(nullptr, DwarfReg, Offset));
+        BuildMI(MBB, Pos, dl, TII.get(TargetOpcode::CFI_INSTRUCTION))
+            .addCFIIndex(CFIIndex)
+            .setMIFlags(MachineInstr::FrameSetup);
+      }
+    }
+  }
+
+  // Now we can emit descriptions of where the canonical frame address was
+  // throughout the process. If we have a frame pointer, it takes over the job
+  // half-way through, so only the first few .cfi_def_cfa_offset instructions
+  // actually get emitted.
+  DefCFAOffsetCandidates.emitDefCFAOffsets(MBB, dl, TII, HasFP);
+
+  if (STI.isTargetELF() && hasFP(MF))
+    MFI.setOffsetAdjustment(MFI.getOffsetAdjustment() -
+                            AFI->getFramePtrSpillOffset());
+
+  AFI->setGPRCalleeSavedArea1Size(GPRCS1Size);
+  AFI->setGPRCalleeSavedArea2Size(GPRCS2Size);
+  AFI->setGPRCalleeSavedArea3Size(GPRCS3Size);
+  AFI->setDPRCalleeSavedGapSize(DPRGapSize);
+  AFI->setDPRCalleeSavedAreaSize(DPRCSSize);
+
+  // If we need dynamic stack realignment, do it here. Be paranoid and make
+  // sure if we also have VLAs, we have a base pointer for frame access.
+  // If aligned NEON registers were spilled, the stack has already been
+  // realigned.
+  if (RegInfo->needsStackRealignment(MF)) {
+    unsigned MaxAlign = MFI.getMaxAlignment();
+    assert(!AFI->isThumbFunction());
+    emitAligningInstructions(MF, AFI, TII, MBB, MBBI, dl, ARM::SP, MaxAlign,
+                             false);
+    AFI->setShouldRestoreSPFromFP(true);
+  }
+
+  assert(!MFI.hasVarSizedObjects());
+}
+
+void ARMFrameLowering::emitJSEpilogue(MachineFunction &MF,
+                                      MachineBasicBlock &MBB) const {
+  ARMFunctionInfo *AFI = MF.getInfo<ARMFunctionInfo>();
+  MachineFrameInfo &MFI = MF.getFrameInfo();
+  const ARMBaseInstrInfo &TII =
+      *static_cast<const ARMBaseInstrInfo *>(MF.getSubtarget().getInstrInfo());
+  assert(!AFI->isThumb1OnlyFunction() &&
+         "This emitEpilogue does not support Thumb1!");
+  bool isARM = !AFI->isThumbFunction();
+  assert(isARM);
+
+  int NumBytes = (int)MFI.getStackSize();
+  unsigned FramePtr = ARM::R11;
+
+  // First put ourselves on the first (from top) terminator instructions.
+  MachineBasicBlock::iterator MBBI = MBB.getFirstTerminator();
+  DebugLoc dl = MBBI != MBB.end() ? MBBI->getDebugLoc() : DebugLoc();
+
+  if (!AFI->hasStackFrame()) {
+    if (NumBytes != 0)
+      emitSPUpdate(isARM, MBB, MBBI, dl, TII, NumBytes);
+  } else {
+    // Unwind MBBI to point to first LDR / VLDRD.
+    if (MBBI != MBB.begin()) {
+      do {
+        --MBBI;
+      } while (MBBI != MBB.begin() && isPopOpcode(MBBI->getOpcode()));
+      if (!isPopOpcode(MBBI->getOpcode()))
+        ++MBBI;
+    }
+
+    // Move SP to start of FP callee save spill area.
+    NumBytes -=
+        (AFI->getGPRCalleeSavedArea1Size() + AFI->getGPRCalleeSavedArea2Size() +
+         AFI->getGPRCalleeSavedArea3Size() + AFI->getDPRCalleeSavedGapSize() +
+         AFI->getDPRCalleeSavedAreaSize());
+
+    if (AFI->getDPRCalleeSavedAreaSize() + AFI->getGPRCalleeSavedArea3Size()) {
+      // Reset SP based on frame pointer only if the stack frame extends beyond
+      // frame pointer stack slot or target is ELF and the function has FP.
+      if (AFI->shouldRestoreSPFromFP()) {
+        NumBytes = AFI->getFramePtrSpillOffset() - NumBytes;
+        if (NumBytes) {
+          emitARMRegPlusImmediate(MBB, MBBI, dl, ARM::SP, FramePtr, -NumBytes,
+                                  ARMCC::AL, 0, TII);
+        } else {
+          // Thumb2 or ARM.
+          BuildMI(MBB, MBBI, dl, TII.get(ARM::MOVr), ARM::SP)
+              .addReg(FramePtr)
+              .add(predOps(ARMCC::AL))
+              .add(condCodeOp());
+        }
+      } else if (NumBytes)
+        emitSPUpdate(isARM, MBB, MBBI, dl, TII, NumBytes);
+    }
+
+    // Increment past our save areas.
+    if (MBBI != MBB.end() && AFI->getDPRCalleeSavedAreaSize()) {
+      MBBI++;
+      // Since vpop register list cannot have gaps, there may be multiple vpop
+      // instructions in the epilogue.
+      while (MBBI != MBB.end() && MBBI->getOpcode() == ARM::VLDMDIA_UPD)
+        MBBI++;
+    }
+    // Skip one more MI when area 3 exists.
+    if (AFI->getGPRCalleeSavedArea3Size()) {
+      MBBI++;
+    }
+
+    BuildMI(MBB, MBBI, dl, TII.get(ARM::MOVr), ARM::SP)
+        .addReg(FramePtr)
+        .add(predOps(ARMCC::AL))
+        .add(condCodeOp());
+    MachineInstrBuilder MIB =
+        BuildMI(MBB, MBBI, dl, TII.get(ARM::LDMIA_UPD), ARM::SP)
+            .addReg(ARM::SP)
+            .add(predOps(ARMCC::AL));
+    MIB.addReg(ARM::R11, getDefRegState(true));
+    MIB.addReg(ARM::LR, getDefRegState(true));
+    MBBI->eraseFromParent();
+  }
+}
Index: lib/Target/ARM/ARMFrameLowering.h
===================================================================
--- lib/Target/ARM/ARMFrameLowering.h	(revision 365284)
+++ lib/Target/ARM/ARMFrameLowering.h	(working copy)
@@ -31,6 +31,8 @@
   /// the function.
   void emitPrologue(MachineFunction &MF, MachineBasicBlock &MBB) const override;
   void emitEpilogue(MachineFunction &MF, MachineBasicBlock &MBB) const override;
+  void emitJSPrologue(MachineFunction &MF, MachineBasicBlock &MBB) const;
+  void emitJSEpilogue(MachineFunction &MF, MachineBasicBlock &MBB) const;
 
   bool spillCalleeSavedRegisters(MachineBasicBlock &MBB,
                                  MachineBasicBlock::iterator MI,
@@ -65,6 +67,11 @@
     return true;
   }
 
+  bool
+  assignCalleeSavedSpillSlots(MachineFunction &MF,
+                              const TargetRegisterInfo *TRI,
+                              std::vector<CalleeSavedInfo> &CSI) const override;
+
 private:
   void emitPushInst(MachineBasicBlock &MBB, MachineBasicBlock::iterator MI,
                     const std::vector<CalleeSavedInfo> &CSI, unsigned StmOpc,
Index: lib/Target/ARM/ARMISelDAGToDAG.cpp
===================================================================
--- lib/Target/ARM/ARMISelDAGToDAG.cpp	(revision 365284)
+++ lib/Target/ARM/ARMISelDAGToDAG.cpp	(working copy)
@@ -13,6 +13,7 @@
 
 #include "ARM.h"
 #include "ARMBaseInstrInfo.h"
+#include "ARMMachineFunctionInfo.h"
 #include "ARMTargetMachine.h"
 #include "MCTargetDesc/ARMAddressingModes.h"
 #include "Utils/ARMBaseInfo.h"
@@ -60,6 +61,15 @@
       : SelectionDAGISel(tm, OptLevel) {}
 
   bool runOnMachineFunction(MachineFunction &MF) override {
+    const Function &F = MF.getFunction();
+    ARMFunctionInfo *AFI = MF.getInfo<ARMFunctionInfo>();
+    if (F.hasFnAttribute("js-function-call"))
+      AFI->setJSFunction(true);
+    else if (F.hasFnAttribute("js-stub-call"))
+      AFI->setJSStub(true);
+    if (F.hasFnAttribute("js-wasm-call"))
+      AFI->setWASM(true);
+
     // Reset the subtarget each time through.
     Subtarget = &MF.getSubtarget<ARMSubtarget>();
     SelectionDAGISel::runOnMachineFunction(MF);
Index: lib/Target/ARM/ARMISelLowering.cpp
===================================================================
--- lib/Target/ARM/ARMISelLowering.cpp	(revision 365284)
+++ lib/Target/ARM/ARMISelLowering.cpp	(working copy)
@@ -1601,6 +1601,10 @@
   switch (CC) {
   default:
     report_fatal_error("Unsupported calling convention");
+  case CallingConv::V8CC:
+    return CallingConv::V8CC;
+  case CallingConv::V8SBCC:
+    return CallingConv::V8SBCC;
   case CallingConv::ARM_AAPCS:
   case CallingConv::ARM_APCS:
   case CallingConv::GHC:
@@ -1650,6 +1654,10 @@
   switch (getEffectiveCallingConv(CC, isVarArg)) {
   default:
     report_fatal_error("Unsupported calling convention");
+  case CallingConv::V8SBCC:
+    return (Return ? RetCC_ARM_V8SB : CC_ARM_V8SB);
+  case CallingConv::V8CC:
+    return (Return ? RetCC_ARM_V8 : CC_ARM_V8);
   case CallingConv::ARM_APCS:
     return (Return ? RetCC_ARM_APCS : CC_ARM_APCS);
   case CallingConv::ARM_AAPCS:
@@ -1810,6 +1818,10 @@
   bool isStructRet    = (Outs.empty()) ? false : Outs[0].Flags.isSRet();
   bool isThisReturn   = false;
   bool isSibCall      = false;
+  bool hasJSCCall     = false;
+  CallingConv::ID CallerCC = MF.getFunction().getCallingConv();
+  bool CallerIsJS =
+      ((CallerCC == CallingConv::V8CC) || (CallerCC == CallingConv::V8SBCC));
   auto Attr = MF.getFunction().getFnAttribute("disable-tail-calls");
 
   // Disable tail calls if they're not supported.
@@ -1818,9 +1830,10 @@
 
   if (isTailCall) {
     // Check if it's really possible to do a tail call.
-    isTailCall = IsEligibleForTailCallOptimization(Callee, CallConv,
-                    isVarArg, isStructRet, MF.getFunction().hasStructRetAttr(),
-                                                   Outs, OutVals, Ins, DAG);
+    isTailCall = CallerIsJS || IsEligibleForTailCallOptimization(
+                                   Callee, CallConv, isVarArg, isStructRet,
+                                   MF.getFunction().hasStructRetAttr(), Outs,
+                                   OutVals, Ins, DAG);
     if (!isTailCall && CLI.CS && CLI.CS.isMustTailCall())
       report_fatal_error("failed to perform tail call elimination on a call "
                          "site marked musttail");
@@ -1849,8 +1862,77 @@
   // These operations are automatically eliminated by the prolog/epilog pass
   if (!isSibCall)
     Chain = DAG.getCALLSEQ_START(Chain, NumBytes, 0, dl);
+  if (CLI.CS.isCall()) {
+    if (CallerIsJS && (CLI.CallConv == CallingConv::C)) {
+      hasJSCCall = true;
+    }
+  }
 
-  SDValue StackPtr =
+  // Add a register mask operand representing the call-preserved registers.
+  bool IsCCallInsideJSSaveFP = [&]() {
+    if (CallerCC != CallingConv::V8SBCC)
+      return false;
+    if (!hasJSCCall)
+      return false;
+    const CallInst *Call = dyn_cast<CallInst>(CLI.CS.getInstruction());
+    if (!Call)
+      return false;
+    return Call->hasFnAttr("save-fp");
+  }();
+  SDValue FPSaveArea0, FPSaveArea1, CCAL, CCR;
+  // Save the FP.
+  if (IsCCallInsideJSSaveFP) {
+    MachineFrameInfo &MFI = MF.getFrameInfo();
+    int FIFPSaveArea0, FIFPSaveArea1;
+    ARMFunctionInfo *AFI = MF.getInfo<ARMFunctionInfo>();
+    FIFPSaveArea0 = AFI->getFIFPSaveArea0();
+    if (FIFPSaveArea0 == -1) {
+      FIFPSaveArea0 = MFI.CreateStackObject(8 * 16, 4, false);
+      FIFPSaveArea1 = MFI.CreateStackObject(8 * 8, 4, false);
+      AFI->setFIFPSaveArea0(FIFPSaveArea0);
+      AFI->setFIFPSaveArea1(FIFPSaveArea1);
+    }
+    FIFPSaveArea1 = AFI->getFIFPSaveArea1();
+    FPSaveArea0 =
+        DAG.getFrameIndex(FIFPSaveArea0, getFrameIndexTy(DAG.getDataLayout()));
+    FPSaveArea1 =
+        DAG.getFrameIndex(FIFPSaveArea1, getFrameIndexTy(DAG.getDataLayout()));
+    CCAL = DAG.getConstant(ARMCC::AL, dl, MVT::i32, true);
+    CCR = DAG.getRegister(0, MVT::i32);
+    std::vector<SDValue> Ops;
+    Ops.emplace_back(FPSaveArea0);
+    Ops.emplace_back(CCAL);
+    Ops.emplace_back(CCR);
+    for (int i = 16; i < 32; ++i)
+      Ops.emplace_back(DAG.getRegister(ARM::D0 + i, MVT::i64));
+    Ops.emplace_back(Chain);
+    Chain = SDValue(DAG.getMachineNode(ARM::VSTMDIA, dl, MVT::Other, Ops), 0);
+    Ops.clear();
+    Ops.emplace_back(FPSaveArea1);
+    Ops.emplace_back(CCAL);
+    Ops.emplace_back(CCR);
+    for (int i = 0; i < 8; ++i)
+      Ops.emplace_back(DAG.getRegister(ARM::D0 + i, MVT::i64));
+    Ops.emplace_back(Chain);
+    Chain = SDValue(DAG.getMachineNode(ARM::VSTMDIA, dl, MVT::Other, Ops), 0);
+  }
+  SDValue StackPtr;
+  if (hasJSCCall) {
+    SDValue OldStackPtr = DAG.getCopyFromReg(Chain, dl, ARM::SP,
+                                             getPointerTy(DAG.getDataLayout()));
+    Chain = OldStackPtr.getValue(1);
+    SDValue InFlag;
+    SDValue NewStackPtr = DAG.getNode(
+        ISD::AND, dl, OldStackPtr.getValueType(), OldStackPtr.getValue(0),
+        DAG.getConstant(-8, dl, OldStackPtr.getValueType()));
+    Chain =
+        DAG.getCopyToReg(Chain, dl, ARM::SP, NewStackPtr.getValue(0), InFlag);
+    MachineFunction &MF = DAG.getMachineFunction();
+    MachineFrameInfo &MFI = MF.getFrameInfo();
+    MFI.setFrameAddressIsTaken(true);
+  }
+
+  StackPtr =
       DAG.getCopyFromReg(Chain, dl, ARM::SP, getPointerTy(DAG.getDataLayout()));
 
   RegsToPassVector RegsToPass;
@@ -2159,7 +2241,6 @@
     Ops.push_back(DAG.getRegister(RegsToPass[i].first,
                                   RegsToPass[i].second.getValueType()));
 
-  // Add a register mask operand representing the call-preserved registers.
   if (!isTailCall) {
     const uint32_t *Mask;
     const ARMBaseRegisterInfo *ARI = Subtarget->getRegisterInfo();
@@ -2173,6 +2254,8 @@
         isThisReturn = false;
         Mask = ARI->getCallPreservedMask(MF, CallConv);
       }
+    } else if (IsCCallInsideJSSaveFP) {
+      Mask = ARI->getCallPreservedMask(MF, CallingConv::V8FPSave);
     } else
       Mask = ARI->getCallPreservedMask(MF, CallConv);
 
@@ -2191,6 +2274,37 @@
 
   // Returns a chain and a flag for retval copy to use.
   Chain = DAG.getNode(CallOpc, dl, NodeTys, Ops);
+
+  if (hasJSCCall) {
+    Chain =
+        SDValue(DAG.getMachineNode(TargetOpcode::RESTORESP, dl,
+                                   DAG.getVTList(MVT::Other, MVT::Glue), Chain),
+                0);
+  }
+
+  // Restore the FP.
+  if (IsCCallInsideJSSaveFP) {
+    std::vector<SDValue> Ops;
+    Ops.emplace_back(FPSaveArea1);
+    Ops.emplace_back(CCAL);
+    Ops.emplace_back(CCR);
+    for (int i = 0; i < 8; ++i)
+      Ops.emplace_back(DAG.getRegister(ARM::D0 + i, MVT::i64));
+    Ops.emplace_back(Chain);
+    Chain = SDValue(DAG.getMachineNode(ARM::VLDMDIA, dl, MVT::Other, Ops), 0);
+    Ops.clear();
+    Ops.emplace_back(FPSaveArea0);
+    Ops.emplace_back(CCAL);
+    Ops.emplace_back(CCR);
+    for (int i = 16; i < 32; ++i)
+      Ops.emplace_back(DAG.getRegister(ARM::D0 + i, MVT::i64));
+    Ops.emplace_back(Chain);
+    Chain =
+        SDValue(DAG.getMachineNode(ARM::VLDMDIA, dl,
+                                   DAG.getVTList(MVT::Other, MVT::Glue), Ops),
+                0);
+  }
+
   InFlag = Chain.getValue(1);
 
   Chain = DAG.getCALLSEQ_END(Chain, DAG.getIntPtrConstant(NumBytes, dl, true),
@@ -9254,6 +9368,17 @@
     llvm_unreachable("Unexpected instr type to insert");
   }
 
+  case TargetOpcode::STATEPOINT:
+    // As an implementation detail, STATEPOINT shares the STACKMAP format at
+    // this point in the process.  We diverge later.
+    return emitPatchPoint(MI, BB);
+
+  case TargetOpcode::STACKMAP:
+  case TargetOpcode::PATCHPOINT:
+    return emitPatchPoint(MI, BB);
+  case TargetOpcode::TCPATCHPOINT:
+    return TargetLoweringBase::emitPatchPoint(MI, BB);
+
   // Thumb1 post-indexed loads are really just single-register LDMs.
   case ARM::tLDR_postidx: {
     MachineOperand Def(MI.getOperand(1));
@@ -10961,6 +11086,10 @@
   unsigned Mask = MaskC->getZExtValue();
   if (Mask == 0xffff)
     return SDValue();
+  // Memory Operator can address constant, save one register.
+  auto UI = dyn_cast<MemSDNode>(*(N->use_begin()));
+  if (UI)
+    return SDValue();
   SDValue Res;
   // Case (1): or (and A, mask), val => ARMbfi A, val, mask
   ConstantSDNode *N1C = dyn_cast<ConstantSDNode>(N1);
@@ -13136,6 +13265,23 @@
   return -1;
 }
 
+const MCPhysReg *
+ARMTargetLowering::getScratchRegisters(CallingConv::ID CC) const {
+  if (CC == CallingConv::V8SBCC)
+    return nullptr;
+  static const MCPhysReg ScratchRegs[] = {ARM::R12, 0};
+  return ScratchRegs;
+}
+
+MachineBasicBlock *
+ARMTargetLowering::emitPatchPoint(MachineInstr &MI,
+                                  MachineBasicBlock *MBB) const {
+  MachineBasicBlock *MBB2 = TargetLoweringBase::emitPatchPoint(MI, MBB);
+  MachineFunction &MF = *MI.getMF();
+  MI.addOperand(MF, MachineOperand::CreateReg(ARM::LR, true, true));
+  return MBB2;
+}
+
 static bool isLegalT1AddressImmediate(int64_t V, EVT VT) {
   if (V < 0)
     return false;
Index: lib/Target/ARM/ARMISelLowering.h
===================================================================
--- lib/Target/ARM/ARMISelLowering.h	(revision 365284)
+++ lib/Target/ARM/ARMISelLowering.h	(working copy)
@@ -501,6 +501,11 @@
     bool functionArgumentNeedsConsecutiveRegisters(
         Type *Ty, CallingConv::ID CallConv, bool isVarArg) const override;
 
+    const MCPhysReg *getScratchRegisters(CallingConv::ID CC) const override;
+
+    MachineBasicBlock *emitPatchPoint(MachineInstr &MI,
+                                      MachineBasicBlock *MBB) const;
+
     /// If a physical register, this returns the register that receives the
     /// exception address on entry to an EH pad.
     unsigned
Index: lib/Target/ARM/ARMMachineFunctionInfo.h
===================================================================
--- lib/Target/ARM/ARMMachineFunctionInfo.h	(revision 365284)
+++ lib/Target/ARM/ARMMachineFunctionInfo.h	(working copy)
@@ -85,6 +85,7 @@
   /// areas.
   unsigned GPRCS1Size = 0;
   unsigned GPRCS2Size = 0;
+  unsigned GPRCS3Size = 0;
   unsigned DPRCSAlignGapSize = 0;
   unsigned DPRCSSize = 0;
 
@@ -127,6 +128,13 @@
   /// The amount the literal pool has been increasedby due to promoted globals.
   int PromotedGlobalsIncrease = 0;
 
+  mutable int LastSPAdjust = 0;
+  int FIFPSaveArea0 = -1;
+  int FIFPSaveArea1 = -1;
+  bool IsJSFunction = false;
+  bool IsJSStub = false;
+  bool IsWASM = false;
+
 public:
   ARMFunctionInfo() = default;
 
@@ -170,11 +178,13 @@
 
   unsigned getGPRCalleeSavedArea1Size() const { return GPRCS1Size; }
   unsigned getGPRCalleeSavedArea2Size() const { return GPRCS2Size; }
+  unsigned getGPRCalleeSavedArea3Size() const { return GPRCS3Size; }
   unsigned getDPRCalleeSavedGapSize() const   { return DPRCSAlignGapSize; }
   unsigned getDPRCalleeSavedAreaSize()  const { return DPRCSSize; }
 
   void setGPRCalleeSavedArea1Size(unsigned s) { GPRCS1Size = s; }
   void setGPRCalleeSavedArea2Size(unsigned s) { GPRCS2Size = s; }
+  void setGPRCalleeSavedArea3Size(unsigned s) { GPRCS3Size = s; }
   void setDPRCalleeSavedGapSize(unsigned s)   { DPRCSAlignGapSize = s; }
   void setDPRCalleeSavedAreaSize(unsigned s)  { DPRCSSize = s; }
 
@@ -239,6 +249,25 @@
   void setPromotedConstpoolIncrease(int Sz) {
     PromotedGlobalsIncrease = Sz;
   }
+  bool isJSFunction() const { return IsJSFunction; }
+  bool isJSStub() const { return IsJSStub; }
+  bool isWASM() const { return IsWASM; }
+
+  void setJSFunction(bool s) { IsJSFunction = s; }
+  void setJSStub(bool s) { IsJSStub = s; }
+  void setWASM(bool s) { IsWASM = s; }
+
+  void pushLastSPAdjust(int n) const { LastSPAdjust = n; }
+  int popLastSPAdjust() const {
+    int r = LastSPAdjust;
+    LastSPAdjust = 0;
+    return r;
+  }
+  int getFIFPSaveArea0() const { return FIFPSaveArea0; }
+  int getFIFPSaveArea1() const { return FIFPSaveArea1; }
+
+  void setFIFPSaveArea0(int a) { FIFPSaveArea0 = a; }
+  void setFIFPSaveArea1(int a) { FIFPSaveArea1 = a; }
 };
 
 } // end namespace llvm
Index: lib/Target/ARM/ARMSubtarget.cpp
===================================================================
--- lib/Target/ARM/ARMSubtarget.cpp	(revision 365284)
+++ lib/Target/ARM/ARMSubtarget.cpp	(working copy)
@@ -207,6 +207,8 @@
     stackAlignment = 8;
   if (isTargetNaCl() || isAAPCS16_ABI())
     stackAlignment = 16;
+  if (isV8_ABI())
+    stackAlignment = 4;
 
   // FIXME: Completely disable sibcall for Thumb1 since ThumbRegisterInfo::
   // emitEpilogue is not ready for them. Thumb tail calls also use t2B, as
@@ -325,6 +327,10 @@
   assert(TM.TargetABI != ARMBaseTargetMachine::ARM_ABI_UNKNOWN);
   return TM.TargetABI == ARMBaseTargetMachine::ARM_ABI_AAPCS16;
 }
+bool ARMSubtarget::isV8_ABI() const {
+  assert(TM.TargetABI != ARMBaseTargetMachine::ARM_ABI_UNKNOWN);
+  return TM.TargetABI == ARMBaseTargetMachine::ARM_ABI_V8;
+}
 
 bool ARMSubtarget::isROPI() const {
   return TM.getRelocationModel() == Reloc::ROPI ||
Index: lib/Target/ARM/ARMSubtarget.h
===================================================================
--- lib/Target/ARM/ARMSubtarget.h	(revision 365284)
+++ lib/Target/ARM/ARMSubtarget.h	(working copy)
@@ -702,6 +702,7 @@
   bool isAPCS_ABI() const;
   bool isAAPCS_ABI() const;
   bool isAAPCS16_ABI() const;
+  bool isV8_ABI() const;
 
   bool isROPI() const;
   bool isRWPI() const;
Index: lib/Target/ARM/ARMTargetMachine.cpp
===================================================================
--- lib/Target/ARM/ARMTargetMachine.cpp	(revision 365284)
+++ lib/Target/ARM/ARMTargetMachine.cpp	(working copy)
@@ -119,6 +119,8 @@
     return ARMBaseTargetMachine::ARM_ABI_AAPCS;
   else if (ABIName.startswith("apcs"))
     return ARMBaseTargetMachine::ARM_ABI_APCS;
+  else if (ABIName == "v8")
+    return ARMBaseTargetMachine::ARM_ABI_V8;
 
   llvm_unreachable("Unhandled/unknown ABI Name!");
   return ARMBaseTargetMachine::ARM_ABI_UNKNOWN;
@@ -143,7 +145,8 @@
   Ret += "-p:32:32";
 
   // ABIs other than APCS have 64 bit integers with natural alignment.
-  if (ABI != ARMBaseTargetMachine::ARM_ABI_APCS)
+  if (ABI != ARMBaseTargetMachine::ARM_ABI_APCS &&
+      ABI != ARMBaseTargetMachine::ARM_ABI_V8)
     Ret += "-i64:64";
 
   // We have 64 bits floats. The APCS ABI requires them to be aligned to 32
@@ -153,7 +156,8 @@
 
   // We have 128 and 64 bit vectors. The APCS ABI aligns them to 32 bits, others
   // to 64. We always ty to give them natural alignment.
-  if (ABI == ARMBaseTargetMachine::ARM_ABI_APCS)
+  if (ABI == ARMBaseTargetMachine::ARM_ABI_APCS ||
+      ABI == ARMBaseTargetMachine::ARM_ABI_V8)
     Ret += "-v64:32:64-v128:32:128";
   else if (ABI != ARMBaseTargetMachine::ARM_ABI_AAPCS16)
     Ret += "-v128:64:128";
Index: lib/Target/ARM/ARMTargetMachine.h
===================================================================
--- lib/Target/ARM/ARMTargetMachine.h	(revision 365284)
+++ lib/Target/ARM/ARMTargetMachine.h	(working copy)
@@ -31,7 +31,8 @@
     ARM_ABI_UNKNOWN,
     ARM_ABI_APCS,
     ARM_ABI_AAPCS, // ARM EABI
-    ARM_ABI_AAPCS16
+    ARM_ABI_AAPCS16,
+    ARM_ABI_V8
   } TargetABI;
 
 protected:
Index: lib/Target/ARM/ARMTargetObjectFile.cpp
===================================================================
--- lib/Target/ARM/ARMTargetObjectFile.cpp	(revision 365284)
+++ lib/Target/ARM/ARMTargetObjectFile.cpp	(working copy)
@@ -32,6 +32,7 @@
                                         const TargetMachine &TM) {
   const ARMBaseTargetMachine &ARM_TM = static_cast<const ARMBaseTargetMachine &>(TM);
   bool isAAPCS_ABI = ARM_TM.TargetABI == ARMBaseTargetMachine::ARMABI::ARM_ABI_AAPCS;
+  bool isV8_ABI = ARM_TM.TargetABI == ARMBaseTargetMachine::ARMABI::ARM_ABI_V8;
   bool genExecuteOnly =
       ARM_TM.getMCSubtargetInfo()->hasFeature(ARM::FeatureExecuteOnly);
 
@@ -38,7 +39,7 @@
   TargetLoweringObjectFileELF::Initialize(Ctx, TM);
   InitializeELF(isAAPCS_ABI);
 
-  if (isAAPCS_ABI) {
+  if (isAAPCS_ABI || isV8_ABI) {
     LSDASection = nullptr;
   }
 
Index: lib/Target/ARM/ARMTargetTransformInfo.cpp
===================================================================
--- lib/Target/ARM/ARMTargetTransformInfo.cpp	(revision 365284)
+++ lib/Target/ARM/ARMTargetTransformInfo.cpp	(working copy)
@@ -130,6 +130,13 @@
   if (Opcode == Instruction::Xor && Imm.isAllOnesValue())
     return 0;
 
+  if ((Opcode == Instruction::GetElementPtr) &&
+      (!ST->isThumb() || ST->isThumb2())) {
+    int64_t SImmVal = Imm.getSExtValue();
+    if (SImmVal < 4096 && SImmVal > -4096)
+      return 0;
+  }
+
   return getIntImmCost(Imm, Ty);
 }
 
Index: lib/Target/ARM/ARMTargetTransformInfo.h
===================================================================
--- lib/Target/ARM/ARMTargetTransformInfo.h	(revision 365284)
+++ lib/Target/ARM/ARMTargetTransformInfo.h	(working copy)
@@ -47,6 +47,7 @@
 
   const ARMSubtarget *ST;
   const ARMTargetLowering *TLI;
+  const Function *Fn;
 
   // Currently the following features are excluded from InlineFeatureWhitelist.
   // ModeThumb, FeatureNoARM, ModeSoftFloat, FeatureVFPOnlySP, FeatureD16
@@ -87,7 +88,7 @@
 public:
   explicit ARMTTIImpl(const ARMBaseTargetMachine *TM, const Function &F)
       : BaseT(TM, F.getParent()->getDataLayout()), ST(TM->getSubtargetImpl(F)),
-        TLI(ST->getTargetLowering()) {}
+        TLI(ST->getTargetLowering()), Fn(&F) {}
 
   bool areInlineCompatible(const Function *Caller,
                            const Function *Callee) const;
@@ -177,6 +178,8 @@
                                TTI::UnrollingPreferences &UP);
 
   bool shouldBuildLookupTablesForConstant(Constant *C) const {
+    if (Fn->getCallingConv() == CallingConv::V8CC)
+      return false;
     // In the ROPI and RWPI relocation models we can't have pointers to global
     // variables or functions in constant data, so don't convert switches to
     // lookup tables if any of the values would need relocation.
Index: lib/Target/Target.cpp
===================================================================
--- lib/Target/Target.cpp	(revision 365284)
+++ lib/Target/Target.cpp	(working copy)
@@ -66,6 +66,12 @@
   unwrap(PM)->add(new TargetLibraryInfoWrapperPass(*unwrap(TLI)));
 }
 
+LLVMTargetLibraryInfoRef LLVMCreateEmptyTargetLibraryInfo() {
+  TargetLibraryInfoImpl *TLII = new TargetLibraryInfoImpl();
+  TLII->disableAllFunctions();
+  return wrap(TLII);
+}
+
 char *LLVMCopyStringRepOfTargetData(LLVMTargetDataRef TD) {
   std::string StringRep = unwrap(TD)->getStringRepresentation();
   return strdup(StringRep.c_str());
Index: lib/Transforms/IPO/PassManagerBuilder.cpp
===================================================================
--- lib/Transforms/IPO/PassManagerBuilder.cpp	(revision 365284)
+++ lib/Transforms/IPO/PassManagerBuilder.cpp	(working copy)
@@ -34,6 +34,7 @@
 #include "llvm/Transforms/IPO/ForceFunctionAttrs.h"
 #include "llvm/Transforms/IPO/FunctionAttrs.h"
 #include "llvm/Transforms/IPO/InferFunctionAttrs.h"
+#include "llvm/Transforms/IPO/SampleProfile.h"
 #include "llvm/Transforms/InstCombine/InstCombine.h"
 #include "llvm/Transforms/Instrumentation.h"
 #include "llvm/Transforms/Scalar.h"
@@ -273,6 +274,8 @@
 
 // Do PGO instrumentation generation or use pass as the option specified.
 void PassManagerBuilder::addPGOInstrPasses(legacy::PassManagerBase &MPM) {
+  if (PGOSampleUse.empty())
+    PGOSampleUse = SampleProfileLoaderPass::SampleProfileFileFromOption();
   if (!EnablePGOInstrGen && PGOInstrUse.empty() && PGOSampleUse.empty())
     return;
   // Perform the preinline and cleanup passes for O1 and above.
@@ -425,6 +428,8 @@
 
 void PassManagerBuilder::populateModulePassManager(
     legacy::PassManagerBase &MPM) {
+  if (PGOSampleUse.empty())
+    PGOSampleUse = SampleProfileLoaderPass::SampleProfileFileFromOption();
   if (!PGOSampleUse.empty()) {
     MPM.add(createPruneEHPass());
     MPM.add(createSampleProfileLoaderPass(PGOSampleUse));
@@ -755,6 +760,8 @@
 }
 
 void PassManagerBuilder::addLTOOptimizationPasses(legacy::PassManagerBase &PM) {
+  if (PGOSampleUse.empty())
+    PGOSampleUse = SampleProfileLoaderPass::SampleProfileFileFromOption();
   // Load sample profile before running the LTO optimization pipeline.
   if (!PGOSampleUse.empty()) {
     PM.add(createPruneEHPass());
Index: lib/Transforms/IPO/SampleProfile.cpp
===================================================================
--- lib/Transforms/IPO/SampleProfile.cpp	(revision 365284)
+++ lib/Transforms/IPO/SampleProfile.cpp	(working copy)
@@ -1669,3 +1669,7 @@
 
   return PreservedAnalyses::none();
 }
+
+std::string SampleProfileLoaderPass::SampleProfileFileFromOption() {
+  return SampleProfileFile;
+}
Index: tools/llvm-shlib/CMakeLists.txt
===================================================================
--- tools/llvm-shlib/CMakeLists.txt	(revision 365284)
+++ tools/llvm-shlib/CMakeLists.txt	(working copy)
@@ -64,6 +64,7 @@
   endif()
 
   target_link_libraries(LLVM PRIVATE ${LIB_NAMES})
+  set_target_properties(LLVM PROPERTIES LINK_FLAGS "-static-libstdc++")
 
   if (APPLE)
     set_property(TARGET LLVM APPEND_STRING PROPERTY
Index: tools/llvm-shlib/simple_version_script.map.in
===================================================================
--- tools/llvm-shlib/simple_version_script.map.in	(revision 365284)
+++ tools/llvm-shlib/simple_version_script.map.in	(working copy)
@@ -1 +1 @@
-LLVM_@LLVM_VERSION_MAJOR@ { global: *; };
+LLVM_@LLVM_VERSION_MAJOR@ { global: LLVM*; __jit_debug_descriptor; local:*; };
