Index: include/llvm/IR/CallingConv.h
===================================================================
--- include/llvm/IR/CallingConv.h	(revision 347537)
+++ include/llvm/IR/CallingConv.h	(working copy)
@@ -220,6 +220,12 @@
     /// shader if tessellation is in use, or otherwise the vertex shader.
     AMDGPU_ES = 96,
 
+    // Calling convention for v8
+    V8CC = 97,
+
+    // Calling convention for v8 store barrier stub
+    V8SBCC = 98,
+
     /// The highest possible calling convention ID. Must be some 2^k - 1.
     MaxID = 1023
   };
Index: include/llvm/Support/TargetOpcodes.def
===================================================================
--- include/llvm/Support/TargetOpcodes.def	(revision 347537)
+++ include/llvm/Support/TargetOpcodes.def	(working copy)
@@ -120,6 +120,7 @@
 /// rewrite calls to runtimes with more efficient code sequences.
 /// This also implies a stack map.
 HANDLE_TARGET_OPCODE(PATCHPOINT)
+HANDLE_TARGET_OPCODE(TCPATCHPOINT)
 
 /// This pseudo-instruction loads the stack guard value. Targets which need
 /// to prevent the stack guard value or address from being spilled to the
Index: include/llvm/Target/Target.td
===================================================================
--- include/llvm/Target/Target.td	(revision 347537)
+++ include/llvm/Target/Target.td	(working copy)
@@ -1066,6 +1066,17 @@
   let mayLoad = 1;
   let usesCustomInserter = 1;
 }
+def TCPATCHPOINT : StandardPseudoInstruction {
+  let OutOperandList = (outs);
+  let InOperandList = (ins i64imm:$id, i32imm:$nbytes, unknown:$callee,
+                       i32imm:$nargs, i32imm:$cc, variable_ops);
+  let hasSideEffects = 1;
+  let isCall = 1;
+  let mayLoad = 1;
+  let usesCustomInserter = 1;
+  let isReturn = 1;
+  let isTerminator = 1;
+}
 def STATEPOINT : StandardPseudoInstruction {
   let OutOperandList = (outs);
   let InOperandList = (ins variable_ops);
Index: include/llvm-c/Core.h
===================================================================
--- include/llvm-c/Core.h	(revision 347537)
+++ include/llvm-c/Core.h	(working copy)
@@ -240,7 +240,9 @@
   LLVMAMDGPUHSCallConv      = 93,
   LLVMMSP430BUILTINCallConv = 94,
   LLVMAMDGPULSCallConv      = 95,
-  LLVMAMDGPUESCallConv      = 96
+  LLVMAMDGPUESCallConv      = 96,
+  LLVMV8CallConv            = 97,
+  LLVMV8SBCallConv          = 98
 } LLVMCallConv;
 
 typedef enum {
@@ -3554,6 +3556,8 @@
     @see llvm::llvm_is_multithreaded */
 LLVMBool LLVMIsMultithreaded(void);
 
+LLVMBool LLVMGetStatepointName(LLVMTypeRef type, char *buffer, size_t n);
+
 /**
  * @}
  */
Index: lib/AsmParser/LLLexer.cpp
===================================================================
--- lib/AsmParser/LLLexer.cpp	(revision 347537)
+++ lib/AsmParser/LLLexer.cpp	(working copy)
@@ -606,6 +606,8 @@
   KEYWORD(webkit_jscc);
   KEYWORD(swiftcc);
   KEYWORD(anyregcc);
+  KEYWORD(v8cc);
+  KEYWORD(v8sbcc);
   KEYWORD(preserve_mostcc);
   KEYWORD(preserve_allcc);
   KEYWORD(ghccc);
Index: lib/AsmParser/LLParser.cpp
===================================================================
--- lib/AsmParser/LLParser.cpp	(revision 347537)
+++ lib/AsmParser/LLParser.cpp	(working copy)
@@ -1906,6 +1906,8 @@
   case lltok::kw_win64cc:        CC = CallingConv::Win64; break;
   case lltok::kw_webkit_jscc:    CC = CallingConv::WebKit_JS; break;
   case lltok::kw_anyregcc:       CC = CallingConv::AnyReg; break;
+  case lltok::kw_v8cc:           CC = CallingConv::V8CC; break;
+  case lltok::kw_v8sbcc:         CC = CallingConv::V8SBCC; break;
   case lltok::kw_preserve_mostcc:CC = CallingConv::PreserveMost; break;
   case lltok::kw_preserve_allcc: CC = CallingConv::PreserveAll; break;
   case lltok::kw_ghccc:          CC = CallingConv::GHC; break;
Index: lib/AsmParser/LLToken.h
===================================================================
--- lib/AsmParser/LLToken.h	(revision 347537)
+++ lib/AsmParser/LLToken.h	(working copy)
@@ -150,6 +150,8 @@
   kw_win64cc,
   kw_webkit_jscc,
   kw_anyregcc,
+  kw_v8cc,
+  kw_v8sbcc,
   kw_swiftcc,
   kw_preserve_mostcc,
   kw_preserve_allcc,
Index: lib/CodeGen/InlineSpiller.cpp
===================================================================
--- lib/CodeGen/InlineSpiller.cpp	(revision 347537)
+++ lib/CodeGen/InlineSpiller.cpp	(working copy)
@@ -57,6 +57,7 @@
 #include <cassert>
 #include <iterator>
 #include <tuple>
+#include <unordered_set>
 #include <utility>
 #include <vector>
 
@@ -213,6 +214,7 @@
   bool isSibling(unsigned Reg);
   bool hoistSpillInsideBB(LiveInterval &SpillLI, MachineInstr &CopyMI);
   void eliminateRedundantSpills(LiveInterval &LI, VNInfo *VNI);
+  void foldStatePoints(unsigned Reg);
 
   void markValueUsed(LiveInterval*, VNInfo*);
   bool reMaterializeFor(LiveInterval &, MachineInstr &MI);
@@ -924,11 +926,60 @@
     HSpiller.addToMergeableSpills(*std::next(MI), StackSlot, Original);
 }
 
+void InlineSpiller::foldStatePoints(unsigned InputReg) {
+  SmallVector<unsigned, 8> WorkList;
+  std::unordered_set<unsigned> VisitedSet;
+  // Don't add Reg to WorkList, spillAroundUses will handle it.
+  for (MachineRegisterInfo::reg_bundle_iterator
+           RegI = MRI.reg_bundle_begin(InputReg),
+           E = MRI.reg_bundle_end();
+       RegI != E;) {
+    MachineInstr &MI = *RegI++;
+    unsigned SibReg = isFullCopyOf(MI, InputReg);
+    if (SibReg && isSibling(SibReg) && SibReg != InputReg) {
+      WorkList.push_back(SibReg);
+      VisitedSet.emplace(SibReg);
+    }
+  }
+  VisitedSet.emplace(InputReg);
+  while (!WorkList.empty()) {
+    unsigned Reg = WorkList.pop_back_val();
+    // Fold the use of state point.
+    for (MachineRegisterInfo::use_instr_nodbg_iterator
+             UI = MRI.use_instr_nodbg_begin(Reg),
+             E = MRI.use_instr_nodbg_end();
+         UI != E;) {
+      MachineInstr &MI = *UI++;
+      if (unsigned DstReg = isFullCopyOf(MI, Reg)) {
+        if (isSibling(DstReg)) {
+          auto pair = VisitedSet.emplace(DstReg);
+          if (pair.second)
+            WorkList.push_back(DstReg);
+        }
+        continue;
+      }
+      // Handle STATEPOINT, PATCHPOINT
+      switch (MI.getOpcode()) {
+      case TargetOpcode::STATEPOINT:
+      case TargetOpcode::PATCHPOINT: {
+        // Analyze instruction.
+        SmallVector<std::pair<MachineInstr *, unsigned>, 8> Ops;
+        MIBundleOperands(MI).analyzeVirtReg(Reg, &Ops);
+        foldMemoryOperand(Ops);
+      } break;
+      default:
+        break;
+      }
+    }
+  }
+}
+
 /// spillAroundUses - insert spill code around each use of Reg.
 void InlineSpiller::spillAroundUses(unsigned Reg) {
   LLVM_DEBUG(dbgs() << "spillAroundUses " << printReg(Reg) << '\n');
   LiveInterval &OldLI = LIS.getInterval(Reg);
 
+  foldStatePoints(Reg);
   // Iterate over instructions using Reg.
   for (MachineRegisterInfo::reg_bundle_iterator
        RegI = MRI.reg_bundle_begin(Reg), E = MRI.reg_bundle_end();
Index: lib/CodeGen/LocalStackSlotAllocation.cpp
===================================================================
--- lib/CodeGen/LocalStackSlotAllocation.cpp	(revision 347537)
+++ lib/CodeGen/LocalStackSlotAllocation.cpp	(working copy)
@@ -299,6 +299,7 @@
       // range, so they don't need any updates.
       if (MI.isDebugInstr() || MI.getOpcode() == TargetOpcode::STATEPOINT ||
           MI.getOpcode() == TargetOpcode::STACKMAP ||
+          MI.getOpcode() == TargetOpcode::TCPATCHPOINT ||
           MI.getOpcode() == TargetOpcode::PATCHPOINT)
         continue;
 
Index: lib/CodeGen/MachineVerifier.cpp
===================================================================
--- lib/CodeGen/MachineVerifier.cpp	(revision 347537)
+++ lib/CodeGen/MachineVerifier.cpp	(working copy)
@@ -1097,6 +1097,8 @@
   unsigned NumDefs = MCID.getNumDefs();
   if (MCID.getOpcode() == TargetOpcode::PATCHPOINT)
     NumDefs = (MONum == 0 && MO->isReg()) ? NumDefs : 0;
+  else if (MCID.getOpcode() == TargetOpcode::TCPATCHPOINT)
+    NumDefs = 0;
 
   // The first MCID.NumDefs operands must be explicit register defines
   if (MONum < NumDefs) {
Index: lib/CodeGen/SelectionDAG/InstrEmitter.cpp
===================================================================
--- lib/CodeGen/SelectionDAG/InstrEmitter.cpp	(revision 347537)
+++ lib/CodeGen/SelectionDAG/InstrEmitter.cpp	(working copy)
@@ -809,7 +809,8 @@
   const MCPhysReg *ScratchRegs = nullptr;
 
   // Handle STACKMAP and PATCHPOINT specially and then use the generic code.
-  if (Opc == TargetOpcode::STACKMAP || Opc == TargetOpcode::PATCHPOINT) {
+  if (Opc == TargetOpcode::STACKMAP || Opc == TargetOpcode::PATCHPOINT ||
+      Opc == TargetOpcode::TCPATCHPOINT) {
     // Stackmaps do not have arguments and do not preserve their calling
     // convention. However, to simplify runtime support, they clobber the same
     // scratch registers as AnyRegCC.
Index: lib/CodeGen/SelectionDAG/ScheduleDAGSDNodes.cpp
===================================================================
--- lib/CodeGen/SelectionDAG/ScheduleDAGSDNodes.cpp	(revision 347537)
+++ lib/CodeGen/SelectionDAG/ScheduleDAGSDNodes.cpp	(working copy)
@@ -540,6 +540,10 @@
     NodeNumDefs = 0;
     return;
   }
+  if (POpc == TargetOpcode::TCPATCHPOINT) {
+    NodeNumDefs = 0;
+    return;
+  }
   if (POpc == TargetOpcode::PATCHPOINT &&
       Node->getValueType(0) == MVT::Other) {
     // PATCHPOINT is defined to have one result, but it might really have none
Index: lib/CodeGen/SelectionDAG/SelectionDAGBuilder.cpp
===================================================================
--- lib/CodeGen/SelectionDAG/SelectionDAGBuilder.cpp	(revision 347537)
+++ lib/CodeGen/SelectionDAG/SelectionDAGBuilder.cpp	(working copy)
@@ -8077,6 +8077,32 @@
   FuncInfo.MF->getFrameInfo().setHasStackMap();
 }
 
+static bool isPatchpointInTailCallPosition(ImmutableCallSite CS) {
+  const Instruction *I = CS.getInstruction();
+  const BasicBlock *ExitBB = I->getParent();
+  const TerminatorInst *Term = ExitBB->getTerminator();
+  const ReturnInst *Ret = dyn_cast<ReturnInst>(Term);
+  // If Term is not a ReturnInst, then it must be a UnreachableInst.
+  if (!Ret && !isa<UnreachableInst>(Term))
+    return false;
+  // Copy from isInTailCallPosition.
+  // If I will have a chain, make sure no other instruction that will have a
+  // chain interposes between I and the return.
+  if (I->mayHaveSideEffects() || I->mayReadFromMemory() ||
+      !isSafeToSpeculativelyExecute(I))
+    for (BasicBlock::const_iterator BBI = std::prev(ExitBB->end(), 2);; --BBI) {
+      if (&*BBI == I)
+        break;
+      // Debug info intrinsics do not get in the way of tail call optimization.
+      if (isa<DbgInfoIntrinsic>(BBI))
+        continue;
+      if (BBI->mayHaveSideEffects() || BBI->mayReadFromMemory() ||
+          !isSafeToSpeculativelyExecute(&*BBI))
+        return false;
+    }
+  return true;
+}
+
 /// Lower llvm.experimental.patchpoint directly to its target opcode.
 void SelectionDAGBuilder::visitPatchpoint(ImmutableCallSite CS,
                                           const BasicBlock *EHPadBB) {
@@ -8120,17 +8146,24 @@
   TargetLowering::CallLoweringInfo CLI(DAG);
   populateCallLoweringInfo(CLI, CS, NumMetaOpers, NumCallArgs, Callee, ReturnTy,
                            true);
+  CLI.IsTailCall = isPatchpointInTailCallPosition(CS);
+  assert((!CLI.IsTailCall || !HasDef) &&
+         "TailCall should not has a return type");
   std::pair<SDValue, SDValue> Result = lowerInvokable(CLI, EHPadBB);
+  SDNode *Call;
+  if (!CLI.IsTailCall) {
+    SDNode *CallEnd = Result.second.getNode();
+    if (HasDef && (CallEnd->getOpcode() == ISD::CopyFromReg))
+      CallEnd = CallEnd->getOperand(0).getNode();
 
-  SDNode *CallEnd = Result.second.getNode();
-  if (HasDef && (CallEnd->getOpcode() == ISD::CopyFromReg))
-    CallEnd = CallEnd->getOperand(0).getNode();
-
-  /// Get a call instruction from the call sequence chain.
-  /// Tail calls are not allowed.
-  assert(CallEnd->getOpcode() == ISD::CALLSEQ_END &&
-         "Expected a callseq node.");
-  SDNode *Call = CallEnd->getOperand(0).getNode();
+    /// Get a call instruction from the call sequence chain.
+    /// Tail calls are not allowed.
+    assert(CallEnd->getOpcode() == ISD::CALLSEQ_END &&
+           "Expected a callseq node.");
+    Call = CallEnd->getOperand(0).getNode();
+  } else {
+    Call = CLI.Chain.getNode();
+  }
   bool HasGlue = Call->getGluedNode();
 
   // Replace the target specific call node with the patchable intrinsic.
@@ -8201,8 +8234,9 @@
     NodeTys = DAG.getVTList(MVT::Other, MVT::Glue);
 
   // Replace the target specific call node with a PATCHPOINT node.
-  MachineSDNode *MN = DAG.getMachineNode(TargetOpcode::PATCHPOINT,
-                                         dl, NodeTys, Ops);
+  MachineSDNode *MN = DAG.getMachineNode(
+      CLI.IsTailCall ? TargetOpcode::TCPATCHPOINT : TargetOpcode::PATCHPOINT,
+      dl, NodeTys, Ops);
 
   // Update the NodeMap.
   if (HasDef) {
Index: lib/CodeGen/SelectionDAG/StatepointLowering.cpp
===================================================================
--- lib/CodeGen/SelectionDAG/StatepointLowering.cpp	(revision 347537)
+++ lib/CodeGen/SelectionDAG/StatepointLowering.cpp	(working copy)
@@ -532,14 +532,15 @@
   // arrays interwoven with each (lowered) base pointer immediately followed by
   // it's (lowered) derived pointer.  i.e
   // (base[0], ptr[0], base[1], ptr[1], ...)
+  bool LiveInOnly = SI.CLI.CallConv == CallingConv::V8CC;
   for (unsigned i = 0; i < SI.Bases.size(); ++i) {
     const Value *Base = SI.Bases[i];
-    lowerIncomingStatepointValue(Builder.getValue(Base), /*LiveInOnly*/ false,
-                                 Ops, Builder);
+    lowerIncomingStatepointValue(Builder.getValue(Base), LiveInOnly, Ops,
+                                 Builder);
 
     const Value *Ptr = SI.Ptrs[i];
-    lowerIncomingStatepointValue(Builder.getValue(Ptr), /*LiveInOnly*/ false,
-                                 Ops, Builder);
+    lowerIncomingStatepointValue(Builder.getValue(Ptr), LiveInOnly, Ops,
+                                 Builder);
   }
 
   // If there are any explicit spill slots passed to the statepoint, record
@@ -818,6 +819,11 @@
 
     unsigned AS = ISP.getCalledValue()->getType()->getPointerAddressSpace();
     ActualCallee = DAG.getConstant(0, getCurSDLoc(), TLI.getPointerTy(DL, AS));
+    if (auto *ConstCallee =
+            dyn_cast<ConstantSDNode>(getValue(ISP.getCalledValue())))
+      ActualCallee =
+          DAG.getIntPtrConstant(ConstCallee->getZExtValue(), getCurSDLoc(),
+                                /*isTarget=*/true);
   } else {
     ActualCallee = getValue(ISP.getCalledValue());
   }
Index: lib/CodeGen/SelectionDAG/TargetLowering.cpp
===================================================================
--- lib/CodeGen/SelectionDAG/TargetLowering.cpp	(revision 347537)
+++ lib/CodeGen/SelectionDAG/TargetLowering.cpp	(working copy)
@@ -87,6 +87,9 @@
     // (We look for a CopyFromReg reading a virtual register that is used
     //  for the function live-in value of register Reg)
     SDValue Value = OutVals[I];
+    // Ignore the undefined input.
+    if (Value->getOpcode() == ISD::UNDEF)
+      continue;
     if (Value->getOpcode() != ISD::CopyFromReg)
       return false;
     unsigned ArgReg = cast<RegisterSDNode>(Value->getOperand(1))->getReg();
Index: lib/CodeGen/ShrinkWrap.cpp
===================================================================
--- lib/CodeGen/ShrinkWrap.cpp	(revision 347537)
+++ lib/CodeGen/ShrinkWrap.cpp	(working copy)
@@ -157,6 +157,8 @@
   /// Current MachineFunction.
   MachineFunction *MachineFunc;
 
+  const BitVector *Reserved;
+
   /// Check if \p MI uses or defines a callee-saved register or
   /// a frame index. If this is the case, this means \p MI must happen
   /// after Save and before Restore.
@@ -204,6 +206,9 @@
     CurrentCSRs.clear();
     MachineFunc = &MF;
 
+    const BitVector &RR = MF.getRegInfo().getReservedRegs();
+    Reserved = &RR;
+
     ++NumFunc;
   }
 
@@ -264,6 +269,8 @@
     LLVM_DEBUG(dbgs() << "Frame instruction: " << MI << '\n');
     return true;
   }
+  const MachineFunction &MF = *MI.getParent()->getParent();
+  const MachineRegisterInfo &MRI = MF.getRegInfo();
   for (const MachineOperand &MO : MI.operands()) {
     bool UseOrDefCSR = false;
     if (MO.isReg()) {
@@ -280,8 +287,10 @@
       // separately. An SP mentioned by a call instruction, we can ignore,
       // though, as it's harmless and we do not want to effectively disable tail
       // calls by forcing the restore point to post-dominate them.
-      UseOrDefCSR = (!MI.isCall() && PhysReg == SP) ||
-                    RCI.getLastCalleeSavedAlias(PhysReg);
+      UseOrDefCSR =
+          (!MI.isCall() && PhysReg == SP) ||
+          (RCI.getLastCalleeSavedAlias(PhysReg) &&
+           (!Reserved->test(PhysReg) || MRI.isPhysRegModified(PhysReg)));
     } else if (MO.isRegMask()) {
       // Check if this regmask clobbers any of the CSRs.
       for (unsigned Reg : getCurrentCSRs(RS)) {
Index: lib/CodeGen/StackMaps.cpp
===================================================================
--- lib/CodeGen/StackMaps.cpp	(revision 347537)
+++ lib/CodeGen/StackMaps.cpp	(working copy)
@@ -30,6 +30,7 @@
 #include "llvm/Support/ErrorHandling.h"
 #include "llvm/Support/MathExtras.h"
 #include "llvm/Support/raw_ostream.h"
+#include "llvm/Target/TargetMachine.h"
 #include <algorithm>
 #include <cassert>
 #include <cstdint>
@@ -372,7 +373,9 @@
 }
 
 void StackMaps::recordPatchPoint(const MachineInstr &MI) {
-  assert(MI.getOpcode() == TargetOpcode::PATCHPOINT && "expected patchpoint");
+  assert(((MI.getOpcode() == TargetOpcode::PATCHPOINT) ||
+          (MI.getOpcode() == TargetOpcode::TCPATCHPOINT)) &&
+         "expected patchpoint");
 
   PatchPointOpers opers(&MI);
   const int64_t ID = opers.getID();
@@ -444,7 +447,7 @@
     LLVM_DEBUG(dbgs() << WSMP << "function addr: " << FR.first
                       << " frame size: " << FR.second.StackSize
                       << " callsite count: " << FR.second.RecordCount << '\n');
-    OS.EmitSymbolValue(FR.first, 8);
+    OS.EmitSymbolValue(FR.first, AP.TM.getProgramPointerSize());
     OS.EmitIntValue(FR.second.StackSize, 8);
     OS.EmitIntValue(FR.second.RecordCount, 8);
   }
Index: lib/CodeGen/TargetInstrInfo.cpp
===================================================================
--- lib/CodeGen/TargetInstrInfo.cpp	(revision 347537)
+++ lib/CodeGen/TargetInstrInfo.cpp	(working copy)
@@ -479,6 +479,7 @@
     StartIdx = StackMapOpers(&MI).getVarIdx();
     break;
   }
+  case TargetOpcode::TCPATCHPOINT:
   case TargetOpcode::PATCHPOINT: {
     // For PatchPoint, the call args are not foldable (even if reported in the
     // stackmap e.g. via anyregcc).
@@ -573,6 +574,7 @@
 
   if (MI.getOpcode() == TargetOpcode::STACKMAP ||
       MI.getOpcode() == TargetOpcode::PATCHPOINT ||
+      MI.getOpcode() == TargetOpcode::TCPATCHPOINT ||
       MI.getOpcode() == TargetOpcode::STATEPOINT) {
     // Fold stackmap/patchpoint.
     NewMI = foldPatchpoint(MF, MI, Ops, FI, *this);
@@ -637,6 +639,7 @@
   int FrameIndex = 0;
 
   if ((MI.getOpcode() == TargetOpcode::STACKMAP ||
+       MI.getOpcode() == TargetOpcode::TCPATCHPOINT ||
        MI.getOpcode() == TargetOpcode::PATCHPOINT ||
        MI.getOpcode() == TargetOpcode::STATEPOINT) &&
       isLoadFromStackSlot(LoadMI, FrameIndex)) {
Index: lib/CodeGen/TargetPassConfig.cpp
===================================================================
--- lib/CodeGen/TargetPassConfig.cpp	(revision 347537)
+++ lib/CodeGen/TargetPassConfig.cpp	(working copy)
@@ -895,6 +895,7 @@
   if (getOptLevel() != CodeGenOpt::None)
     addBlockPlacement();
 
+  addPass(&StackMapLivenessID, false);
   addPreEmitPass();
 
   if (TM->Options.EnableIPRA)
@@ -904,7 +905,6 @@
 
   addPass(&FuncletLayoutID, false);
 
-  addPass(&StackMapLivenessID, false);
   addPass(&LiveDebugValuesID, false);
 
   // Insert before XRay Instrumentation.
Index: lib/ExecutionEngine/ExecutionEngineBindings.cpp
===================================================================
--- lib/ExecutionEngine/ExecutionEngineBindings.cpp	(revision 347537)
+++ lib/ExecutionEngine/ExecutionEngineBindings.cpp	(working copy)
@@ -200,6 +200,14 @@
          .setErrorStr(&Error)
          .setOptLevel((CodeGenOpt::Level)options.OptLevel)
          .setTargetOptions(targetOptions);
+  builder.setRelocationModel(Reloc::PIC_);
+  std::vector<std::string> AttrList;
+  AttrList.emplace_back("+armv7-a");
+  AttrList.emplace_back("+dsp");
+  AttrList.emplace_back("+neon");
+  AttrList.emplace_back("+vfp3");
+  builder.setMAttrs(AttrList);
+  builder.setMCPU("cortex-a73");
   bool JIT;
   if (Optional<CodeModel::Model> CM = unwrap(options.CodeModel, JIT))
     builder.setCodeModel(*CM);
Index: lib/IR/AsmWriter.cpp
===================================================================
--- lib/IR/AsmWriter.cpp	(revision 347537)
+++ lib/IR/AsmWriter.cpp	(working copy)
@@ -350,6 +350,8 @@
   case CallingConv::Cold:          Out << "coldcc"; break;
   case CallingConv::WebKit_JS:     Out << "webkit_jscc"; break;
   case CallingConv::AnyReg:        Out << "anyregcc"; break;
+  case CallingConv::V8CC:          Out << "v8cc"; break;
+  case CallingConv::V8SBCC:        Out << "v8sbcc"; break;
   case CallingConv::PreserveMost:  Out << "preserve_mostcc"; break;
   case CallingConv::PreserveAll:   Out << "preserve_allcc"; break;
   case CallingConv::CXX_FAST_TLS:  Out << "cxx_fast_tlscc"; break;
Index: lib/IR/Core.cpp
===================================================================
--- lib/IR/Core.cpp	(revision 347537)
+++ lib/IR/Core.cpp	(working copy)
@@ -3553,3 +3553,15 @@
 LLVMBool LLVMIsMultithreaded() {
   return llvm_is_multithreaded();
 }
+
+LLVMBool LLVMGetStatepointName(LLVMTypeRef type, char *buffer, size_t n) {
+  FunctionType *unwrapped = unwrap<FunctionType>(type);
+  PointerType *target_function_pointer_type =
+      dyn_cast<PointerType>(unwrapped->params()[2]);
+  std::string name = Intrinsic::getName(Intrinsic::experimental_gc_statepoint,
+                                        target_function_pointer_type);
+  if (name.size() >= n)
+    return false;
+  strcpy(buffer, name.c_str());
+  return true;
+}
Index: lib/MC/MCAsmStreamer.cpp
===================================================================
--- lib/MC/MCAsmStreamer.cpp	(revision 347537)
+++ lib/MC/MCAsmStreamer.cpp	(working copy)
@@ -914,7 +914,7 @@
   if (!Directive) {
     int64_t IntValue;
     if (!Value->evaluateAsAbsolute(IntValue))
-      report_fatal_error("Don't know how to emit this value.");
+      IntValue = -1;
 
     // We couldn't handle the requested integer size so we fallback by breaking
     // the request down into several, smaller, integers.
Index: lib/Target/ARM/ARMAsmPrinter.cpp
===================================================================
--- lib/Target/ARM/ARMAsmPrinter.cpp	(revision 347537)
+++ lib/Target/ARM/ARMAsmPrinter.cpp	(working copy)
@@ -55,7 +55,7 @@
 ARMAsmPrinter::ARMAsmPrinter(TargetMachine &TM,
                              std::unique_ptr<MCStreamer> Streamer)
     : AsmPrinter(TM, std::move(Streamer)), AFI(nullptr), MCP(nullptr),
-      InConstantPool(false), OptimizationGoals(-1) {}
+      InConstantPool(false), OptimizationGoals(-1), SM(*this) {}
 
 void ARMAsmPrinter::EmitFunctionBodyEnd() {
   // Make sure to terminate any constant pools that were at the end
@@ -547,6 +547,7 @@
       OutStreamer->AddBlankLine();
     }
 
+    SM.serializeToStackMapSection();
     // Funny Darwin hack: This flag tells the linker that no global symbols
     // contain code that falls through to other global symbols (e.g. the obvious
     // implementation of multiple entry points).  If this doesn't occur, the
@@ -566,6 +567,15 @@
   OptimizationGoals = -1;
 
   ATS.finishAttributeSection();
+  if (TT.isOSBinFormatCOFF()) {
+    SM.serializeToStackMapSection();
+    return;
+  }
+
+  if (TT.isOSBinFormatELF()) {
+    SM.serializeToStackMapSection();
+    return;
+  }
 }
 
 //===----------------------------------------------------------------------===//
@@ -2032,6 +2042,13 @@
   case ARM::PATCHABLE_TAIL_CALL:
     LowerPATCHABLE_TAIL_CALL(*MI);
     return;
+  case TargetOpcode::STACKMAP:
+    return LowerSTACKMAP(*OutStreamer, SM, *MI);
+  case TargetOpcode::PATCHPOINT:
+  case TargetOpcode::TCPATCHPOINT:
+    return LowerPATCHPOINT(*OutStreamer, SM, *MI);
+  case TargetOpcode::STATEPOINT:
+    return LowerSTATEPOINT(*OutStreamer, SM, *MI);
   }
 
   MCInst TmpInst;
@@ -2040,6 +2057,107 @@
   EmitToStreamer(*OutStreamer, TmpInst);
 }
 
+static unsigned roundUpTo4ByteAligned(unsigned n) {
+  unsigned mask = 3;
+  unsigned rev = ~3;
+  n = (n & rev) + (((n & mask) + mask) & rev);
+  return n;
+}
+
+void ARMAsmPrinter::LowerSTACKMAP(MCStreamer &OutStreamer, StackMaps &SM,
+                                  const MachineInstr &MI) {
+  assert(!AFI->isThumbFunction());
+  unsigned NumNOPBytes =
+      roundUpTo4ByteAligned(StackMapOpers(&MI).getNumPatchBytes());
+
+  SM.recordStackMap(MI);
+  assert(NumNOPBytes % 4 == 0 && "Invalid number of NOP bytes requested!");
+
+  // Scan ahead to trim the shadow.
+  const MachineBasicBlock &MBB = *MI.getParent();
+  MachineBasicBlock::const_iterator MII(MI);
+  ++MII;
+  while (NumNOPBytes > 0) {
+    if (MII == MBB.end() || MII->isCall() ||
+        MII->getOpcode() == ARM::DBG_VALUE ||
+        MII->getOpcode() == TargetOpcode::PATCHPOINT ||
+        MII->getOpcode() == TargetOpcode::TCPATCHPOINT ||
+        MII->getOpcode() == TargetOpcode::STACKMAP)
+      break;
+    ++MII;
+    NumNOPBytes -= 4;
+  }
+
+  // Emit nops.
+  MCInst Noop;
+  Subtarget->getInstrInfo()->getNoop(Noop);
+  for (unsigned i = 0; i < NumNOPBytes; i += 4)
+    EmitToStreamer(OutStreamer, Noop);
+}
+
+// Lower a patchpoint of the form:
+// [<def>], <id>, <numBytes>, <target>, <numArgs>
+void ARMAsmPrinter::LowerPATCHPOINT(MCStreamer &OutStreamer, StackMaps &SM,
+                                    const MachineInstr &MI) {
+  assert(!AFI->isThumbFunction());
+  SM.recordPatchPoint(MI);
+
+  PatchPointOpers Opers(&MI);
+
+  int64_t CallTarget = Opers.getCallTarget().getImm();
+  unsigned EncodedBytes = 0;
+  if (CallTarget) {
+    assert((CallTarget & 0xFFFFFFFFLL) == CallTarget &&
+           "High 32 bits of call target should be zero.");
+    unsigned ScratchReg = MI.getOperand(Opers.getNextScratchIdx()).getReg();
+    EncodedBytes = 16;
+    EmitToStreamer(OutStreamer, MCInstBuilder(ARM::MOVTi16)
+                                    .addReg(ScratchReg)
+                                    .addReg(ScratchReg)
+                                    .addImm((CallTarget >> 16) & 0xFFFF)
+                                    .addImm(ARMCC::AL)
+                                    .addImm(0));
+    EmitToStreamer(OutStreamer, MCInstBuilder(ARM::MOVi16)
+                                    .addReg(ScratchReg)
+                                    .addImm(CallTarget & 0xFFFF)
+                                    .addImm(ARMCC::AL)
+                                    .addImm(0));
+    EmitToStreamer(OutStreamer, MCInstBuilder(ARM::BLX).addReg(ScratchReg));
+  }
+  // Emit padding.
+  unsigned NumBytes = roundUpTo4ByteAligned(Opers.getNumPatchBytes());
+  assert(NumBytes >= EncodedBytes &&
+         "Patchpoint can't request size less than the length of a call.");
+  assert((NumBytes - EncodedBytes) % 4 == 0 &&
+         "Invalid number of NOP bytes requested!");
+  MCInst Noop;
+  Subtarget->getInstrInfo()->getNoop(Noop);
+  for (unsigned i = EncodedBytes; i < NumBytes; i += 4)
+    EmitToStreamer(OutStreamer, Noop);
+}
+
+void ARMAsmPrinter::LowerSTATEPOINT(MCStreamer &OutStreamer, StackMaps &SM,
+                                    const MachineInstr &MI) {
+  assert(!AFI->isThumbFunction());
+  SM.recordStatepoint(MI);
+
+  StatepointOpers SOpers(&MI);
+  MCInst Noop;
+  Subtarget->getInstrInfo()->getNoop(Noop);
+  if (unsigned PatchBytes = SOpers.getNumPatchBytes()) {
+    unsigned NumBytes = roundUpTo4ByteAligned(PatchBytes);
+    unsigned EncodedBytes = 0;
+    assert(NumBytes >= EncodedBytes &&
+           "Patchpoint can't request size less than the length of a call.");
+    assert((NumBytes - EncodedBytes) % 4 == 0 &&
+           "Invalid number of NOP bytes requested!");
+    MCInst Noop;
+    Subtarget->getInstrInfo()->getNoop(Noop);
+    for (unsigned i = EncodedBytes; i < NumBytes; i += 4)
+      EmitToStreamer(OutStreamer, Noop);
+  }
+}
+
 //===----------------------------------------------------------------------===//
 // Target Registry Stuff
 //===----------------------------------------------------------------------===//
Index: lib/Target/ARM/ARMAsmPrinter.h
===================================================================
--- lib/Target/ARM/ARMAsmPrinter.h	(revision 347537)
+++ lib/Target/ARM/ARMAsmPrinter.h	(working copy)
@@ -12,6 +12,7 @@
 
 #include "ARMSubtarget.h"
 #include "llvm/CodeGen/AsmPrinter.h"
+#include "llvm/CodeGen/StackMaps.h"
 #include "llvm/Target/TargetMachine.h"
 
 namespace llvm {
@@ -64,8 +65,10 @@
   /// Set of globals in PromotedGlobals that we've emitted labels for.
   /// We need to emit labels even for promoted globals so that DWARF
   /// debug info can link properly.
-  SmallPtrSet<const GlobalVariable*,2> EmittedPromotedGlobalLabels;
+  SmallPtrSet<const GlobalVariable *, 2> EmittedPromotedGlobalLabels;
 
+  StackMaps SM;
+
 public:
   explicit ARMAsmPrinter(TargetMachine &TM,
                          std::unique_ptr<MCStreamer> Streamer);
@@ -131,6 +134,15 @@
   bool emitPseudoExpansionLowering(MCStreamer &OutStreamer,
                                    const MachineInstr *MI);
 
+  void LowerSTACKMAP(MCStreamer &OutStreamer, StackMaps &SM,
+                     const MachineInstr &MI);
+
+  void LowerPATCHPOINT(MCStreamer &OutStreamer, StackMaps &SM,
+                       const MachineInstr &MI);
+
+  void LowerSTATEPOINT(MCStreamer &OutStreamer, StackMaps &SM,
+                       const MachineInstr &MI);
+
 public:
   unsigned getISAEncoding() override {
     // ARM/Darwin adds ISA to the DWARF info for each function.
Index: lib/Target/ARM/ARMBaseInstrInfo.cpp
===================================================================
--- lib/Target/ARM/ARMBaseInstrInfo.cpp	(revision 347537)
+++ lib/Target/ARM/ARMBaseInstrInfo.cpp	(working copy)
@@ -717,6 +717,10 @@
     return 0;
   case TargetOpcode::BUNDLE:
     return getInstBundleLength(MI);
+  case TargetOpcode::TCPATCHPOINT:
+  case TargetOpcode::PATCHPOINT:
+  case TargetOpcode::STATEPOINT:
+    return MI.getOperand(1).getImm();
   case ARM::MOVi16_ga_pcrel:
   case ARM::MOVTi16_ga_pcrel:
   case ARM::t2MOVi16_ga_pcrel:
Index: lib/Target/ARM/ARMBaseRegisterInfo.cpp
===================================================================
--- lib/Target/ARM/ARMBaseRegisterInfo.cpp	(revision 347537)
+++ lib/Target/ARM/ARMBaseRegisterInfo.cpp	(working copy)
@@ -90,6 +90,10 @@
       // exception handling.
       return CSR_GenericInt_SaveList;
     }
+  } else if (F.getCallingConv() == CallingConv::V8CC) {
+    return CSR_V8CC_SaveList;
+  } else if (F.getCallingConv() == CallingConv::V8SBCC) {
+    return CSR_V8SBCC_SaveList;
   }
 
   if (STI.getTargetLowering()->supportSwiftError() &&
@@ -124,6 +128,10 @@
   if (CC == CallingConv::GHC)
     // This is academic because all GHC calls are (supposed to be) tail calls
     return CSR_NoRegs_RegMask;
+  if (CC == CallingConv::V8CC)
+    return CSR_V8CC_RegMask;
+  if (CC == CallingConv::V8SBCC)
+    return CSR_V8SBCC_RegMask;
 
   if (STI.getTargetLowering()->supportSwiftError() &&
       MF.getFunction().getAttributes().hasAttrSomewhere(Attribute::SwiftError))
@@ -205,6 +213,18 @@
       if (Reserved.test(*SI))
         markSuperRegs(Reserved, Reg);
 
+  const Function &F = MF.getFunction();
+  if (F.getCallingConv() == CallingConv::V8SBCC) {
+    markSuperRegs(Reserved, ARM::R5);
+    markSuperRegs(Reserved, ARM::R6);
+    markSuperRegs(Reserved, ARM::R7);
+    markSuperRegs(Reserved, ARM::R8);
+    markSuperRegs(Reserved, ARM::R9);
+    markSuperRegs(Reserved, ARM::R10);
+    markSuperRegs(Reserved, ARM::R11);
+  } else if (F.getCallingConv() == CallingConv::V8CC) {
+    markSuperRegs(Reserved, ARM::R10);
+  }
   assert(checkAllSuperRegsMarked(Reserved));
   return Reserved;
 }
Index: lib/Target/ARM/ARMCallingConv.td
===================================================================
--- lib/Target/ARM/ARMCallingConv.td	(revision 347537)
+++ lib/Target/ARM/ARMCallingConv.td	(working copy)
@@ -239,6 +239,60 @@
 ]>;
 
 //===----------------------------------------------------------------------===//
+// V8 Calling Conventions
+//===----------------------------------------------------------------------===//
+
+def CC_ARM_V8: CallingConv<[
+  CCIfType<[i1, i8, i16], CCPromoteToType<i32>>,
+
+  // i64/f64 is passed in even pairs of GPRs
+  // i64 is 8-aligned i32 here, so we may need to eat R1 as a pad register
+  // (and the same is true for f64 if VFP is not enabled)
+  CCIfType<[i32], CCIfAlign<"8", CCAssignToRegWithShadow<[R0, R2], [R0, R1]>>>,
+  CCIfType<[i32], CCIf<"ArgFlags.getOrigAlign() != 8",
+                       CCAssignToReg<[R0, R1, R2, R3, R4, R5, R6, R7, R8, R9, R10, R11]>>>,
+
+  CCIfType<[i32], CCIfAlign<"8", CCAssignToStackWithShadow<4, 8, [R0, R1, R2, R3, R4, R5, R6, R7, R8, R9, R10, R11]>>>,
+  CCIfType<[i32], CCAssignToStackWithShadow<4, 4, [R0, R1, R2, R3, R4, R5, R6, R7, R8, R9, R10, R11]>>,
+  CCIfType<[f32], CCAssignToStackWithShadow<4, 4, [Q0, Q1, Q2, Q3]>>,
+  CCIfType<[f64], CCAssignToStackWithShadow<8, 8, [Q0, Q1, Q2, Q3]>>,
+  CCIfType<[v2f64], CCIfAlign<"16",
+           CCAssignToStackWithShadow<16, 16, [Q0, Q1, Q2, Q3]>>>,
+  CCIfType<[v2f64], CCAssignToStackWithShadow<16, 8, [Q0, Q1, Q2, Q3]>>
+]>;
+
+def RetCC_ARM_V8: CallingConv<[
+  CCIfType<[i1, i8, i16], CCPromoteToType<i32>>,
+  CCIfType<[i32], CCAssignToReg<[R0, R1, R2, R3]>>,
+  CCIfType<[i64], CCAssignToRegWithShadow<[R0, R2], [R1, R3]>>
+]>;
+
+def CC_ARM_V8SB: CallingConv<[
+  CCIfType<[i1, i8, i16], CCPromoteToType<i32>>,
+
+  // i64/f64 is passed in even pairs of GPRs
+  // i64 is 8-aligned i32 here, so we may need to eat R1 as a pad register
+  // (and the same is true for f64 if VFP is not enabled)
+  CCIfType<[i32], CCIfAlign<"8", CCAssignToRegWithShadow<[R0, R2], [R0, R1]>>>,
+  CCIfType<[i32], CCIf<"ArgFlags.getOrigAlign() != 8",
+                       CCAssignToReg<[R0, R1, R2, R3, R4, R10, R11, R12]>>>,
+
+  CCIfType<[i32], CCIfAlign<"8", CCAssignToStackWithShadow<4, 8, [R0, R1, R2, R3, R4, R10, R11, R12]>>>,
+  CCIfType<[i32], CCAssignToStackWithShadow<4, 4, [R0, R1, R2, R3, R4, R10, R11, R12]>>,
+  CCIfType<[f32], CCAssignToStackWithShadow<4, 4, [Q0, Q1, Q2, Q3]>>,
+  CCIfType<[f64], CCAssignToStackWithShadow<8, 8, [Q0, Q1, Q2, Q3]>>,
+  CCIfType<[v2f64], CCIfAlign<"16",
+           CCAssignToStackWithShadow<16, 16, [Q0, Q1, Q2, Q3]>>>,
+  CCIfType<[v2f64], CCAssignToStackWithShadow<16, 8, [Q0, Q1, Q2, Q3]>>
+]>;
+
+def RetCC_ARM_V8SB: CallingConv<[
+  CCIfType<[i1, i8, i16], CCPromoteToType<i32>>,
+  CCIfType<[i32], CCAssignToReg<[R0, R1, R2, R3]>>,
+  CCIfType<[i64], CCAssignToRegWithShadow<[R0, R2], [R1, R3]>>
+]>;
+
+//===----------------------------------------------------------------------===//
 // Callee-saved register lists.
 //===----------------------------------------------------------------------===//
 
@@ -248,6 +302,10 @@
 def CSR_AAPCS : CalleeSavedRegs<(add LR, R11, R10, R9, R8, R7, R6, R5, R4,
                                      (sequence "D%u", 15, 8))>;
 
+def CSR_V8CC : CalleeSavedRegs<(add LR, R11, R10)>;
+
+def CSR_V8SBCC : CalleeSavedRegs<(add LR, R11, R10, R9, R8, R7, R6, R5)>;
+
 // R8 is used to pass swifterror, remove it from CSR.
 def CSR_AAPCS_SwiftError : CalleeSavedRegs<(sub CSR_AAPCS, R8)>;
 
Index: lib/Target/ARM/ARMConstantIslandPass.cpp
===================================================================
--- lib/Target/ARM/ARMConstantIslandPass.cpp	(revision 347537)
+++ lib/Target/ARM/ARMConstantIslandPass.cpp	(working copy)
@@ -241,6 +241,7 @@
     CPEntry *findConstPoolEntry(unsigned CPI, const MachineInstr *CPEMI);
     unsigned getCPELogAlign(const MachineInstr *CPEMI);
     void scanFunctionJumpTables();
+    bool replaceInlineAsm();
     void initializeFunctionInfo(const std::vector<MachineInstr*> &CPEMIs);
     MachineBasicBlock *splitBlockBeforeInstr(MachineInstr *MI);
     void updateForInsertedWaterBlock(MachineBasicBlock *NewBB);
@@ -367,7 +368,7 @@
 
   // Try to reorder and otherwise adjust the block layout to make good use
   // of the TB[BH] instructions.
-  bool MadeChange = false;
+  bool MadeChange = replaceInlineAsm();
   if (GenerateTBB && AdjustJumpTableBlocks) {
     scanFunctionJumpTables();
     MadeChange |= reorderThumb2JumpTables();
@@ -678,6 +679,49 @@
   }
 }
 
+bool ARMConstantIslands::replaceInlineAsm() {
+  bool MadeChange = false;
+  std::vector<MachineInstr *> toRemove;
+  for (MachineBasicBlock &MBB : *MF) {
+    for (MachineInstr &MI : MBB) {
+      if (MI.getOpcode() == TargetOpcode::INLINEASM) {
+        // handle const pool load
+        if (!strcmp(MI.getOperand(0).getSymbolName(), "ldr $0, =${1:c}")) {
+          MachineFunction *MF = MBB.getParent();
+          MachineConstantPool *ConstantPool = MF->getConstantPool();
+          Type *Int32Ty = Type::getInt32Ty(MF->getFunction().getContext());
+          const Constant *C =
+              ConstantInt::get(Int32Ty, MI.getOperand(5).getImm());
+
+          // MachineConstantPool wants an explicit alignment.
+          unsigned Align = MF->getDataLayout().getPrefTypeAlignment(Int32Ty);
+          if (Align == 0)
+            Align = MF->getDataLayout().getTypeAllocSize(C->getType());
+          unsigned Idx = ConstantPool->getConstantPoolIndex(C, Align);
+
+          if (isThumb)
+            BuildMI(MBB, MI, MI.getDebugLoc(), TII->get(ARM::tLDRpci))
+                .addReg(MI.getOperand(3).getReg(), RegState::Define)
+                .addConstantPoolIndex(Idx)
+                .add(predOps(ARMCC::AL));
+          else
+            BuildMI(MBB, MI, MI.getDebugLoc(), TII->get(ARM::LDRcp))
+                .addReg(MI.getOperand(3).getReg(), RegState::Define)
+                .addConstantPoolIndex(Idx)
+                .addImm(0)
+                .add(predOps(ARMCC::AL));
+          toRemove.push_back(&MI);
+          MadeChange = true;
+        }
+      }
+    }
+  }
+  for (MachineInstr *MI : toRemove) {
+    MI->eraseFromParent();
+  }
+  return MadeChange;
+}
+
 /// initializeFunctionInfo - Do the initial scan of the function, building up
 /// information about the sizes of each block, the location of all the water,
 /// and finding all of the constant pool users.
Index: lib/Target/ARM/ARMFastISel.cpp
===================================================================
--- lib/Target/ARM/ARMFastISel.cpp	(revision 347537)
+++ lib/Target/ARM/ARMFastISel.cpp	(working copy)
@@ -1890,6 +1890,10 @@
       report_fatal_error("Can't return in GHC call convention");
     else
       return CC_ARM_APCS_GHC;
+  case CallingConv::V8CC:
+    return (Return ? RetCC_ARM_V8 : CC_ARM_V8);
+  case CallingConv::V8SBCC:
+    return (Return ? RetCC_ARM_V8SB : CC_ARM_V8SB);
   }
 }
 
Index: lib/Target/ARM/ARMFrameLowering.cpp
===================================================================
--- lib/Target/ARM/ARMFrameLowering.cpp	(revision 347537)
+++ lib/Target/ARM/ARMFrameLowering.cpp	(working copy)
@@ -380,6 +380,7 @@
   unsigned GPRCS1Size = 0, GPRCS2Size = 0, DPRCSSize = 0;
   int FramePtrSpillFI = 0;
   int D8SpillFI = 0;
+  bool IsJSFamily = false;
 
   // All calls are tail calls in GHC calling conv, and functions have no
   // prologue/epilogue.
@@ -408,6 +409,16 @@
     return;
   }
 
+  if (AFI->isJSStub()) {
+    GPRCS1Size += 4;
+    IsJSFamily = true;
+    FramePtr = ARM::R11;
+  } else if (AFI->isJSFunction()) {
+    GPRCS1Size += 12;
+    IsJSFamily = true;
+    FramePtr = ARM::R11;
+  }
+
   // Determine spill area sizes.
   for (unsigned i = 0, e = CSI.size(); i != e; ++i) {
     unsigned Reg = CSI[i].getReg();
@@ -459,7 +470,7 @@
   unsigned DPRGapSize = (GPRCS1Size + GPRCS2Size + ArgRegsSaveSize) % DPRAlign;
   unsigned DPRCSOffset = GPRCS2Offset - DPRGapSize - DPRCSSize;
   int FramePtrOffsetInPush = 0;
-  if (HasFP) {
+  if (HasFP || IsJSFamily) {
     int FPOffset = MFI.getObjectOffset(FramePtrSpillFI);
     assert(getMaxFPOffset(MF.getFunction(), *AFI) <= FPOffset &&
            "Max FP estimation is wrong");
@@ -559,7 +570,7 @@
 
   if (NumBytes) {
     // Adjust SP after all the callee-save spills.
-    if (AFI->getNumAlignedDPRCS2Regs() == 0 &&
+    if (AFI->getNumAlignedDPRCS2Regs() == 0 && !IsJSFamily &&
         tryFoldSPUpdateIntoPushPop(STI, MF, &*LastPush, NumBytes))
       DefCFAOffsetCandidates.addExtraBytes(LastPush, NumBytes);
     else {
@@ -586,7 +597,7 @@
   // into spill area 1, including the FP in R11.  In either case, it
   // is in area one and the adjustment needs to take place just after
   // that push.
-  if (HasFP) {
+  if (HasFP || IsJSFamily) {
     MachineBasicBlock::iterator AfterPush = std::next(GPRCS1Push);
     unsigned PushSize = sizeOfSPAdjustment(*GPRCS1Push);
     emitRegPlusImmediate(!AFI->isThumbFunction(), MBB, AfterPush,
@@ -608,6 +619,19 @@
           .addCFIIndex(CFIIndex)
           .setMIFlags(MachineInstr::FrameSetup);
     }
+    if (AFI->isJSStub()) {
+      MachineBasicBlock::iterator MI = AfterPush;
+      BuildMI(MBB, MI, dl, TII.get(ARM::MOVi), ARM::R12)
+          .addImm(22)
+          .add(predOps(ARMCC::AL))
+          .add(condCodeOp());
+      BuildMI(MBB, MI, dl, TII.get(ARM::STR_PRE_IMM), ARM::SP)
+          .addReg(ARM::R12, RegState::Kill)
+          .addReg(ARM::SP)
+          .setMIFlags(MachineInstr::NoFlags)
+          .addImm(-4)
+          .add(predOps(ARMCC::AL));
+    }
   }
 
   // Now that the prologue's actual instructions are finalised, we can insert
@@ -806,6 +830,11 @@
                  AFI->getGPRCalleeSavedArea2Size() +
                  AFI->getDPRCalleeSavedGapSize() +
                  AFI->getDPRCalleeSavedAreaSize());
+    if (AFI->isJSStub()) {
+      NumBytes += 4;
+    } else if (AFI->isJSFunction()) {
+      NumBytes += 12;
+    }
 
     // Reset SP based on frame pointer only if the stack frame extends beyond
     // frame pointer stack slot or target is ELF and the function has FP.
@@ -1054,7 +1083,9 @@
   if (MBB.end() != MI) {
     DL = MI->getDebugLoc();
     unsigned RetOpcode = MI->getOpcode();
-    isTailCall = (RetOpcode == ARM::TCRETURNdi || RetOpcode == ARM::TCRETURNri);
+    isTailCall =
+        (RetOpcode == ARM::TCRETURNdi || RetOpcode == ARM::TCRETURNri ||
+         RetOpcode == TargetOpcode::TCPATCHPOINT);
     isInterrupt =
         RetOpcode == ARM::SUBS_PC_LR || RetOpcode == ARM::t2SUBS_PC_LR;
     isTrap =
@@ -1428,8 +1459,18 @@
     ARM::t2STR_PRE : ARM::STR_PRE_IMM;
   unsigned FltOpc = ARM::VSTMDDB_UPD;
   unsigned NumAlignedDPRCS2Regs = AFI->getNumAlignedDPRCS2Regs();
-  emitPushInst(MBB, MI, CSI, PushOpc, PushOneOpc, false, &isARMArea1Register, 0,
-               MachineInstr::FrameSetup);
+  if (AFI->isJSFunction()) {
+    std::vector<CalleeSavedInfo> FakeCSI;
+    FakeCSI.emplace_back(ARM::R0, 0);
+    FakeCSI.emplace_back(ARM::R1, 0);
+    FakeCSI.emplace_back(ARM::R7, 0);
+    std::copy(CSI.begin(), CSI.end(), std::back_inserter(FakeCSI));
+    emitPushInst(MBB, MI, FakeCSI, PushOpc, PushOneOpc, false,
+                 &isARMArea1Register, 0, MachineInstr::FrameSetup);
+  } else {
+    emitPushInst(MBB, MI, CSI, PushOpc, PushOneOpc, false, &isARMArea1Register,
+                 0, MachineInstr::FrameSetup);
+  }
   emitPushInst(MBB, MI, CSI, PushOpc, PushOneOpc, false, &isARMArea2Register, 0,
                MachineInstr::FrameSetup);
   emitPushInst(MBB, MI, CSI, FltOpc, 0, true, &isARMArea3Register,
@@ -1460,7 +1501,6 @@
   // registers. Do that here instead.
   if (NumAlignedDPRCS2Regs)
     emitAlignedDPRCS2Restores(MBB, MI, NumAlignedDPRCS2Regs, CSI, TRI);
-
   unsigned PopOpc = AFI->isThumbFunction() ? ARM::t2LDMIA_UPD : ARM::LDMIA_UPD;
   unsigned LdrOpc = AFI->isThumbFunction() ? ARM::t2LDR_POST :ARM::LDR_POST_IMM;
   unsigned FltOpc = ARM::VLDMDIA_UPD;
@@ -1610,6 +1650,20 @@
   MachineRegisterInfo &MRI = MF.getRegInfo();
   const TargetRegisterInfo *TRI = MF.getSubtarget().getRegisterInfo();
   (void)TRI;  // Silence unused warning in non-assert builds.
+
+  const Function &F = MF.getFunction();
+  if (F.hasFnAttribute("js-function-call"))
+    AFI->setJSFunction(true);
+  else if (F.hasFnAttribute("js-stub-call"))
+    AFI->setJSStub(true);
+
+  if (AFI->isJSFunction() || AFI->isJSStub()) {
+    assert((F.getCallingConv() == CallingConv::V8CC) ||
+           (F.getCallingConv() == CallingConv::V8SBCC));
+    SavedRegs.set(ARM::R11);
+    SavedRegs.set(ARM::LR);
+    CanEliminateFrame = false;
+  }
   unsigned FramePtr = RegInfo->getFrameRegister(MF);
 
   // Spill R4 if Thumb2 function requires stack realignment - it will be used as
@@ -1782,7 +1836,10 @@
     AFI->setHasStackFrame(true);
 
     if (HasFP) {
-      SavedRegs.set(FramePtr);
+      if (!SavedRegs.test(FramePtr)) {
+        SavedRegs.set(FramePtr);
+        NumGPRSpills++;
+      }
       // If the frame pointer is required by the ABI, also spill LR so that we
       // emit a complete frame record.
       if (MF.getTarget().Options.DisableFramePointerElim(MF) && !LRSpilled) {
@@ -1796,7 +1853,6 @@
       auto FPPos = llvm::find(UnspilledCS1GPRs, FramePtr);
       if (FPPos != UnspilledCS1GPRs.end())
         UnspilledCS1GPRs.erase(FPPos);
-      NumGPRSpills++;
       if (FramePtr == ARM::R7)
         CS1Spilled = true;
     }
@@ -2499,3 +2555,25 @@
   MF.verify();
 #endif
 }
+
+bool ARMFrameLowering::assignCalleeSavedSpillSlots(
+    MachineFunction &MF, const TargetRegisterInfo *TRI,
+    std::vector<CalleeSavedInfo> &CSI) const {
+  ARMFunctionInfo *AFI = MF.getInfo<ARMFunctionInfo>();
+  // default handle.
+  if (!AFI->isJSStub() && !AFI->isJSFunction())
+    return false;
+  assert(CSI.size() == 2 && CSI[0].getReg() == ARM::LR &&
+         CSI[1].getReg() == ARM::R11);
+  MachineFrameInfo &MFI = MF.getFrameInfo();
+  CSI[0].setFrameIdx(MFI.CreateFixedSpillStackObject(4, -4));
+  CSI[1].setFrameIdx(MFI.CreateFixedSpillStackObject(4, -8));
+  if (AFI->isJSStub()) {
+    MFI.CreateFixedSpillStackObject(4, -12);
+  } else {
+    MFI.CreateFixedSpillStackObject(4, -12); // cp
+    MFI.CreateFixedSpillStackObject(4, -16); // function
+    MFI.CreateFixedSpillStackObject(4, -20); // arg count.
+  }
+  return true;
+}
Index: lib/Target/ARM/ARMFrameLowering.h
===================================================================
--- lib/Target/ARM/ARMFrameLowering.h	(revision 347537)
+++ lib/Target/ARM/ARMFrameLowering.h	(working copy)
@@ -65,6 +65,11 @@
     return true;
   }
 
+  bool
+  assignCalleeSavedSpillSlots(MachineFunction &MF,
+                              const TargetRegisterInfo *TRI,
+                              std::vector<CalleeSavedInfo> &CSI) const override;
+
 private:
   void emitPushInst(MachineBasicBlock &MBB, MachineBasicBlock::iterator MI,
                     const std::vector<CalleeSavedInfo> &CSI, unsigned StmOpc,
Index: lib/Target/ARM/ARMISelLowering.cpp
===================================================================
--- lib/Target/ARM/ARMISelLowering.cpp	(revision 347537)
+++ lib/Target/ARM/ARMISelLowering.cpp	(working copy)
@@ -1578,6 +1578,10 @@
   switch (CC) {
   default:
     report_fatal_error("Unsupported calling convention");
+  case CallingConv::V8CC:
+    return CallingConv::V8CC;
+  case CallingConv::V8SBCC:
+    return CallingConv::V8SBCC;
   case CallingConv::ARM_AAPCS:
   case CallingConv::ARM_APCS:
   case CallingConv::GHC:
@@ -1627,6 +1631,10 @@
   switch (getEffectiveCallingConv(CC, isVarArg)) {
   default:
     report_fatal_error("Unsupported calling convention");
+  case CallingConv::V8SBCC:
+    return (Return ? RetCC_ARM_V8SB : CC_ARM_V8SB);
+  case CallingConv::V8CC:
+    return (Return ? RetCC_ARM_V8 : CC_ARM_V8);
   case CallingConv::ARM_APCS:
     return (Return ? RetCC_ARM_APCS : CC_ARM_APCS);
   case CallingConv::ARM_AAPCS:
@@ -9256,6 +9264,17 @@
     llvm_unreachable("Unexpected instr type to insert");
   }
 
+  case TargetOpcode::STATEPOINT:
+    // As an implementation detail, STATEPOINT shares the STACKMAP format at
+    // this point in the process.  We diverge later.
+    return emitPatchPoint(MI, BB);
+
+  case TargetOpcode::STACKMAP:
+  case TargetOpcode::PATCHPOINT:
+    return emitPatchPoint(MI, BB);
+  case TargetOpcode::TCPATCHPOINT:
+    return TargetLoweringBase::emitPatchPoint(MI, BB);
+
   // Thumb1 post-indexed loads are really just single-register LDMs.
   case ARM::tLDR_postidx: {
     MachineOperand Def(MI.getOperand(1));
@@ -13061,6 +13080,23 @@
   return -1;
 }
 
+const MCPhysReg *
+ARMTargetLowering::getScratchRegisters(CallingConv::ID CC) const {
+  if (CC == CallingConv::V8SBCC)
+    return nullptr;
+  static const MCPhysReg ScratchRegs[] = {ARM::R12, 0};
+  return ScratchRegs;
+}
+
+MachineBasicBlock *
+ARMTargetLowering::emitPatchPoint(MachineInstr &MI,
+                                  MachineBasicBlock *MBB) const {
+  MachineBasicBlock *MBB2 = TargetLoweringBase::emitPatchPoint(MI, MBB);
+  MachineFunction &MF = *MI.getMF();
+  MI.addOperand(MF, MachineOperand::CreateReg(ARM::LR, true, true));
+  return MBB2;
+}
+
 static bool isLegalT1AddressImmediate(int64_t V, EVT VT) {
   if (V < 0)
     return false;
Index: lib/Target/ARM/ARMISelLowering.h
===================================================================
--- lib/Target/ARM/ARMISelLowering.h	(revision 347537)
+++ lib/Target/ARM/ARMISelLowering.h	(working copy)
@@ -696,6 +696,11 @@
     /// correctness bugs.
     bool isFMAFasterThanFMulAndFAdd(EVT VT) const override { return false; }
 
+    const MCPhysReg *getScratchRegisters(CallingConv::ID CC) const override;
+
+    MachineBasicBlock *emitPatchPoint(MachineInstr &MI,
+                                      MachineBasicBlock *MBB) const;
+
     SDValue ReconstructShuffle(SDValue Op, SelectionDAG &DAG) const;
 
     SDValue LowerCallResult(SDValue Chain, SDValue InFlag,
Index: lib/Target/ARM/ARMMachineFunctionInfo.h
===================================================================
--- lib/Target/ARM/ARMMachineFunctionInfo.h	(revision 347537)
+++ lib/Target/ARM/ARMMachineFunctionInfo.h	(working copy)
@@ -127,6 +127,9 @@
   /// The amount the literal pool has been increasedby due to promoted globals.
   int PromotedGlobalsIncrease = 0;
 
+  bool IsJSFunction = false;
+  bool IsJSStub = false;
+
 public:
   ARMFunctionInfo() = default;
 
@@ -239,6 +242,10 @@
   void setPromotedConstpoolIncrease(int Sz) {
     PromotedGlobalsIncrease = Sz;
   }
+  bool isJSFunction() const { return IsJSFunction; }
+  bool isJSStub() const { return IsJSStub; }
+  void setJSFunction(bool s) { IsJSFunction = s; }
+  void setJSStub(bool s) { IsJSStub = s; }
 };
 
 } // end namespace llvm
Index: tools/llvm-shlib/CMakeLists.txt
===================================================================
--- tools/llvm-shlib/CMakeLists.txt	(revision 347537)
+++ tools/llvm-shlib/CMakeLists.txt	(working copy)
@@ -58,6 +58,7 @@
 endif()
 
 target_link_libraries(LLVM PRIVATE ${LIB_NAMES})
+set_target_properties(LLVM PROPERTIES LINK_FLAGS "-static-libstdc++")
 
 if (APPLE)
   set_property(TARGET LLVM APPEND_STRING PROPERTY
Index: tools/llvm-shlib/simple_version_script.map.in
===================================================================
--- tools/llvm-shlib/simple_version_script.map.in	(revision 347537)
+++ tools/llvm-shlib/simple_version_script.map.in	(working copy)
@@ -1 +1 @@
-LLVM_@LLVM_VERSION_MAJOR@ { global: *; };
+LLVM_@LLVM_VERSION_MAJOR@ { global: LLVM*;_ZN4llvm13linkCoreCLRGCEv; local:*; };
