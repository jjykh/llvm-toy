Index: include/llvm/IR/CallingConv.h
===================================================================
--- include/llvm/IR/CallingConv.h	(revision 359070)
+++ include/llvm/IR/CallingConv.h	(working copy)
@@ -220,8 +220,14 @@
     /// shader if tessellation is in use, or otherwise the vertex shader.
     AMDGPU_ES = 96,
 
+    // Calling convention for v8
+    V8CC = 97,
+
+    // Calling convention for v8 store barrier stub
+    V8SBCC = 98,
+
     // Calling convention between AArch64 Advanced SIMD functions
-    AArch64_VectorCall = 97,
+    AArch64_VectorCall = 99,
 
     /// The highest possible calling convention ID. Must be some 2^k - 1.
     MaxID = 1023
Index: include/llvm/Support/TargetOpcodes.def
===================================================================
--- include/llvm/Support/TargetOpcodes.def	(revision 359070)
+++ include/llvm/Support/TargetOpcodes.def	(working copy)
@@ -120,6 +120,7 @@
 /// rewrite calls to runtimes with more efficient code sequences.
 /// This also implies a stack map.
 HANDLE_TARGET_OPCODE(PATCHPOINT)
+HANDLE_TARGET_OPCODE(TCPATCHPOINT)
 
 /// This pseudo-instruction loads the stack guard value. Targets which need
 /// to prevent the stack guard value or address from being spilled to the
Index: include/llvm/Target/Target.td
===================================================================
--- include/llvm/Target/Target.td	(revision 359070)
+++ include/llvm/Target/Target.td	(working copy)
@@ -1068,6 +1068,17 @@
   let mayLoad = 1;
   let usesCustomInserter = 1;
 }
+def TCPATCHPOINT : StandardPseudoInstruction {
+  let OutOperandList = (outs);
+  let InOperandList = (ins i64imm:$id, i32imm:$nbytes, unknown:$callee,
+                       i32imm:$nargs, i32imm:$cc, variable_ops);
+  let hasSideEffects = 1;
+  let isCall = 1;
+  let mayLoad = 1;
+  let usesCustomInserter = 1;
+  let isReturn = 1;
+  let isTerminator = 1;
+}
 def STATEPOINT : StandardPseudoInstruction {
   let OutOperandList = (outs);
   let InOperandList = (ins variable_ops);
Index: include/llvm-c/Core.h
===================================================================
--- include/llvm-c/Core.h	(revision 359070)
+++ include/llvm-c/Core.h	(working copy)
@@ -245,7 +245,9 @@
   LLVMAMDGPUHSCallConv      = 93,
   LLVMMSP430BUILTINCallConv = 94,
   LLVMAMDGPULSCallConv      = 95,
-  LLVMAMDGPUESCallConv      = 96
+  LLVMAMDGPUESCallConv      = 96,
+  LLVMV8CallConv            = 97,
+  LLVMV8SBCallConv          = 98
 } LLVMCallConv;
 
 typedef enum {
@@ -3887,6 +3889,8 @@
     @see llvm::llvm_is_multithreaded */
 LLVMBool LLVMIsMultithreaded(void);
 
+LLVMBool LLVMGetStatepointName(LLVMTypeRef type, char *buffer, size_t n);
+
 /**
  * @}
  */
Index: lib/AsmParser/LLLexer.cpp
===================================================================
--- lib/AsmParser/LLLexer.cpp	(revision 359070)
+++ lib/AsmParser/LLLexer.cpp	(working copy)
@@ -607,6 +607,8 @@
   KEYWORD(webkit_jscc);
   KEYWORD(swiftcc);
   KEYWORD(anyregcc);
+  KEYWORD(v8cc);
+  KEYWORD(v8sbcc);
   KEYWORD(preserve_mostcc);
   KEYWORD(preserve_allcc);
   KEYWORD(ghccc);
Index: lib/AsmParser/LLParser.cpp
===================================================================
--- lib/AsmParser/LLParser.cpp	(revision 359070)
+++ lib/AsmParser/LLParser.cpp	(working copy)
@@ -1931,6 +1931,12 @@
   case lltok::kw_win64cc:        CC = CallingConv::Win64; break;
   case lltok::kw_webkit_jscc:    CC = CallingConv::WebKit_JS; break;
   case lltok::kw_anyregcc:       CC = CallingConv::AnyReg; break;
+  case lltok::kw_v8cc:
+    CC = CallingConv::V8CC;
+    break;
+  case lltok::kw_v8sbcc:
+    CC = CallingConv::V8SBCC;
+    break;
   case lltok::kw_preserve_mostcc:CC = CallingConv::PreserveMost; break;
   case lltok::kw_preserve_allcc: CC = CallingConv::PreserveAll; break;
   case lltok::kw_ghccc:          CC = CallingConv::GHC; break;
Index: lib/AsmParser/LLToken.h
===================================================================
--- lib/AsmParser/LLToken.h	(revision 359070)
+++ lib/AsmParser/LLToken.h	(working copy)
@@ -151,6 +151,8 @@
   kw_win64cc,
   kw_webkit_jscc,
   kw_anyregcc,
+  kw_v8cc,
+  kw_v8sbcc,
   kw_swiftcc,
   kw_preserve_mostcc,
   kw_preserve_allcc,
Index: lib/CodeGen/InlineSpiller.cpp
===================================================================
--- lib/CodeGen/InlineSpiller.cpp	(revision 359070)
+++ lib/CodeGen/InlineSpiller.cpp	(working copy)
@@ -57,6 +57,7 @@
 #include <cassert>
 #include <iterator>
 #include <tuple>
+#include <unordered_set>
 #include <utility>
 #include <vector>
 
@@ -213,6 +214,7 @@
   bool isSibling(unsigned Reg);
   bool hoistSpillInsideBB(LiveInterval &SpillLI, MachineInstr &CopyMI);
   void eliminateRedundantSpills(LiveInterval &LI, VNInfo *VNI);
+  void foldStatePoints(unsigned Reg);
 
   void markValueUsed(LiveInterval*, VNInfo*);
   bool reMaterializeFor(LiveInterval &, MachineInstr &MI);
@@ -924,11 +926,60 @@
     HSpiller.addToMergeableSpills(*std::next(MI), StackSlot, Original);
 }
 
+void InlineSpiller::foldStatePoints(unsigned InputReg) {
+  SmallVector<unsigned, 8> WorkList;
+  std::unordered_set<unsigned> VisitedSet;
+  // Don't add Reg to WorkList, spillAroundUses will handle it.
+  for (MachineRegisterInfo::reg_bundle_iterator
+           RegI = MRI.reg_bundle_begin(InputReg),
+           E = MRI.reg_bundle_end();
+       RegI != E;) {
+    MachineInstr &MI = *RegI++;
+    unsigned SibReg = isFullCopyOf(MI, InputReg);
+    if (SibReg && isSibling(SibReg) && SibReg != InputReg) {
+      WorkList.push_back(SibReg);
+      VisitedSet.emplace(SibReg);
+    }
+  }
+  VisitedSet.emplace(InputReg);
+  while (!WorkList.empty()) {
+    unsigned Reg = WorkList.pop_back_val();
+    // Fold the use of state point.
+    for (MachineRegisterInfo::use_instr_nodbg_iterator
+             UI = MRI.use_instr_nodbg_begin(Reg),
+             E = MRI.use_instr_nodbg_end();
+         UI != E;) {
+      MachineInstr &MI = *UI++;
+      if (unsigned DstReg = isFullCopyOf(MI, Reg)) {
+        if (isSibling(DstReg)) {
+          auto pair = VisitedSet.emplace(DstReg);
+          if (pair.second)
+            WorkList.push_back(DstReg);
+        }
+        continue;
+      }
+      // Handle STATEPOINT, PATCHPOINT
+      switch (MI.getOpcode()) {
+      case TargetOpcode::STATEPOINT:
+      case TargetOpcode::PATCHPOINT: {
+        // Analyze instruction.
+        SmallVector<std::pair<MachineInstr *, unsigned>, 8> Ops;
+        MIBundleOperands(MI).analyzeVirtReg(Reg, &Ops);
+        foldMemoryOperand(Ops);
+      } break;
+      default:
+        break;
+      }
+    }
+  }
+}
+
 /// spillAroundUses - insert spill code around each use of Reg.
 void InlineSpiller::spillAroundUses(unsigned Reg) {
   LLVM_DEBUG(dbgs() << "spillAroundUses " << printReg(Reg) << '\n');
   LiveInterval &OldLI = LIS.getInterval(Reg);
 
+  foldStatePoints(Reg);
   // Iterate over instructions using Reg.
   for (MachineRegisterInfo::reg_bundle_iterator
        RegI = MRI.reg_bundle_begin(Reg), E = MRI.reg_bundle_end();
Index: lib/CodeGen/LocalStackSlotAllocation.cpp
===================================================================
--- lib/CodeGen/LocalStackSlotAllocation.cpp	(revision 359070)
+++ lib/CodeGen/LocalStackSlotAllocation.cpp	(working copy)
@@ -299,6 +299,7 @@
       // range, so they don't need any updates.
       if (MI.isDebugInstr() || MI.getOpcode() == TargetOpcode::STATEPOINT ||
           MI.getOpcode() == TargetOpcode::STACKMAP ||
+          MI.getOpcode() == TargetOpcode::TCPATCHPOINT ||
           MI.getOpcode() == TargetOpcode::PATCHPOINT)
         continue;
 
Index: lib/CodeGen/MachineVerifier.cpp
===================================================================
--- lib/CodeGen/MachineVerifier.cpp	(revision 359070)
+++ lib/CodeGen/MachineVerifier.cpp	(working copy)
@@ -1203,6 +1203,8 @@
   unsigned NumDefs = MCID.getNumDefs();
   if (MCID.getOpcode() == TargetOpcode::PATCHPOINT)
     NumDefs = (MONum == 0 && MO->isReg()) ? NumDefs : 0;
+  else if (MCID.getOpcode() == TargetOpcode::TCPATCHPOINT)
+    NumDefs = 0;
 
   // The first MCID.NumDefs operands must be explicit register defines
   if (MONum < NumDefs) {
Index: lib/CodeGen/SelectionDAG/InstrEmitter.cpp
===================================================================
--- lib/CodeGen/SelectionDAG/InstrEmitter.cpp	(revision 359070)
+++ lib/CodeGen/SelectionDAG/InstrEmitter.cpp	(working copy)
@@ -832,7 +832,8 @@
   const MCPhysReg *ScratchRegs = nullptr;
 
   // Handle STACKMAP and PATCHPOINT specially and then use the generic code.
-  if (Opc == TargetOpcode::STACKMAP || Opc == TargetOpcode::PATCHPOINT) {
+  if (Opc == TargetOpcode::STACKMAP || Opc == TargetOpcode::PATCHPOINT ||
+      Opc == TargetOpcode::TCPATCHPOINT) {
     // Stackmaps do not have arguments and do not preserve their calling
     // convention. However, to simplify runtime support, they clobber the same
     // scratch registers as AnyRegCC.
Index: lib/CodeGen/SelectionDAG/ScheduleDAGSDNodes.cpp
===================================================================
--- lib/CodeGen/SelectionDAG/ScheduleDAGSDNodes.cpp	(revision 359070)
+++ lib/CodeGen/SelectionDAG/ScheduleDAGSDNodes.cpp	(working copy)
@@ -538,6 +538,10 @@
     NodeNumDefs = 0;
     return;
   }
+  if (POpc == TargetOpcode::TCPATCHPOINT) {
+    NodeNumDefs = 0;
+    return;
+  }
   if (POpc == TargetOpcode::PATCHPOINT &&
       Node->getValueType(0) == MVT::Other) {
     // PATCHPOINT is defined to have one result, but it might really have none
Index: lib/CodeGen/SelectionDAG/SelectionDAGBuilder.cpp
===================================================================
--- lib/CodeGen/SelectionDAG/SelectionDAGBuilder.cpp	(revision 359070)
+++ lib/CodeGen/SelectionDAG/SelectionDAGBuilder.cpp	(working copy)
@@ -1221,6 +1221,16 @@
   SDValue &N = NodeMap[V];
   if (N.getNode()) return N;
 
+  // Special handling BitCastInst with a constant.
+  if (const BitCastInst *I = dyn_cast<BitCastInst>(V)) {
+    if (const Constant *C = dyn_cast<Constant>(I->getOperand(0))) {
+      SDValue Val = getValueImpl(C);
+      NodeMap[V] = Val;
+      resolveDanglingDebugInfo(V, Val);
+      return Val;
+    }
+  }
+
   // If there's a virtual register allocated and initialized for this
   // value, use it.
   if (SDValue copyFromReg = getCopyFromRegs(V, V->getType()))
@@ -8187,6 +8197,32 @@
   FuncInfo.MF->getFrameInfo().setHasStackMap();
 }
 
+static bool isPatchpointInTailCallPosition(ImmutableCallSite CS) {
+  const Instruction *I = CS.getInstruction();
+  const BasicBlock *ExitBB = I->getParent();
+  const Instruction *Term = ExitBB->getTerminator();
+  const ReturnInst *Ret = dyn_cast<ReturnInst>(Term);
+  // If Term is not a ReturnInst, then it must be a UnreachableInst.
+  if (!Ret && !isa<UnreachableInst>(Term))
+    return false;
+  // Copy from isInTailCallPosition.
+  // If I will have a chain, make sure no other instruction that will have a
+  // chain interposes between I and the return.
+  if (I->mayHaveSideEffects() || I->mayReadFromMemory() ||
+      !isSafeToSpeculativelyExecute(I))
+    for (BasicBlock::const_iterator BBI = std::prev(ExitBB->end(), 2);; --BBI) {
+      if (&*BBI == I)
+        break;
+      // Debug info intrinsics do not get in the way of tail call optimization.
+      if (isa<DbgInfoIntrinsic>(BBI))
+        continue;
+      if (BBI->mayHaveSideEffects() || BBI->mayReadFromMemory() ||
+          !isSafeToSpeculativelyExecute(&*BBI))
+        return false;
+    }
+  return true;
+}
+
 /// Lower llvm.experimental.patchpoint directly to its target opcode.
 void SelectionDAGBuilder::visitPatchpoint(ImmutableCallSite CS,
                                           const BasicBlock *EHPadBB) {
@@ -8230,17 +8266,24 @@
   TargetLowering::CallLoweringInfo CLI(DAG);
   populateCallLoweringInfo(CLI, CS, NumMetaOpers, NumCallArgs, Callee, ReturnTy,
                            true);
+  CLI.IsTailCall = isPatchpointInTailCallPosition(CS);
+  assert((!CLI.IsTailCall || !HasDef) &&
+         "TailCall should not has a return type");
   std::pair<SDValue, SDValue> Result = lowerInvokable(CLI, EHPadBB);
+  SDNode *Call;
+  if (!CLI.IsTailCall) {
+    SDNode *CallEnd = Result.second.getNode();
+    if (HasDef && (CallEnd->getOpcode() == ISD::CopyFromReg))
+      CallEnd = CallEnd->getOperand(0).getNode();
 
-  SDNode *CallEnd = Result.second.getNode();
-  if (HasDef && (CallEnd->getOpcode() == ISD::CopyFromReg))
-    CallEnd = CallEnd->getOperand(0).getNode();
-
-  /// Get a call instruction from the call sequence chain.
-  /// Tail calls are not allowed.
-  assert(CallEnd->getOpcode() == ISD::CALLSEQ_END &&
-         "Expected a callseq node.");
-  SDNode *Call = CallEnd->getOperand(0).getNode();
+    /// Get a call instruction from the call sequence chain.
+    /// Tail calls are not allowed.
+    assert(CallEnd->getOpcode() == ISD::CALLSEQ_END &&
+           "Expected a callseq node.");
+    Call = CallEnd->getOperand(0).getNode();
+  } else {
+    Call = CLI.Chain.getNode();
+  }
   bool HasGlue = Call->getGluedNode();
 
   // Replace the target specific call node with the patchable intrinsic.
@@ -8311,8 +8354,9 @@
     NodeTys = DAG.getVTList(MVT::Other, MVT::Glue);
 
   // Replace the target specific call node with a PATCHPOINT node.
-  MachineSDNode *MN = DAG.getMachineNode(TargetOpcode::PATCHPOINT,
-                                         dl, NodeTys, Ops);
+  MachineSDNode *MN = DAG.getMachineNode(
+      CLI.IsTailCall ? TargetOpcode::TCPATCHPOINT : TargetOpcode::PATCHPOINT,
+      dl, NodeTys, Ops);
 
   // Update the NodeMap.
   if (HasDef) {
Index: lib/CodeGen/SelectionDAG/StatepointLowering.cpp
===================================================================
--- lib/CodeGen/SelectionDAG/StatepointLowering.cpp	(revision 359070)
+++ lib/CodeGen/SelectionDAG/StatepointLowering.cpp	(working copy)
@@ -541,14 +541,15 @@
   // arrays interwoven with each (lowered) base pointer immediately followed by
   // it's (lowered) derived pointer.  i.e
   // (base[0], ptr[0], base[1], ptr[1], ...)
+  bool LiveInOnly = SI.CLI.CallConv == CallingConv::V8CC;
   for (unsigned i = 0; i < SI.Bases.size(); ++i) {
     const Value *Base = SI.Bases[i];
-    lowerIncomingStatepointValue(Builder.getValue(Base), /*LiveInOnly*/ false,
-                                 Ops, Builder);
+    lowerIncomingStatepointValue(Builder.getValue(Base), LiveInOnly, Ops,
+                                 Builder);
 
     const Value *Ptr = SI.Ptrs[i];
-    lowerIncomingStatepointValue(Builder.getValue(Ptr), /*LiveInOnly*/ false,
-                                 Ops, Builder);
+    lowerIncomingStatepointValue(Builder.getValue(Ptr), LiveInOnly, Ops,
+                                 Builder);
   }
 
   // If there are any explicit spill slots passed to the statepoint, record
@@ -827,6 +828,11 @@
 
     unsigned AS = ISP.getCalledValue()->getType()->getPointerAddressSpace();
     ActualCallee = DAG.getConstant(0, getCurSDLoc(), TLI.getPointerTy(DL, AS));
+    if (auto *ConstCallee =
+            dyn_cast<ConstantSDNode>(getValue(ISP.getCalledValue())))
+      ActualCallee =
+          DAG.getIntPtrConstant(ConstCallee->getZExtValue(), getCurSDLoc(),
+                                /*isTarget=*/true);
   } else {
     ActualCallee = getValue(ISP.getCalledValue());
   }
Index: lib/CodeGen/SelectionDAG/TargetLowering.cpp
===================================================================
--- lib/CodeGen/SelectionDAG/TargetLowering.cpp	(revision 359070)
+++ lib/CodeGen/SelectionDAG/TargetLowering.cpp	(working copy)
@@ -89,6 +89,9 @@
     // (We look for a CopyFromReg reading a virtual register that is used
     //  for the function live-in value of register Reg)
     SDValue Value = OutVals[I];
+    // Ignore the undefined input.
+    if (Value->getOpcode() == ISD::UNDEF)
+      continue;
     if (Value->getOpcode() != ISD::CopyFromReg)
       return false;
     unsigned ArgReg = cast<RegisterSDNode>(Value->getOperand(1))->getReg();
Index: lib/CodeGen/StackMaps.cpp
===================================================================
--- lib/CodeGen/StackMaps.cpp	(revision 359070)
+++ lib/CodeGen/StackMaps.cpp	(working copy)
@@ -30,6 +30,7 @@
 #include "llvm/Support/ErrorHandling.h"
 #include "llvm/Support/MathExtras.h"
 #include "llvm/Support/raw_ostream.h"
+#include "llvm/Target/TargetMachine.h"
 #include <algorithm>
 #include <cassert>
 #include <cstdint>
@@ -371,7 +372,9 @@
 }
 
 void StackMaps::recordPatchPoint(const MachineInstr &MI) {
-  assert(MI.getOpcode() == TargetOpcode::PATCHPOINT && "expected patchpoint");
+  assert(((MI.getOpcode() == TargetOpcode::PATCHPOINT) ||
+          (MI.getOpcode() == TargetOpcode::TCPATCHPOINT)) &&
+         "expected patchpoint");
 
   PatchPointOpers opers(&MI);
   const int64_t ID = opers.getID();
@@ -443,7 +446,7 @@
     LLVM_DEBUG(dbgs() << WSMP << "function addr: " << FR.first
                       << " frame size: " << FR.second.StackSize
                       << " callsite count: " << FR.second.RecordCount << '\n');
-    OS.EmitSymbolValue(FR.first, 8);
+    OS.EmitSymbolValue(FR.first, AP.TM.getProgramPointerSize());
     OS.EmitIntValue(FR.second.StackSize, 8);
     OS.EmitIntValue(FR.second.RecordCount, 8);
   }
Index: lib/CodeGen/TargetInstrInfo.cpp
===================================================================
--- lib/CodeGen/TargetInstrInfo.cpp	(revision 359070)
+++ lib/CodeGen/TargetInstrInfo.cpp	(working copy)
@@ -468,6 +468,7 @@
     StartIdx = StackMapOpers(&MI).getVarIdx();
     break;
   }
+  case TargetOpcode::TCPATCHPOINT:
   case TargetOpcode::PATCHPOINT: {
     // For PatchPoint, the call args are not foldable (even if reported in the
     // stackmap e.g. via anyregcc).
@@ -562,6 +563,7 @@
 
   if (MI.getOpcode() == TargetOpcode::STACKMAP ||
       MI.getOpcode() == TargetOpcode::PATCHPOINT ||
+      MI.getOpcode() == TargetOpcode::TCPATCHPOINT ||
       MI.getOpcode() == TargetOpcode::STATEPOINT) {
     // Fold stackmap/patchpoint.
     NewMI = foldPatchpoint(MF, MI, Ops, FI, *this);
@@ -626,6 +628,7 @@
   int FrameIndex = 0;
 
   if ((MI.getOpcode() == TargetOpcode::STACKMAP ||
+       MI.getOpcode() == TargetOpcode::TCPATCHPOINT ||
        MI.getOpcode() == TargetOpcode::PATCHPOINT ||
        MI.getOpcode() == TargetOpcode::STATEPOINT) &&
       isLoadFromStackSlot(LoadMI, FrameIndex)) {
Index: lib/CodeGen/TargetPassConfig.cpp
===================================================================
--- lib/CodeGen/TargetPassConfig.cpp	(revision 359070)
+++ lib/CodeGen/TargetPassConfig.cpp	(working copy)
@@ -954,6 +954,7 @@
   if (getOptLevel() != CodeGenOpt::None)
     addBlockPlacement();
 
+  addPass(&StackMapLivenessID, false);
   addPreEmitPass();
 
   if (TM->Options.EnableIPRA)
@@ -963,7 +964,6 @@
 
   addPass(&FuncletLayoutID, false);
 
-  addPass(&StackMapLivenessID, false);
   addPass(&LiveDebugValuesID, false);
 
   // Insert before XRay Instrumentation.
Index: lib/ExecutionEngine/ExecutionEngineBindings.cpp
===================================================================
--- lib/ExecutionEngine/ExecutionEngineBindings.cpp	(revision 359070)
+++ lib/ExecutionEngine/ExecutionEngineBindings.cpp	(working copy)
@@ -200,6 +200,14 @@
          .setErrorStr(&Error)
          .setOptLevel((CodeGenOpt::Level)options.OptLevel)
          .setTargetOptions(targetOptions);
+  builder.setRelocationModel(Reloc::PIC_);
+  std::vector<std::string> AttrList;
+  AttrList.emplace_back("+armv7-a");
+  AttrList.emplace_back("+dsp");
+  AttrList.emplace_back("+neon");
+  AttrList.emplace_back("+vfp3");
+  AttrList.emplace_back("+hwdiv-arm");
+  builder.setMAttrs(AttrList);
   bool JIT;
   if (Optional<CodeModel::Model> CM = unwrap(options.CodeModel, JIT))
     builder.setCodeModel(*CM);
Index: lib/IR/AsmWriter.cpp
===================================================================
--- lib/IR/AsmWriter.cpp	(revision 359070)
+++ lib/IR/AsmWriter.cpp	(working copy)
@@ -349,6 +349,12 @@
   case CallingConv::Cold:          Out << "coldcc"; break;
   case CallingConv::WebKit_JS:     Out << "webkit_jscc"; break;
   case CallingConv::AnyReg:        Out << "anyregcc"; break;
+  case CallingConv::V8CC:
+    Out << "v8cc";
+    break;
+  case CallingConv::V8SBCC:
+    Out << "v8sbcc";
+    break;
   case CallingConv::PreserveMost:  Out << "preserve_mostcc"; break;
   case CallingConv::PreserveAll:   Out << "preserve_allcc"; break;
   case CallingConv::CXX_FAST_TLS:  Out << "cxx_fast_tlscc"; break;
Index: lib/IR/Core.cpp
===================================================================
--- lib/IR/Core.cpp	(revision 359070)
+++ lib/IR/Core.cpp	(working copy)
@@ -3921,3 +3921,15 @@
 LLVMBool LLVMIsMultithreaded() {
   return llvm_is_multithreaded();
 }
+
+LLVMBool LLVMGetStatepointName(LLVMTypeRef type, char *buffer, size_t n) {
+  FunctionType *unwrapped = unwrap<FunctionType>(type);
+  PointerType *target_function_pointer_type =
+      dyn_cast<PointerType>(unwrapped->params()[2]);
+  std::string name = Intrinsic::getName(Intrinsic::experimental_gc_statepoint,
+                                        target_function_pointer_type);
+  if (name.size() >= n)
+    return false;
+  strcpy(buffer, name.c_str());
+  return true;
+}
Index: lib/MC/MCAsmStreamer.cpp
===================================================================
--- lib/MC/MCAsmStreamer.cpp	(revision 359070)
+++ lib/MC/MCAsmStreamer.cpp	(working copy)
@@ -941,7 +941,7 @@
   if (!Directive) {
     int64_t IntValue;
     if (!Value->evaluateAsAbsolute(IntValue))
-      report_fatal_error("Don't know how to emit this value.");
+      IntValue = -1;
 
     // We couldn't handle the requested integer size so we fallback by breaking
     // the request down into several, smaller, integers.
Index: lib/Target/ARM/ARMAsmPrinter.cpp
===================================================================
--- lib/Target/ARM/ARMAsmPrinter.cpp	(revision 359070)
+++ lib/Target/ARM/ARMAsmPrinter.cpp	(working copy)
@@ -55,7 +55,7 @@
 ARMAsmPrinter::ARMAsmPrinter(TargetMachine &TM,
                              std::unique_ptr<MCStreamer> Streamer)
     : AsmPrinter(TM, std::move(Streamer)), AFI(nullptr), MCP(nullptr),
-      InConstantPool(false), OptimizationGoals(-1) {}
+      InConstantPool(false), OptimizationGoals(-1), SM(*this) {}
 
 void ARMAsmPrinter::EmitFunctionBodyEnd() {
   // Make sure to terminate any constant pools that were at the end
@@ -559,6 +559,7 @@
       OutStreamer->AddBlankLine();
     }
 
+    SM.serializeToStackMapSection();
     // Funny Darwin hack: This flag tells the linker that no global symbols
     // contain code that falls through to other global symbols (e.g. the obvious
     // implementation of multiple entry points).  If this doesn't occur, the
@@ -578,6 +579,15 @@
   OptimizationGoals = -1;
 
   ATS.finishAttributeSection();
+  if (TT.isOSBinFormatCOFF()) {
+    SM.serializeToStackMapSection();
+    return;
+  }
+
+  if (TT.isOSBinFormatELF()) {
+    SM.serializeToStackMapSection();
+    return;
+  }
 }
 
 //===----------------------------------------------------------------------===//
@@ -2064,6 +2074,13 @@
   case ARM::PATCHABLE_TAIL_CALL:
     LowerPATCHABLE_TAIL_CALL(*MI);
     return;
+  case TargetOpcode::STACKMAP:
+    return LowerSTACKMAP(*OutStreamer, SM, *MI);
+  case TargetOpcode::PATCHPOINT:
+  case TargetOpcode::TCPATCHPOINT:
+    return LowerPATCHPOINT(*OutStreamer, SM, *MI);
+  case TargetOpcode::STATEPOINT:
+    return LowerSTATEPOINT(*OutStreamer, SM, *MI);
   }
 
   MCInst TmpInst;
@@ -2072,6 +2089,107 @@
   EmitToStreamer(*OutStreamer, TmpInst);
 }
 
+static unsigned roundUpTo4ByteAligned(unsigned n) {
+  unsigned mask = 3;
+  unsigned rev = ~3;
+  n = (n & rev) + (((n & mask) + mask) & rev);
+  return n;
+}
+
+void ARMAsmPrinter::LowerSTACKMAP(MCStreamer &OutStreamer, StackMaps &SM,
+                                  const MachineInstr &MI) {
+  assert(!AFI->isThumbFunction());
+  unsigned NumNOPBytes =
+      roundUpTo4ByteAligned(StackMapOpers(&MI).getNumPatchBytes());
+
+  SM.recordStackMap(MI);
+  assert(NumNOPBytes % 4 == 0 && "Invalid number of NOP bytes requested!");
+
+  // Scan ahead to trim the shadow.
+  const MachineBasicBlock &MBB = *MI.getParent();
+  MachineBasicBlock::const_iterator MII(MI);
+  ++MII;
+  while (NumNOPBytes > 0) {
+    if (MII == MBB.end() || MII->isCall() ||
+        MII->getOpcode() == ARM::DBG_VALUE ||
+        MII->getOpcode() == TargetOpcode::PATCHPOINT ||
+        MII->getOpcode() == TargetOpcode::TCPATCHPOINT ||
+        MII->getOpcode() == TargetOpcode::STACKMAP)
+      break;
+    ++MII;
+    NumNOPBytes -= 4;
+  }
+
+  // Emit nops.
+  MCInst Noop;
+  Subtarget->getInstrInfo()->getNoop(Noop);
+  for (unsigned i = 0; i < NumNOPBytes; i += 4)
+    EmitToStreamer(OutStreamer, Noop);
+}
+
+// Lower a patchpoint of the form:
+// [<def>], <id>, <numBytes>, <target>, <numArgs>
+void ARMAsmPrinter::LowerPATCHPOINT(MCStreamer &OutStreamer, StackMaps &SM,
+                                    const MachineInstr &MI) {
+  assert(!AFI->isThumbFunction());
+  SM.recordPatchPoint(MI);
+
+  PatchPointOpers Opers(&MI);
+
+  int64_t CallTarget = Opers.getCallTarget().getImm();
+  unsigned EncodedBytes = 0;
+  if (CallTarget) {
+    assert((CallTarget & 0xFFFFFFFFLL) == CallTarget &&
+           "High 32 bits of call target should be zero.");
+    unsigned ScratchReg = MI.getOperand(Opers.getNextScratchIdx()).getReg();
+    EncodedBytes = 16;
+    EmitToStreamer(OutStreamer, MCInstBuilder(ARM::MOVTi16)
+                                    .addReg(ScratchReg)
+                                    .addReg(ScratchReg)
+                                    .addImm((CallTarget >> 16) & 0xFFFF)
+                                    .addImm(ARMCC::AL)
+                                    .addImm(0));
+    EmitToStreamer(OutStreamer, MCInstBuilder(ARM::MOVi16)
+                                    .addReg(ScratchReg)
+                                    .addImm(CallTarget & 0xFFFF)
+                                    .addImm(ARMCC::AL)
+                                    .addImm(0));
+    EmitToStreamer(OutStreamer, MCInstBuilder(ARM::BLX).addReg(ScratchReg));
+  }
+  // Emit padding.
+  unsigned NumBytes = roundUpTo4ByteAligned(Opers.getNumPatchBytes());
+  assert(NumBytes >= EncodedBytes &&
+         "Patchpoint can't request size less than the length of a call.");
+  assert((NumBytes - EncodedBytes) % 4 == 0 &&
+         "Invalid number of NOP bytes requested!");
+  MCInst Noop;
+  Subtarget->getInstrInfo()->getNoop(Noop);
+  for (unsigned i = EncodedBytes; i < NumBytes; i += 4)
+    EmitToStreamer(OutStreamer, Noop);
+}
+
+void ARMAsmPrinter::LowerSTATEPOINT(MCStreamer &OutStreamer, StackMaps &SM,
+                                    const MachineInstr &MI) {
+  assert(!AFI->isThumbFunction());
+  SM.recordStatepoint(MI);
+
+  StatepointOpers SOpers(&MI);
+  MCInst Noop;
+  Subtarget->getInstrInfo()->getNoop(Noop);
+  if (unsigned PatchBytes = SOpers.getNumPatchBytes()) {
+    unsigned NumBytes = roundUpTo4ByteAligned(PatchBytes);
+    unsigned EncodedBytes = 0;
+    assert(NumBytes >= EncodedBytes &&
+           "Patchpoint can't request size less than the length of a call.");
+    assert((NumBytes - EncodedBytes) % 4 == 0 &&
+           "Invalid number of NOP bytes requested!");
+    MCInst Noop;
+    Subtarget->getInstrInfo()->getNoop(Noop);
+    for (unsigned i = EncodedBytes; i < NumBytes; i += 4)
+      EmitToStreamer(OutStreamer, Noop);
+  }
+}
+
 //===----------------------------------------------------------------------===//
 // Target Registry Stuff
 //===----------------------------------------------------------------------===//
Index: lib/Target/ARM/ARMAsmPrinter.h
===================================================================
--- lib/Target/ARM/ARMAsmPrinter.h	(revision 359070)
+++ lib/Target/ARM/ARMAsmPrinter.h	(working copy)
@@ -12,6 +12,7 @@
 
 #include "ARMSubtarget.h"
 #include "llvm/CodeGen/AsmPrinter.h"
+#include "llvm/CodeGen/StackMaps.h"
 #include "llvm/Target/TargetMachine.h"
 
 namespace llvm {
@@ -64,8 +65,10 @@
   /// Set of globals in PromotedGlobals that we've emitted labels for.
   /// We need to emit labels even for promoted globals so that DWARF
   /// debug info can link properly.
-  SmallPtrSet<const GlobalVariable*,2> EmittedPromotedGlobalLabels;
+  SmallPtrSet<const GlobalVariable *, 2> EmittedPromotedGlobalLabels;
 
+  StackMaps SM;
+
 public:
   explicit ARMAsmPrinter(TargetMachine &TM,
                          std::unique_ptr<MCStreamer> Streamer);
@@ -131,6 +134,15 @@
   bool emitPseudoExpansionLowering(MCStreamer &OutStreamer,
                                    const MachineInstr *MI);
 
+  void LowerSTACKMAP(MCStreamer &OutStreamer, StackMaps &SM,
+                     const MachineInstr &MI);
+
+  void LowerPATCHPOINT(MCStreamer &OutStreamer, StackMaps &SM,
+                       const MachineInstr &MI);
+
+  void LowerSTATEPOINT(MCStreamer &OutStreamer, StackMaps &SM,
+                       const MachineInstr &MI);
+
 public:
   unsigned getISAEncoding() override {
     // ARM/Darwin adds ISA to the DWARF info for each function.
Index: lib/Target/ARM/ARMBaseInstrInfo.cpp
===================================================================
--- lib/Target/ARM/ARMBaseInstrInfo.cpp	(revision 359070)
+++ lib/Target/ARM/ARMBaseInstrInfo.cpp	(working copy)
@@ -721,6 +721,10 @@
     return 0;
   case TargetOpcode::BUNDLE:
     return getInstBundleLength(MI);
+  case TargetOpcode::TCPATCHPOINT:
+  case TargetOpcode::PATCHPOINT:
+  case TargetOpcode::STATEPOINT:
+    return MI.getOperand(1).getImm();
   case ARM::MOVi16_ga_pcrel:
   case ARM::MOVTi16_ga_pcrel:
   case ARM::t2MOVi16_ga_pcrel:
@@ -1462,6 +1466,49 @@
   BB->erase(MI);
 }
 
+void ARMBaseInstrInfo::expandRESTORESP(MachineBasicBlock::iterator MI) const {
+  MachineBasicBlock *BB = MI->getParent();
+  bool ShouldExpand = true;
+  MachineBasicBlock::iterator it = MI;
+  ++it;
+  MachineBasicBlock::iterator end = MI->getParent()->end();
+  for (; it != end; ++it) {
+    bool IsDef = false, IsUse = false;
+    for (unsigned i = 0, e = it->getNumOperands(); i != e; ++i) {
+      const MachineOperand &MO = it->getOperand(i);
+      if (!MO.isReg())
+        continue;
+      unsigned MOReg = MO.getReg();
+      if (MOReg != ARM::SP)
+        continue;
+      // Define new sp
+      if (MO.isDef())
+        IsDef = true;
+      else
+        IsUse = true;
+    }
+    // Should expand now.
+    if (IsUse)
+      break;
+    if (IsDef) {
+      ShouldExpand = false;
+      break;
+    }
+  }
+  if (ShouldExpand) {
+    MachineFunction *MF = BB->getParent();
+    DebugLoc dl = MI->getDebugLoc();
+
+    ARMFunctionInfo *AFI = MF->getInfo<ARMFunctionInfo>();
+    BuildMI(*BB, MI, dl, get(ARM::SUBri), ARM::SP)
+        .addReg(ARM::R11)
+        .addImm(AFI->getFramePtrSpillOffset())
+        .add(predOps(ARMCC::AL))
+        .add(condCodeOp());
+  }
+  BB->erase(MI);
+}
+
 bool ARMBaseInstrInfo::expandPostRAPseudo(MachineInstr &MI) const {
   if (MI.getOpcode() == TargetOpcode::LOAD_STACK_GUARD) {
     assert(getSubtarget().getTargetTriple().isOSBinFormatMachO() &&
@@ -1476,6 +1523,11 @@
     return true;
   }
 
+  if (MI.getOpcode() == ARM::RESTORESP) {
+    expandRESTORESP(MI);
+    return true;
+  }
+
   // This hook gets to expand COPY instructions before they become
   // copyPhysReg() calls.  Look for VMOVS instructions that can legally be
   // widened to VMOVD.  We prefer the VMOVD when possible because it may be
Index: lib/Target/ARM/ARMBaseInstrInfo.h
===================================================================
--- lib/Target/ARM/ARMBaseInstrInfo.h	(revision 359070)
+++ lib/Target/ARM/ARMBaseInstrInfo.h	(working copy)
@@ -398,6 +398,7 @@
   virtual void expandLoadStackGuard(MachineBasicBlock::iterator MI) const = 0;
 
   void expandMEMCPY(MachineBasicBlock::iterator) const;
+  void expandRESTORESP(MachineBasicBlock::iterator) const;
 
 private:
   /// Modeling special VFP / NEON fp MLA / MLS hazards.
Index: lib/Target/ARM/ARMBaseRegisterInfo.cpp
===================================================================
--- lib/Target/ARM/ARMBaseRegisterInfo.cpp	(revision 359070)
+++ lib/Target/ARM/ARMBaseRegisterInfo.cpp	(working copy)
@@ -90,6 +90,10 @@
       // exception handling.
       return CSR_GenericInt_SaveList;
     }
+  } else if (F.getCallingConv() == CallingConv::V8CC) {
+    return CSR_V8CC_SaveList;
+  } else if (F.getCallingConv() == CallingConv::V8SBCC) {
+    return CSR_V8SBCC_SaveList;
   }
 
   if (STI.getTargetLowering()->supportSwiftError() &&
@@ -124,6 +128,10 @@
   if (CC == CallingConv::GHC)
     // This is academic because all GHC calls are (supposed to be) tail calls
     return CSR_NoRegs_RegMask;
+  if (CC == CallingConv::V8CC)
+    return CSR_V8CC_RegMask;
+  if (CC == CallingConv::V8SBCC)
+    return CSR_V8SBCC_RegMask;
 
   if (STI.getTargetLowering()->supportSwiftError() &&
       MF.getFunction().getAttributes().hasAttrSomewhere(Attribute::SwiftError))
@@ -205,6 +213,19 @@
       if (Reserved.test(*SI))
         markSuperRegs(Reserved, Reg);
 
+  const Function &F = MF.getFunction();
+  if (F.getCallingConv() == CallingConv::V8SBCC) {
+    markSuperRegs(Reserved, ARM::R5);
+    markSuperRegs(Reserved, ARM::R6);
+    markSuperRegs(Reserved, ARM::R7);
+    markSuperRegs(Reserved, ARM::R8);
+    markSuperRegs(Reserved, ARM::R9);
+    markSuperRegs(Reserved, ARM::R10);
+    markSuperRegs(Reserved, ARM::R11);
+  } else if (F.getCallingConv() == CallingConv::V8CC) {
+    markSuperRegs(Reserved, ARM::R10);
+    markSuperRegs(Reserved, ARM::R11);
+  }
   assert(checkAllSuperRegsMarked(Reserved));
   return Reserved;
 }
Index: lib/Target/ARM/ARMCallingConv.td
===================================================================
--- lib/Target/ARM/ARMCallingConv.td	(revision 359070)
+++ lib/Target/ARM/ARMCallingConv.td	(working copy)
@@ -239,6 +239,60 @@
 ]>;
 
 //===----------------------------------------------------------------------===//
+// V8 Calling Conventions
+//===----------------------------------------------------------------------===//
+
+def CC_ARM_V8: CallingConv<[
+  CCIfType<[i1, i8, i16], CCPromoteToType<i32>>,
+
+  // i64/f64 is passed in even pairs of GPRs
+  // i64 is 8-aligned i32 here, so we may need to eat R1 as a pad register
+  // (and the same is true for f64 if VFP is not enabled)
+  CCIfType<[i32], CCIfAlign<"8", CCAssignToRegWithShadow<[R0, R2], [R0, R1]>>>,
+  CCIfType<[i32], CCIf<"ArgFlags.getOrigAlign() != 8",
+                       CCAssignToReg<[R0, R1, R2, R3, R4, R5, R6, R7, R8, R9, R10, R11]>>>,
+
+  CCIfType<[i32], CCIfAlign<"8", CCAssignToStackWithShadow<4, 8, [R0, R1, R2, R3, R4, R5, R6, R7, R8, R9, R10, R11]>>>,
+  CCIfType<[i32], CCAssignToStackWithShadow<4, 4, [R0, R1, R2, R3, R4, R5, R6, R7, R8, R9, R10, R11]>>,
+  CCIfType<[f32], CCAssignToStackWithShadow<4, 4, [Q0, Q1, Q2, Q3]>>,
+  CCIfType<[f64], CCAssignToStackWithShadow<8, 8, [Q0, Q1, Q2, Q3]>>,
+  CCIfType<[v2f64], CCIfAlign<"16",
+           CCAssignToStackWithShadow<16, 16, [Q0, Q1, Q2, Q3]>>>,
+  CCIfType<[v2f64], CCAssignToStackWithShadow<16, 8, [Q0, Q1, Q2, Q3]>>
+]>;
+
+def RetCC_ARM_V8: CallingConv<[
+  CCIfType<[i1, i8, i16], CCPromoteToType<i32>>,
+  CCIfType<[i32], CCAssignToReg<[R0, R1, R2, R3]>>,
+  CCIfType<[i64], CCAssignToRegWithShadow<[R0, R2], [R1, R3]>>
+]>;
+
+def CC_ARM_V8SB: CallingConv<[
+  CCIfType<[i1, i8, i16], CCPromoteToType<i32>>,
+
+  // i64/f64 is passed in even pairs of GPRs
+  // i64 is 8-aligned i32 here, so we may need to eat R1 as a pad register
+  // (and the same is true for f64 if VFP is not enabled)
+  CCIfType<[i32], CCIfAlign<"8", CCAssignToRegWithShadow<[R0, R2], [R0, R1]>>>,
+  CCIfType<[i32], CCIf<"ArgFlags.getOrigAlign() != 8",
+                       CCAssignToReg<[R0, R1, R2, R3, R4, R10, R11, R12]>>>,
+
+  CCIfType<[i32], CCIfAlign<"8", CCAssignToStackWithShadow<4, 8, [R0, R1, R2, R3, R4, R10, R11, R12]>>>,
+  CCIfType<[i32], CCAssignToStackWithShadow<4, 4, [R0, R1, R2, R3, R4, R10, R11, R12]>>,
+  CCIfType<[f32], CCAssignToStackWithShadow<4, 4, [Q0, Q1, Q2, Q3]>>,
+  CCIfType<[f64], CCAssignToStackWithShadow<8, 8, [Q0, Q1, Q2, Q3]>>,
+  CCIfType<[v2f64], CCIfAlign<"16",
+           CCAssignToStackWithShadow<16, 16, [Q0, Q1, Q2, Q3]>>>,
+  CCIfType<[v2f64], CCAssignToStackWithShadow<16, 8, [Q0, Q1, Q2, Q3]>>
+]>;
+
+def RetCC_ARM_V8SB: CallingConv<[
+  CCIfType<[i1, i8, i16], CCPromoteToType<i32>>,
+  CCIfType<[i32], CCAssignToReg<[R0, R1, R2, R3]>>,
+  CCIfType<[i64], CCAssignToRegWithShadow<[R0, R2], [R1, R3]>>
+]>;
+
+//===----------------------------------------------------------------------===//
 // Callee-saved register lists.
 //===----------------------------------------------------------------------===//
 
@@ -248,6 +302,10 @@
 def CSR_AAPCS : CalleeSavedRegs<(add LR, R11, R10, R9, R8, R7, R6, R5, R4,
                                      (sequence "D%u", 15, 8))>;
 
+def CSR_V8CC : CalleeSavedRegs<(add LR, R11)>;
+
+def CSR_V8SBCC : CalleeSavedRegs<(add LR, R11, R10, R9, R8, R7, R6, R5)>;
+
 // R8 is used to pass swifterror, remove it from CSR.
 def CSR_AAPCS_SwiftError : CalleeSavedRegs<(sub CSR_AAPCS, R8)>;
 
Index: lib/Target/ARM/ARMConstantIslandPass.cpp
===================================================================
--- lib/Target/ARM/ARMConstantIslandPass.cpp	(revision 359070)
+++ lib/Target/ARM/ARMConstantIslandPass.cpp	(working copy)
@@ -241,6 +241,7 @@
     CPEntry *findConstPoolEntry(unsigned CPI, const MachineInstr *CPEMI);
     unsigned getCPELogAlign(const MachineInstr *CPEMI);
     void scanFunctionJumpTables();
+    bool replaceInlineAsm();
     void initializeFunctionInfo(const std::vector<MachineInstr*> &CPEMIs);
     MachineBasicBlock *splitBlockBeforeInstr(MachineInstr *MI);
     void updateForInsertedWaterBlock(MachineBasicBlock *NewBB);
@@ -367,7 +368,7 @@
 
   // Try to reorder and otherwise adjust the block layout to make good use
   // of the TB[BH] instructions.
-  bool MadeChange = false;
+  bool MadeChange = replaceInlineAsm();
   if (GenerateTBB && AdjustJumpTableBlocks) {
     scanFunctionJumpTables();
     MadeChange |= reorderThumb2JumpTables();
@@ -678,6 +679,49 @@
   }
 }
 
+bool ARMConstantIslands::replaceInlineAsm() {
+  bool MadeChange = false;
+  std::vector<MachineInstr *> toRemove;
+  for (MachineBasicBlock &MBB : *MF) {
+    for (MachineInstr &MI : MBB) {
+      if (MI.getOpcode() == TargetOpcode::INLINEASM) {
+        // handle const pool load
+        if (!strcmp(MI.getOperand(0).getSymbolName(), "ldr $0, =${1:c}")) {
+          MachineFunction *MF = MBB.getParent();
+          MachineConstantPool *ConstantPool = MF->getConstantPool();
+          Type *Int32Ty = Type::getInt32Ty(MF->getFunction().getContext());
+          const Constant *C =
+              ConstantInt::get(Int32Ty, MI.getOperand(5).getImm());
+
+          // MachineConstantPool wants an explicit alignment.
+          unsigned Align = MF->getDataLayout().getPrefTypeAlignment(Int32Ty);
+          if (Align == 0)
+            Align = MF->getDataLayout().getTypeAllocSize(C->getType());
+          unsigned Idx = ConstantPool->getConstantPoolIndex(C, Align);
+
+          if (isThumb)
+            BuildMI(MBB, MI, MI.getDebugLoc(), TII->get(ARM::tLDRpci))
+                .addReg(MI.getOperand(3).getReg(), RegState::Define)
+                .addConstantPoolIndex(Idx)
+                .add(predOps(ARMCC::AL));
+          else
+            BuildMI(MBB, MI, MI.getDebugLoc(), TII->get(ARM::LDRcp))
+                .addReg(MI.getOperand(3).getReg(), RegState::Define)
+                .addConstantPoolIndex(Idx)
+                .addImm(0)
+                .add(predOps(ARMCC::AL));
+          toRemove.push_back(&MI);
+          MadeChange = true;
+        }
+      }
+    }
+  }
+  for (MachineInstr *MI : toRemove) {
+    MI->eraseFromParent();
+  }
+  return MadeChange;
+}
+
 /// initializeFunctionInfo - Do the initial scan of the function, building up
 /// information about the sizes of each block, the location of all the water,
 /// and finding all of the constant pool users.
Index: lib/Target/ARM/ARMFastISel.cpp
===================================================================
--- lib/Target/ARM/ARMFastISel.cpp	(revision 359070)
+++ lib/Target/ARM/ARMFastISel.cpp	(working copy)
@@ -1890,6 +1890,10 @@
       report_fatal_error("Can't return in GHC call convention");
     else
       return CC_ARM_APCS_GHC;
+  case CallingConv::V8CC:
+    return (Return ? RetCC_ARM_V8 : CC_ARM_V8);
+  case CallingConv::V8SBCC:
+    return (Return ? RetCC_ARM_V8SB : CC_ARM_V8SB);
   }
 }
 
Index: lib/Target/ARM/ARMFrameLowering.cpp
===================================================================
--- lib/Target/ARM/ARMFrameLowering.cpp	(revision 359070)
+++ lib/Target/ARM/ARMFrameLowering.cpp	(working copy)
@@ -379,6 +379,7 @@
   unsigned GPRCS1Size = 0, GPRCS2Size = 0, DPRCSSize = 0;
   int FramePtrSpillFI = 0;
   int D8SpillFI = 0;
+  bool IsJSFamily = false;
 
   // All calls are tail calls in GHC calling conv, and functions have no
   // prologue/epilogue.
@@ -407,6 +408,16 @@
     return;
   }
 
+  if (AFI->isJSStub()) {
+    GPRCS1Size += 4;
+    IsJSFamily = true;
+    FramePtr = ARM::R11;
+  } else if (AFI->isJSFunction()) {
+    GPRCS1Size += 12;
+    IsJSFamily = true;
+    FramePtr = ARM::R11;
+  }
+
   // Determine spill area sizes.
   for (unsigned i = 0, e = CSI.size(); i != e; ++i) {
     unsigned Reg = CSI[i].getReg();
@@ -458,7 +469,7 @@
   unsigned DPRGapSize = (GPRCS1Size + GPRCS2Size + ArgRegsSaveSize) % DPRAlign;
   unsigned DPRCSOffset = GPRCS2Offset - DPRGapSize - DPRCSSize;
   int FramePtrOffsetInPush = 0;
-  if (HasFP) {
+  if (HasFP || IsJSFamily) {
     int FPOffset = MFI.getObjectOffset(FramePtrSpillFI);
     assert(getMaxFPOffset(MF.getFunction(), *AFI) <= FPOffset &&
            "Max FP estimation is wrong");
@@ -560,7 +571,7 @@
 
   if (NumBytes) {
     // Adjust SP after all the callee-save spills.
-    if (AFI->getNumAlignedDPRCS2Regs() == 0 &&
+    if (AFI->getNumAlignedDPRCS2Regs() == 0 && !IsJSFamily &&
         tryFoldSPUpdateIntoPushPop(STI, MF, &*LastPush, NumBytes))
       DefCFAOffsetCandidates.addExtraBytes(LastPush, NumBytes);
     else {
@@ -587,7 +598,7 @@
   // into spill area 1, including the FP in R11.  In either case, it
   // is in area one and the adjustment needs to take place just after
   // that push.
-  if (HasFP) {
+  if (HasFP || IsJSFamily) {
     MachineBasicBlock::iterator AfterPush = std::next(GPRCS1Push);
     unsigned PushSize = sizeOfSPAdjustment(*GPRCS1Push);
     emitRegPlusImmediate(!AFI->isThumbFunction(), MBB, AfterPush,
@@ -609,6 +620,19 @@
           .addCFIIndex(CFIIndex)
           .setMIFlags(MachineInstr::FrameSetup);
     }
+    if (AFI->isJSStub()) {
+      MachineBasicBlock::iterator MI = AfterPush;
+      BuildMI(MBB, MI, dl, TII.get(ARM::MOVi), ARM::R12)
+          .addImm(22)
+          .add(predOps(ARMCC::AL))
+          .add(condCodeOp());
+      BuildMI(MBB, MI, dl, TII.get(ARM::STR_PRE_IMM), ARM::SP)
+          .addReg(ARM::R12, RegState::Kill)
+          .addReg(ARM::SP)
+          .setMIFlags(MachineInstr::NoFlags)
+          .addImm(-4)
+          .add(predOps(ARMCC::AL));
+    }
   }
 
   // Now that the prologue's actual instructions are finalised, we can insert
@@ -807,6 +831,11 @@
                  AFI->getGPRCalleeSavedArea2Size() +
                  AFI->getDPRCalleeSavedGapSize() +
                  AFI->getDPRCalleeSavedAreaSize());
+    if (AFI->isJSStub()) {
+      NumBytes += 4;
+    } else if (AFI->isJSFunction()) {
+      NumBytes += 12;
+    }
 
     // Reset SP based on frame pointer only if the stack frame extends beyond
     // frame pointer stack slot or target is ELF and the function has FP.
@@ -951,6 +980,9 @@
       // Otherwise, use SP or FP, whichever is closer to the stack slot.
       FrameReg = RegInfo->getFrameRegister(MF);
       return FPOffset;
+    } else if (AFI->isJSStub() || AFI->isJSFunction()) {
+      FrameReg = RegInfo->getFrameRegister(MF);
+      return FPOffset;
     }
   }
   // Use the base pointer if we have one.
@@ -1055,7 +1087,9 @@
   if (MBB.end() != MI) {
     DL = MI->getDebugLoc();
     unsigned RetOpcode = MI->getOpcode();
-    isTailCall = (RetOpcode == ARM::TCRETURNdi || RetOpcode == ARM::TCRETURNri);
+    isTailCall =
+        (RetOpcode == ARM::TCRETURNdi || RetOpcode == ARM::TCRETURNri ||
+         RetOpcode == TargetOpcode::TCPATCHPOINT);
     isInterrupt =
         RetOpcode == ARM::SUBS_PC_LR || RetOpcode == ARM::t2SUBS_PC_LR;
     isTrap =
@@ -1429,8 +1463,18 @@
     ARM::t2STR_PRE : ARM::STR_PRE_IMM;
   unsigned FltOpc = ARM::VSTMDDB_UPD;
   unsigned NumAlignedDPRCS2Regs = AFI->getNumAlignedDPRCS2Regs();
-  emitPushInst(MBB, MI, CSI, PushOpc, PushOneOpc, false, &isARMArea1Register, 0,
-               MachineInstr::FrameSetup);
+  if (AFI->isJSFunction()) {
+    std::vector<CalleeSavedInfo> FakeCSI;
+    FakeCSI.emplace_back(ARM::R0, 0);
+    FakeCSI.emplace_back(ARM::R1, 0);
+    FakeCSI.emplace_back(ARM::R7, 0);
+    std::copy(CSI.begin(), CSI.end(), std::back_inserter(FakeCSI));
+    emitPushInst(MBB, MI, FakeCSI, PushOpc, PushOneOpc, false,
+                 &isARMArea1Register, 0, MachineInstr::FrameSetup);
+  } else {
+    emitPushInst(MBB, MI, CSI, PushOpc, PushOneOpc, false, &isARMArea1Register,
+                 0, MachineInstr::FrameSetup);
+  }
   emitPushInst(MBB, MI, CSI, PushOpc, PushOneOpc, false, &isARMArea2Register, 0,
                MachineInstr::FrameSetup);
   emitPushInst(MBB, MI, CSI, FltOpc, 0, true, &isARMArea3Register,
@@ -1461,7 +1505,6 @@
   // registers. Do that here instead.
   if (NumAlignedDPRCS2Regs)
     emitAlignedDPRCS2Restores(MBB, MI, NumAlignedDPRCS2Regs, CSI, TRI);
-
   unsigned PopOpc = AFI->isThumbFunction() ? ARM::t2LDMIA_UPD : ARM::LDMIA_UPD;
   unsigned LdrOpc = AFI->isThumbFunction() ? ARM::t2LDR_POST :ARM::LDR_POST_IMM;
   unsigned FltOpc = ARM::VLDMDIA_UPD;
@@ -1611,6 +1654,20 @@
   MachineRegisterInfo &MRI = MF.getRegInfo();
   const TargetRegisterInfo *TRI = MF.getSubtarget().getRegisterInfo();
   (void)TRI;  // Silence unused warning in non-assert builds.
+
+  const Function &F = MF.getFunction();
+  if (F.hasFnAttribute("js-function-call"))
+    AFI->setJSFunction(true);
+  else if (F.hasFnAttribute("js-stub-call"))
+    AFI->setJSStub(true);
+
+  if (AFI->isJSFunction() || AFI->isJSStub()) {
+    assert((F.getCallingConv() == CallingConv::V8CC) ||
+           (F.getCallingConv() == CallingConv::V8SBCC));
+    SavedRegs.set(ARM::R11);
+    SavedRegs.set(ARM::LR);
+    CanEliminateFrame = false;
+  }
   unsigned FramePtr = RegInfo->getFrameRegister(MF);
 
   // Spill R4 if Thumb2 function requires stack realignment - it will be used as
@@ -1783,7 +1840,10 @@
     AFI->setHasStackFrame(true);
 
     if (HasFP) {
-      SavedRegs.set(FramePtr);
+      if (!SavedRegs.test(FramePtr)) {
+        SavedRegs.set(FramePtr);
+        NumGPRSpills++;
+      }
       // If the frame pointer is required by the ABI, also spill LR so that we
       // emit a complete frame record.
       if (MF.getTarget().Options.DisableFramePointerElim(MF) && !LRSpilled) {
@@ -1797,7 +1857,6 @@
       auto FPPos = llvm::find(UnspilledCS1GPRs, FramePtr);
       if (FPPos != UnspilledCS1GPRs.end())
         UnspilledCS1GPRs.erase(FPPos);
-      NumGPRSpills++;
       if (FramePtr == ARM::R7)
         CS1Spilled = true;
     }
@@ -2511,3 +2570,25 @@
   MF.verify();
 #endif
 }
+
+bool ARMFrameLowering::assignCalleeSavedSpillSlots(
+    MachineFunction &MF, const TargetRegisterInfo *TRI,
+    std::vector<CalleeSavedInfo> &CSI) const {
+  ARMFunctionInfo *AFI = MF.getInfo<ARMFunctionInfo>();
+  // default handle.
+  if (!AFI->isJSStub() && !AFI->isJSFunction())
+    return false;
+  assert(CSI.size() == 2 && CSI[0].getReg() == ARM::LR &&
+         CSI[1].getReg() == ARM::R11);
+  MachineFrameInfo &MFI = MF.getFrameInfo();
+  CSI[0].setFrameIdx(MFI.CreateFixedSpillStackObject(4, -4));
+  CSI[1].setFrameIdx(MFI.CreateFixedSpillStackObject(4, -8));
+  if (AFI->isJSStub()) {
+    MFI.CreateFixedSpillStackObject(4, -12);
+  } else {
+    MFI.CreateFixedSpillStackObject(4, -12); // cp
+    MFI.CreateFixedSpillStackObject(4, -16); // function
+    MFI.CreateFixedSpillStackObject(4, -20); // arg count.
+  }
+  return true;
+}
Index: lib/Target/ARM/ARMFrameLowering.h
===================================================================
--- lib/Target/ARM/ARMFrameLowering.h	(revision 359070)
+++ lib/Target/ARM/ARMFrameLowering.h	(working copy)
@@ -65,6 +65,11 @@
     return true;
   }
 
+  bool
+  assignCalleeSavedSpillSlots(MachineFunction &MF,
+                              const TargetRegisterInfo *TRI,
+                              std::vector<CalleeSavedInfo> &CSI) const override;
+
 private:
   void emitPushInst(MachineBasicBlock &MBB, MachineBasicBlock::iterator MI,
                     const std::vector<CalleeSavedInfo> &CSI, unsigned StmOpc,
Index: lib/Target/ARM/ARMISelLowering.cpp
===================================================================
--- lib/Target/ARM/ARMISelLowering.cpp	(revision 359070)
+++ lib/Target/ARM/ARMISelLowering.cpp	(working copy)
@@ -1388,6 +1388,7 @@
   case ARMISD::VBICIMM:       return "ARMISD::VBICIMM";
   case ARMISD::VBSL:          return "ARMISD::VBSL";
   case ARMISD::MEMCPY:        return "ARMISD::MEMCPY";
+  case ARMISD::RESTORESP:     return "ARMISD::RESTORESP";
   case ARMISD::VLD1DUP:       return "ARMISD::VLD1DUP";
   case ARMISD::VLD2DUP:       return "ARMISD::VLD2DUP";
   case ARMISD::VLD3DUP:       return "ARMISD::VLD3DUP";
@@ -1601,6 +1602,10 @@
   switch (CC) {
   default:
     report_fatal_error("Unsupported calling convention");
+  case CallingConv::V8CC:
+    return CallingConv::V8CC;
+  case CallingConv::V8SBCC:
+    return CallingConv::V8SBCC;
   case CallingConv::ARM_AAPCS:
   case CallingConv::ARM_APCS:
   case CallingConv::GHC:
@@ -1650,6 +1655,10 @@
   switch (getEffectiveCallingConv(CC, isVarArg)) {
   default:
     report_fatal_error("Unsupported calling convention");
+  case CallingConv::V8SBCC:
+    return (Return ? RetCC_ARM_V8SB : CC_ARM_V8SB);
+  case CallingConv::V8CC:
+    return (Return ? RetCC_ARM_V8 : CC_ARM_V8);
   case CallingConv::ARM_APCS:
     return (Return ? RetCC_ARM_APCS : CC_ARM_APCS);
   case CallingConv::ARM_AAPCS:
@@ -1810,6 +1819,7 @@
   bool isStructRet    = (Outs.empty()) ? false : Outs[0].Flags.isSRet();
   bool isThisReturn   = false;
   bool isSibCall      = false;
+  bool hasJSCCall = false;
   auto Attr = MF.getFunction().getFnAttribute("disable-tail-calls");
 
   // Disable tail calls if they're not supported.
@@ -1849,8 +1859,34 @@
   // These operations are automatically eliminated by the prolog/epilog pass
   if (!isSibCall)
     Chain = DAG.getCALLSEQ_START(Chain, NumBytes, 0, dl);
+  if (CLI.CS.isCall()) {
+    MachineFunction &MF = DAG.getMachineFunction();
+    const Function &CallerF = MF.getFunction();
+    CallingConv::ID CallerCC = CallerF.getCallingConv();
+    if (((CallerCC == CallingConv::V8CC) ||
+         (CallerCC == CallingConv::V8SBCC)) &&
+        (CLI.CallConv == CallingConv::C)) {
+      hasJSCCall = true;
+    }
+  }
 
-  SDValue StackPtr =
+  SDValue StackPtr;
+  if (hasJSCCall) {
+    SDValue OldStackPtr = DAG.getCopyFromReg(Chain, dl, ARM::SP,
+                                             getPointerTy(DAG.getDataLayout()));
+    Chain = OldStackPtr.getValue(1);
+    SDValue InFlag;
+    SDValue NewStackPtr = DAG.getNode(
+        ISD::AND, dl, OldStackPtr.getValueType(), OldStackPtr.getValue(0),
+        DAG.getConstant(-8, dl, OldStackPtr.getValueType()));
+    Chain =
+        DAG.getCopyToReg(Chain, dl, ARM::SP, NewStackPtr.getValue(0), InFlag);
+    MachineFunction &MF = DAG.getMachineFunction();
+    MachineFrameInfo &MFI = MF.getFrameInfo();
+    MFI.setFrameAddressIsTaken(true);
+  }
+
+  StackPtr =
       DAG.getCopyFromReg(Chain, dl, ARM::SP, getPointerTy(DAG.getDataLayout()));
 
   RegsToPassVector RegsToPass;
@@ -2191,6 +2227,12 @@
 
   // Returns a chain and a flag for retval copy to use.
   Chain = DAG.getNode(CallOpc, dl, NodeTys, Ops);
+
+  if (hasJSCCall) {
+    Chain = DAG.getNode(ARMISD::RESTORESP, dl,
+                        DAG.getVTList(MVT::Other, MVT::Glue), Chain);
+  }
+
   InFlag = Chain.getValue(1);
 
   Chain = DAG.getCALLSEQ_END(Chain, DAG.getIntPtrConstant(NumBytes, dl, true),
@@ -9254,6 +9296,17 @@
     llvm_unreachable("Unexpected instr type to insert");
   }
 
+  case TargetOpcode::STATEPOINT:
+    // As an implementation detail, STATEPOINT shares the STACKMAP format at
+    // this point in the process.  We diverge later.
+    return emitPatchPoint(MI, BB);
+
+  case TargetOpcode::STACKMAP:
+  case TargetOpcode::PATCHPOINT:
+    return emitPatchPoint(MI, BB);
+  case TargetOpcode::TCPATCHPOINT:
+    return TargetLoweringBase::emitPatchPoint(MI, BB);
+
   // Thumb1 post-indexed loads are really just single-register LDMs.
   case ARM::tLDR_postidx: {
     MachineOperand Def(MI.getOperand(1));
@@ -10961,6 +11014,10 @@
   unsigned Mask = MaskC->getZExtValue();
   if (Mask == 0xffff)
     return SDValue();
+  // Memory Operator can address constant, save one register.
+  auto UI = dyn_cast<MemSDNode>(*(N->use_begin()));
+  if (UI)
+    return SDValue();
   SDValue Res;
   // Case (1): or (and A, mask), val => ARMbfi A, val, mask
   ConstantSDNode *N1C = dyn_cast<ConstantSDNode>(N1);
@@ -13136,6 +13193,23 @@
   return -1;
 }
 
+const MCPhysReg *
+ARMTargetLowering::getScratchRegisters(CallingConv::ID CC) const {
+  if (CC == CallingConv::V8SBCC)
+    return nullptr;
+  static const MCPhysReg ScratchRegs[] = {ARM::R12, 0};
+  return ScratchRegs;
+}
+
+MachineBasicBlock *
+ARMTargetLowering::emitPatchPoint(MachineInstr &MI,
+                                  MachineBasicBlock *MBB) const {
+  MachineBasicBlock *MBB2 = TargetLoweringBase::emitPatchPoint(MI, MBB);
+  MachineFunction &MF = *MI.getMF();
+  MI.addOperand(MF, MachineOperand::CreateReg(ARM::LR, true, true));
+  return MBB2;
+}
+
 static bool isLegalT1AddressImmediate(int64_t V, EVT VT) {
   if (V < 0)
     return false;
Index: lib/Target/ARM/ARMISelLowering.h
===================================================================
--- lib/Target/ARM/ARMISelLowering.h	(revision 359070)
+++ lib/Target/ARM/ARMISelLowering.h	(working copy)
@@ -233,6 +233,8 @@
       // Pseudo-instruction representing a memory copy using ldm/stm
       // instructions.
       MEMCPY,
+      // Pseudo-instruction representing restoring sp from fp.
+      RESTORESP,
 
       // Vector load N-element structure to all lanes:
       VLD1DUP = ISD::FIRST_TARGET_MEMORY_OPCODE,
@@ -501,6 +503,12 @@
     bool functionArgumentNeedsConsecutiveRegisters(
         Type *Ty, CallingConv::ID CallConv, bool isVarArg) const override;
 
+    const MCPhysReg *getScratchRegisters(CallingConv::ID CC) const override;
+
+    MachineBasicBlock *emitPatchPoint(MachineInstr &MI,
+                                      MachineBasicBlock *MBB) const;
+
+
     /// If a physical register, this returns the register that receives the
     /// exception address on entry to an EH pad.
     unsigned
Index: lib/Target/ARM/ARMInstrInfo.td
===================================================================
--- lib/Target/ARM/ARMInstrInfo.td	(revision 359070)
+++ lib/Target/ARM/ARMInstrInfo.td	(working copy)
@@ -79,6 +79,7 @@
 def SDT_ARMMEMCPY  : SDTypeProfile<2, 3, [SDTCisVT<0, i32>, SDTCisVT<1, i32>,
                                           SDTCisVT<2, i32>, SDTCisVT<3, i32>,
                                           SDTCisVT<4, i32>]>;
+def SDT_ARMRESTORESP: SDTypeProfile<0, 0, []>;
 
 def SDTBinaryArithWithFlags : SDTypeProfile<2, 2,
                                             [SDTCisSameAs<0, 2>,
@@ -207,6 +208,10 @@
                         [SDNPHasChain, SDNPInGlue, SDNPOutGlue,
                          SDNPMayStore, SDNPMayLoad]>;
 
+def ARMrestoresp : SDNode<"ARMISD::RESTORESP", SDT_ARMRESTORESP,
+                        [SDNPHasChain, SDNPOutGlue,
+                         SDNPMayStore, SDNPMayLoad]>;
+
 def ARMsmulwb       : SDNode<"ARMISD::SMULWB", SDTIntBinOp, []>;
 def ARMsmulwt       : SDNode<"ARMISD::SMULWT", SDTIntBinOp, []>;
 def ARMsmlalbb      : SDNode<"ARMISD::SMLALBB", SDT_LongMac, []>;
@@ -4931,6 +4936,14 @@
             (ARMmemcopy GPR:$dst, GPR:$src, imm:$nreg))]>;
 }
 
+let hasPostISelHook = 1, hasNoSchedulingInfo = 1 in {
+    def RESTORESP : PseudoInst<
+      (outs),
+      (ins),
+      NoItinerary,
+      [(ARMrestoresp)]>;
+}
+
 def ldrex_1 : PatFrag<(ops node:$ptr), (int_arm_ldrex node:$ptr), [{
   return cast<MemIntrinsicSDNode>(N)->getMemoryVT() == MVT::i8;
 }]>;
Index: lib/Target/ARM/ARMMachineFunctionInfo.h
===================================================================
--- lib/Target/ARM/ARMMachineFunctionInfo.h	(revision 359070)
+++ lib/Target/ARM/ARMMachineFunctionInfo.h	(working copy)
@@ -127,6 +127,9 @@
   /// The amount the literal pool has been increasedby due to promoted globals.
   int PromotedGlobalsIncrease = 0;
 
+  bool IsJSFunction = false;
+  bool IsJSStub = false;
+
 public:
   ARMFunctionInfo() = default;
 
@@ -239,6 +242,11 @@
   void setPromotedConstpoolIncrease(int Sz) {
     PromotedGlobalsIncrease = Sz;
   }
+  bool isJSFunction() const { return IsJSFunction; }
+  bool isJSStub() const { return IsJSStub; }
+
+  void setJSFunction(bool s) { IsJSFunction = s; }
+  void setJSStub(bool s) { IsJSStub = s; }
 };
 
 } // end namespace llvm
Index: tools/llvm-shlib/CMakeLists.txt
===================================================================
--- tools/llvm-shlib/CMakeLists.txt	(revision 359070)
+++ tools/llvm-shlib/CMakeLists.txt	(working copy)
@@ -64,6 +64,7 @@
   endif()
 
   target_link_libraries(LLVM PRIVATE ${LIB_NAMES})
+set_target_properties(LLVM PROPERTIES LINK_FLAGS "-static-libstdc++")
 
   if (APPLE)
     set_property(TARGET LLVM APPEND_STRING PROPERTY
Index: tools/llvm-shlib/simple_version_script.map.in
===================================================================
--- tools/llvm-shlib/simple_version_script.map.in	(revision 359070)
+++ tools/llvm-shlib/simple_version_script.map.in	(working copy)
@@ -1 +1 @@
-LLVM_@LLVM_VERSION_MAJOR@ { global: *; };
+LLVM_@LLVM_VERSION_MAJOR@ { global: LLVM*; local:*; };
