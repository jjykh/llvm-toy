Index: include/llvm/CodeGen/MachineRegisterInfo.h
===================================================================
--- include/llvm/CodeGen/MachineRegisterInfo.h	(revision 375507)
+++ include/llvm/CodeGen/MachineRegisterInfo.h	(working copy)
@@ -937,6 +937,10 @@
   /// corresponding live-in physical register.
   unsigned getLiveInVirtReg(unsigned PReg) const;
 
+  /// updateVirtRegIfLivein - If a VReg is a live-in virtual register, update
+  /// it to VNewReg.
+  void updateVirtRegIfLivein(unsigned VReg, unsigned VNewReg);
+
   /// EmitLiveInCopies - Emit copies to initialize livein virtual registers
   /// into the given entry block.
   void EmitLiveInCopies(MachineBasicBlock *EntryMBB,
Index: include/llvm/CodeGen/TargetRegisterInfo.h
===================================================================
--- include/llvm/CodeGen/TargetRegisterInfo.h	(revision 375507)
+++ include/llvm/CodeGen/TargetRegisterInfo.h	(working copy)
@@ -972,6 +972,13 @@
     return StringRef(getName(Reg));
   }
 
+  using VirtRegToFixSlotMap = std::vector<std::pair<unsigned, int>>;
+
+  virtual VirtRegToFixSlotMap
+  getHoistToFixStackSlotMap(MachineFunction &) const {
+    return VirtRegToFixSlotMap();
+  }
+
   //===--------------------------------------------------------------------===//
   /// Subtarget Hooks
 
Index: include/llvm/IR/CallingConv.h
===================================================================
--- include/llvm/IR/CallingConv.h	(revision 375507)
+++ include/llvm/IR/CallingConv.h	(working copy)
@@ -219,8 +219,14 @@
     /// shader if tessellation is in use, or otherwise the vertex shader.
     AMDGPU_ES = 96,
 
+    // Calling convention for v8
+    V8CC = 97,
+
+    // Calling convention for v8 store barrier stub
+    V8SBCC = 98,
+
     // Calling convention between AArch64 Advanced SIMD functions
-    AArch64_VectorCall = 97,
+    AArch64_VectorCall = 99,
 
     /// The highest possible calling convention ID. Must be some 2^k - 1.
     MaxID = 1023
Index: include/llvm/IR/Intrinsics.td
===================================================================
--- include/llvm/IR/Intrinsics.td	(revision 375507)
+++ include/llvm/IR/Intrinsics.td	(working copy)
@@ -954,6 +954,8 @@
 def int_experimental_gc_relocate : Intrinsic<[llvm_any_ty],
                                 [llvm_token_ty, llvm_i32_ty, llvm_i32_ty],
                                 [IntrReadMem, ImmArg<1>, ImmArg<2>]>;
+def int_experimental_gc_exception : Intrinsic<[llvm_any_ty], [llvm_token_ty],
+                                             [IntrReadMem]>;
 
 //===------------------------ Coroutine Intrinsics ---------------===//
 // These are documented in docs/Coroutines.rst
Index: include/llvm/IR/Statepoint.h
===================================================================
--- include/llvm/IR/Statepoint.h	(revision 375507)
+++ include/llvm/IR/Statepoint.h	(working copy)
@@ -317,7 +317,8 @@
 public:
   static bool classof(const IntrinsicInst *I) {
     return I->getIntrinsicID() == Intrinsic::experimental_gc_relocate ||
-      I->getIntrinsicID() == Intrinsic::experimental_gc_result;
+           I->getIntrinsicID() == Intrinsic::experimental_gc_result ||
+           I->getIntrinsicID() == Intrinsic::experimental_gc_exception;
   }
 
   static bool classof(const Value *V) {
@@ -401,6 +402,17 @@
   }
 };
 
+class GCExceptionInst : public GCProjectionInst {
+public:
+  static bool classof(const IntrinsicInst *I) {
+    return I->getIntrinsicID() == Intrinsic::experimental_gc_exception;
+  }
+
+  static bool classof(const Value *V) {
+    return isa<IntrinsicInst>(V) && classof(cast<IntrinsicInst>(V));
+  }
+};
+
 template <typename FunTy, typename InstructionTy, typename ValueTy,
           typename CallBaseTy>
 std::vector<const GCRelocateInst *>
Index: include/llvm/Support/TargetOpcodes.def
===================================================================
--- include/llvm/Support/TargetOpcodes.def	(revision 375507)
+++ include/llvm/Support/TargetOpcodes.def	(working copy)
@@ -120,6 +120,7 @@
 /// rewrite calls to runtimes with more efficient code sequences.
 /// This also implies a stack map.
 HANDLE_TARGET_OPCODE(PATCHPOINT)
+HANDLE_TARGET_OPCODE(TCPATCHPOINT)
 
 /// This pseudo-instruction loads the stack guard value. Targets which need
 /// to prevent the stack guard value or address from being spilled to the
Index: include/llvm/Target/Target.td
===================================================================
--- include/llvm/Target/Target.td	(revision 375507)
+++ include/llvm/Target/Target.td	(working copy)
@@ -1101,6 +1101,17 @@
   let mayLoad = 1;
   let usesCustomInserter = 1;
 }
+def TCPATCHPOINT : StandardPseudoInstruction {
+  let OutOperandList = (outs);
+  let InOperandList = (ins i64imm:$id, i32imm:$nbytes, unknown:$callee,
+                       i32imm:$nargs, i32imm:$cc, variable_ops);
+  let hasSideEffects = 1;
+  let isCall = 1;
+  let mayLoad = 1;
+  let usesCustomInserter = 1;
+  let isReturn = 1;
+  let isTerminator = 1;
+}
 def STATEPOINT : StandardPseudoInstruction {
   let OutOperandList = (outs);
   let InOperandList = (ins variable_ops);
Index: include/llvm/Transforms/IPO/SampleProfile.h
===================================================================
--- include/llvm/Transforms/IPO/SampleProfile.h	(revision 375507)
+++ include/llvm/Transforms/IPO/SampleProfile.h	(working copy)
@@ -30,6 +30,7 @@
         IsThinLTOPreLink(IsThinLTOPreLink) {}
 
   PreservedAnalyses run(Module &M, ModuleAnalysisManager &AM);
+  static std::string SampleProfileFileFromOption();
 
 private:
   std::string ProfileFileName;
Index: include/llvm-c/Core.h
===================================================================
--- include/llvm-c/Core.h	(revision 375507)
+++ include/llvm-c/Core.h	(working copy)
@@ -246,7 +246,9 @@
   LLVMAMDGPUHSCallConv      = 93,
   LLVMMSP430BUILTINCallConv = 94,
   LLVMAMDGPULSCallConv      = 95,
-  LLVMAMDGPUESCallConv      = 96
+  LLVMAMDGPUESCallConv      = 96,
+  LLVMV8CallConv            = 97,
+  LLVMV8SBCallConv          = 98
 } LLVMCallConv;
 
 typedef enum {
Index: include/llvm-c/Target.h
===================================================================
--- include/llvm-c/Target.h	(revision 375507)
+++ include/llvm-c/Target.h	(working copy)
@@ -207,6 +207,9 @@
 void LLVMAddTargetLibraryInfo(LLVMTargetLibraryInfoRef TLI,
                               LLVMPassManagerRef PM);
 
+/** Create empty TargetLibraryInfoImpl, with all functions disabled */
+LLVMTargetLibraryInfoRef LLVMCreateEmptyTargetLibraryInfo();
+
 /** Converts target data to a target layout string. The string must be disposed
     with LLVMDisposeMessage.
     See the constructor llvm::DataLayout::DataLayout. */
Index: lib/AsmParser/LLLexer.cpp
===================================================================
--- lib/AsmParser/LLLexer.cpp	(revision 375507)
+++ lib/AsmParser/LLLexer.cpp	(working copy)
@@ -607,6 +607,8 @@
   KEYWORD(webkit_jscc);
   KEYWORD(swiftcc);
   KEYWORD(anyregcc);
+  KEYWORD(v8cc);
+  KEYWORD(v8sbcc);
   KEYWORD(preserve_mostcc);
   KEYWORD(preserve_allcc);
   KEYWORD(ghccc);
Index: lib/AsmParser/LLParser.cpp
===================================================================
--- lib/AsmParser/LLParser.cpp	(revision 375507)
+++ lib/AsmParser/LLParser.cpp	(working copy)
@@ -1984,6 +1984,12 @@
   case lltok::kw_win64cc:        CC = CallingConv::Win64; break;
   case lltok::kw_webkit_jscc:    CC = CallingConv::WebKit_JS; break;
   case lltok::kw_anyregcc:       CC = CallingConv::AnyReg; break;
+  case lltok::kw_v8cc:
+    CC = CallingConv::V8CC;
+    break;
+  case lltok::kw_v8sbcc:
+    CC = CallingConv::V8SBCC;
+    break;
   case lltok::kw_preserve_mostcc:CC = CallingConv::PreserveMost; break;
   case lltok::kw_preserve_allcc: CC = CallingConv::PreserveAll; break;
   case lltok::kw_ghccc:          CC = CallingConv::GHC; break;
Index: lib/AsmParser/LLToken.h
===================================================================
--- lib/AsmParser/LLToken.h	(revision 375507)
+++ lib/AsmParser/LLToken.h	(working copy)
@@ -152,6 +152,8 @@
   kw_win64cc,
   kw_webkit_jscc,
   kw_anyregcc,
+  kw_v8cc,
+  kw_v8sbcc,
   kw_swiftcc,
   kw_preserve_mostcc,
   kw_preserve_allcc,
Index: lib/CodeGen/InlineSpiller.cpp
===================================================================
--- lib/CodeGen/InlineSpiller.cpp	(revision 375507)
+++ lib/CodeGen/InlineSpiller.cpp	(working copy)
@@ -56,6 +56,7 @@
 #include <cassert>
 #include <iterator>
 #include <tuple>
+#include <unordered_set>
 #include <utility>
 #include <vector>
 
@@ -216,6 +217,7 @@
   bool isSibling(unsigned Reg);
   bool hoistSpillInsideBB(LiveInterval &SpillLI, MachineInstr &CopyMI);
   void eliminateRedundantSpills(LiveInterval &LI, VNInfo *VNI);
+  void foldStatePoints(unsigned Reg);
 
   void markValueUsed(LiveInterval*, VNInfo*);
   bool canGuaranteeAssignmentAfterRemat(unsigned VReg, MachineInstr &MI);
@@ -960,11 +962,68 @@
     HSpiller.addToMergeableSpills(*std::next(MI), StackSlot, Original);
 }
 
+void InlineSpiller::foldStatePoints(unsigned InputReg) {
+  SmallVector<unsigned, 8> WorkList;
+  std::unordered_set<unsigned> VisitedSet;
+  // Don't add Reg to WorkList, spillAroundUses will handle it.
+  for (MachineRegisterInfo::reg_bundle_iterator
+           RegI = MRI.reg_bundle_begin(InputReg),
+           E = MRI.reg_bundle_end();
+       RegI != E;) {
+    MachineInstr &MI = *RegI++;
+    unsigned SibReg = isFullCopyOf(MI, InputReg);
+    if (SibReg && isSibling(SibReg) && SibReg != InputReg) {
+      WorkList.push_back(SibReg);
+      VisitedSet.emplace(SibReg);
+    }
+  }
+  VisitedSet.emplace(InputReg);
+  LiveInterval &OldLI = LIS.getInterval(Original);
+  while (!WorkList.empty()) {
+    unsigned Reg = WorkList.pop_back_val();
+    // Fold the use of state point.
+    for (MachineRegisterInfo::use_instr_nodbg_iterator
+             UI = MRI.use_instr_nodbg_begin(Reg),
+             E = MRI.use_instr_nodbg_end();
+         UI != E;) {
+      MachineInstr &MI = *UI++;
+      if (unsigned DstReg = isFullCopyOf(MI, Reg)) {
+        if (isSibling(DstReg)) {
+          auto pair = VisitedSet.emplace(DstReg);
+          if (pair.second)
+            WorkList.push_back(DstReg);
+        }
+        continue;
+      }
+      // Handle STATEPOINT, PATCHPOINT
+      switch (MI.getOpcode()) {
+      case TargetOpcode::STATEPOINT:
+      case TargetOpcode::PATCHPOINT: {
+        // Ignore if the input reg's live range does not contain
+        // the MI.
+        SlotIndex Idx = LIS.getInstructionIndex(MI).getRegSlot();
+        auto Segment = OldLI.FindSegmentContaining(Idx);
+        if ((OldLI.end() == Segment) || (Segment->end <= Idx)) {
+          continue;
+        }
+        // Analyze instruction.
+        SmallVector<std::pair<MachineInstr *, unsigned>, 8> Ops;
+        MIBundleOperands(MI).analyzeVirtReg(Reg, &Ops);
+        foldMemoryOperand(Ops);
+      } break;
+      default:
+        break;
+      }
+    }
+  }
+}
+
 /// spillAroundUses - insert spill code around each use of Reg.
 void InlineSpiller::spillAroundUses(unsigned Reg) {
   LLVM_DEBUG(dbgs() << "spillAroundUses " << printReg(Reg) << '\n');
   LiveInterval &OldLI = LIS.getInterval(Reg);
 
+  foldStatePoints(Reg);
   // Iterate over instructions using Reg.
   for (MachineRegisterInfo::reg_bundle_iterator
        RegI = MRI.reg_bundle_begin(Reg), E = MRI.reg_bundle_end();
Index: lib/CodeGen/LocalStackSlotAllocation.cpp
===================================================================
--- lib/CodeGen/LocalStackSlotAllocation.cpp	(revision 375507)
+++ lib/CodeGen/LocalStackSlotAllocation.cpp	(working copy)
@@ -306,6 +306,7 @@
       // range, so they don't need any updates.
       if (MI.isDebugInstr() || MI.getOpcode() == TargetOpcode::STATEPOINT ||
           MI.getOpcode() == TargetOpcode::STACKMAP ||
+          MI.getOpcode() == TargetOpcode::TCPATCHPOINT ||
           MI.getOpcode() == TargetOpcode::PATCHPOINT)
         continue;
 
Index: lib/CodeGen/MachineRegisterInfo.cpp
===================================================================
--- lib/CodeGen/MachineRegisterInfo.cpp	(revision 375507)
+++ lib/CodeGen/MachineRegisterInfo.cpp	(working copy)
@@ -464,6 +464,14 @@
   return 0;
 }
 
+void MachineRegisterInfo::updateVirtRegIfLivein(unsigned VReg,
+                                                unsigned VNewReg) {
+  for (auto I = LiveIns.begin(), E = LiveIns.end(); I != E; ++I)
+    if (I->second == VReg) {
+      I->second = VNewReg;
+    }
+}
+
 /// EmitLiveInCopies - Emit copies to initialize livein virtual registers
 /// into the given entry block.
 void
Index: lib/CodeGen/MachineVerifier.cpp
===================================================================
--- lib/CodeGen/MachineVerifier.cpp	(revision 375507)
+++ lib/CodeGen/MachineVerifier.cpp	(working copy)
@@ -1495,6 +1495,8 @@
   unsigned NumDefs = MCID.getNumDefs();
   if (MCID.getOpcode() == TargetOpcode::PATCHPOINT)
     NumDefs = (MONum == 0 && MO->isReg()) ? NumDefs : 0;
+  else if (MCID.getOpcode() == TargetOpcode::TCPATCHPOINT)
+    NumDefs = 0;
 
   // The first MCID.NumDefs operands must be explicit register defines
   if (MONum < NumDefs) {
Index: lib/CodeGen/RegAllocBase.cpp
===================================================================
--- lib/CodeGen/RegAllocBase.cpp	(revision 375507)
+++ lib/CodeGen/RegAllocBase.cpp	(working copy)
@@ -30,6 +30,7 @@
 #include "llvm/Support/Timer.h"
 #include "llvm/Support/raw_ostream.h"
 #include <cassert>
+#include <unordered_map>
 
 using namespace llvm;
 
@@ -169,4 +170,39 @@
     DeadInst->eraseFromParent();
   }
   DeadRemats.clear();
+  tryHoistToStackSlot();
 }
+
+void RegAllocBase::tryHoistToStackSlot() {
+  MachineFunction &MF = VRM->getMachineFunction();
+  auto RegMapping = TRI->getHoistToFixStackSlotMap(MF);
+  if (RegMapping.empty())
+    return;
+  std::unordered_map<int, int> SlotMapping;
+  for (auto &Entry : RegMapping) {
+    int Slot = VRM->getStackSlot(Entry.first);
+    if (Slot == VirtRegMap::NO_STACK_SLOT)
+      continue;
+    SlotMapping.emplace(Slot, Entry.second);
+  }
+  if (SlotMapping.empty())
+    return;
+  for (MachineBasicBlock &MBB : MF) {
+    for (MachineInstr &MI : MBB)
+      for (unsigned i = 0, ee = MI.getNumOperands(); i != ee; ++i) {
+        MachineOperand &MO = MI.getOperand(i);
+        if (!MO.isFI())
+          continue;
+        int OldFI = MO.getIndex();
+        if (OldFI < 0)
+          continue;
+        auto FoundFI = SlotMapping.find(OldFI);
+        if (FoundFI == SlotMapping.end())
+          continue;
+        int NewFI = FoundFI->second;
+        if (NewFI == OldFI)
+          continue;
+        MO.setIndex(NewFI);
+      }
+  }
+}
Index: lib/CodeGen/RegAllocBase.h
===================================================================
--- lib/CodeGen/RegAllocBase.h	(revision 375507)
+++ lib/CodeGen/RegAllocBase.h	(working copy)
@@ -117,6 +117,7 @@
 
 private:
   void seedLiveRegs();
+  void tryHoistToStackSlot();
 };
 
 } // end namespace llvm
Index: lib/CodeGen/RegisterCoalescer.cpp
===================================================================
--- lib/CodeGen/RegisterCoalescer.cpp	(revision 375507)
+++ lib/CodeGen/RegisterCoalescer.cpp	(working copy)
@@ -35,6 +35,7 @@
 #include "llvm/CodeGen/Passes.h"
 #include "llvm/CodeGen/RegisterClassInfo.h"
 #include "llvm/CodeGen/SlotIndexes.h"
+#include "llvm/CodeGen/StackMaps.h"
 #include "llvm/CodeGen/TargetInstrInfo.h"
 #include "llvm/CodeGen/TargetOpcodes.h"
 #include "llvm/CodeGen/TargetRegisterInfo.h"
@@ -181,6 +182,11 @@
     /// Join compatible live intervals
     void joinAllIntervals();
 
+    /// Remove the redundant move immediate from statepoints.
+    void removeMoveImmediateFromPatchpoint();
+
+    void removeMoveImmediateFromPatchpoint(MachineInstr *MI);
+
     /// Coalesce copies in the specified MBB, putting
     /// copies that cannot yet be coalesced into WorkList.
     void copyCoalesceInMBB(MachineBasicBlock *MBB);
@@ -1960,6 +1966,8 @@
   // Update regalloc hint.
   TRI->updateRegAllocHint(CP.getSrcReg(), CP.getDstReg(), *MF);
 
+  MRI->updateVirtRegIfLivein(CP.getSrcReg(), CP.getDstReg());
+
   LLVM_DEBUG({
     dbgs() << "\tSuccess: " << printReg(CP.getSrcReg(), TRI, CP.getSrcIdx())
            << " -> " << printReg(CP.getDstReg(), TRI, CP.getDstIdx()) << '\n';
@@ -3666,6 +3674,103 @@
   lateLiveIntervalUpdate();
 }
 
+void RegisterCoalescer::removeMoveImmediateFromPatchpoint() {
+  SmallVector<MachineInstr *, 8> WorkList;
+  for (MachineFunction::iterator I = MF->begin(), E = MF->end(); I != E; ++I) {
+    MachineBasicBlock *MBB = &*I;
+    for (MachineInstr &MI : *MBB) {
+      switch (MI.getOpcode()) {
+      case TargetOpcode::STACKMAP:
+      case TargetOpcode::PATCHPOINT:
+      case TargetOpcode::TCPATCHPOINT:
+      case TargetOpcode::STATEPOINT:
+        WorkList.emplace_back(&MI);
+        break;
+      }
+    }
+  }
+  for (MachineInstr *MI : WorkList)
+    removeMoveImmediateFromPatchpoint(MI);
+}
+
+static bool shouldFoldAsConstant(const MachineOperand &MO,
+                                 const MachineRegisterInfo &MRI, int64_t *imm) {
+  if (!MO.isReg())
+    return false;
+  bool changed = false;
+  const MachineOperand *Target = &MO;
+  while (!changed) {
+    auto defs_iterator = MRI.def_begin(Target->getReg());
+    auto defs_end = MRI.def_end();
+    if (defs_end == defs_iterator)
+      return false;
+    for (; defs_iterator != defs_end; ++defs_iterator) {
+      // Could not handle more than one defs.
+      if (changed)
+        return false;
+
+      const MachineInstr *MI = defs_iterator->getParent();
+      if (MI->isFullCopy() &&
+          TargetRegisterInfo::isVirtualRegister(MI->getOperand(1).getReg())) {
+        Target = &MI->getOperand(1);
+        break;
+      }
+      if (!MI->isMoveImmediate())
+        return false;
+      const MachineOperand &ImmOperand = MI->getOperand(1);
+      assert(ImmOperand.isImm());
+      *imm = ImmOperand.getImm();
+      changed = true;
+    }
+  }
+  return changed;
+}
+
+void RegisterCoalescer::removeMoveImmediateFromPatchpoint(
+    MachineInstr *PatchPoint) {
+  unsigned StartIdx = 0;
+  switch (PatchPoint->getOpcode()) {
+  case TargetOpcode::STACKMAP: {
+    StartIdx = StackMapOpers(PatchPoint).getVarIdx();
+    break;
+  }
+  case TargetOpcode::TCPATCHPOINT:
+  case TargetOpcode::PATCHPOINT: {
+    StartIdx = PatchPointOpers(PatchPoint).getVarIdx();
+    break;
+  }
+  case TargetOpcode::STATEPOINT: {
+    StartIdx = StatepointOpers(PatchPoint).getVarIdx();
+    break;
+  }
+  default:
+    llvm_unreachable("unexpected stackmap opcode");
+  }
+
+  MachineInstr *NewMI = MF->CreateMachineInstr(
+      TII->get(PatchPoint->getOpcode()), PatchPoint->getDebugLoc(), true);
+  MachineInstrBuilder MIB(*MF, NewMI);
+
+  // No need to fold return, the meta data, and function arguments
+  for (unsigned i = 0; i < StartIdx; ++i)
+    MIB.add(PatchPoint->getOperand(i));
+  for (unsigned i = StartIdx; i < PatchPoint->getNumOperands(); ++i) {
+    MachineOperand &MO = PatchPoint->getOperand(i);
+    int64_t imm;
+    if (!shouldFoldAsConstant(MO, *MRI, &imm)) {
+      MIB.add(MO);
+      continue;
+    }
+    MIB.addImm(StackMaps::ConstantOp);
+    MIB.addImm(imm);
+  }
+  MachineBasicBlock *MBB = PatchPoint->getParent();
+  LIS->ReplaceMachineInstrInMaps(*PatchPoint, *NewMI);
+  MachineBasicBlock::iterator Pos = PatchPoint;
+  MBB->insert(Pos, NewMI);
+  MBB->erase(PatchPoint);
+}
+
 void RegisterCoalescer::releaseMemory() {
   ErasedInstrs.clear();
   WorkList.clear();
@@ -3701,6 +3806,7 @@
 
   RegClassInfo.runOnMachineFunction(fn);
 
+  removeMoveImmediateFromPatchpoint();
   // Join (coalesce) intervals if requested.
   if (EnableJoining)
     joinAllIntervals();
Index: lib/CodeGen/SelectionDAG/InstrEmitter.cpp
===================================================================
--- lib/CodeGen/SelectionDAG/InstrEmitter.cpp	(revision 375507)
+++ lib/CodeGen/SelectionDAG/InstrEmitter.cpp	(working copy)
@@ -815,7 +815,8 @@
   const MCPhysReg *ScratchRegs = nullptr;
 
   // Handle STACKMAP and PATCHPOINT specially and then use the generic code.
-  if (Opc == TargetOpcode::STACKMAP || Opc == TargetOpcode::PATCHPOINT) {
+  if (Opc == TargetOpcode::STACKMAP || Opc == TargetOpcode::PATCHPOINT ||
+      Opc == TargetOpcode::TCPATCHPOINT) {
     // Stackmaps do not have arguments and do not preserve their calling
     // convention. However, to simplify runtime support, they clobber the same
     // scratch registers as AnyRegCC.
Index: lib/CodeGen/SelectionDAG/ScheduleDAGSDNodes.cpp
===================================================================
--- lib/CodeGen/SelectionDAG/ScheduleDAGSDNodes.cpp	(revision 375507)
+++ lib/CodeGen/SelectionDAG/ScheduleDAGSDNodes.cpp	(working copy)
@@ -556,6 +556,10 @@
     NodeNumDefs = 0;
     return;
   }
+  if (POpc == TargetOpcode::TCPATCHPOINT) {
+    NodeNumDefs = 0;
+    return;
+  }
   if (POpc == TargetOpcode::PATCHPOINT &&
       Node->getValueType(0) == MVT::Other) {
     // PATCHPOINT is defined to have one result, but it might really have none
Index: lib/CodeGen/SelectionDAG/SelectionDAGBuilder.cpp
===================================================================
--- lib/CodeGen/SelectionDAG/SelectionDAGBuilder.cpp	(revision 375507)
+++ lib/CodeGen/SelectionDAG/SelectionDAGBuilder.cpp	(working copy)
@@ -6587,6 +6587,9 @@
   case Intrinsic::experimental_gc_relocate:
     visitGCRelocate(cast<GCRelocateInst>(I));
     return;
+  case Intrinsic::experimental_gc_exception:
+    visitGCException(cast<GCExceptionInst>(I));
+    return;
   case Intrinsic::instrprof_increment:
     llvm_unreachable("instrprof failed to lower an increment");
   case Intrinsic::instrprof_value_profile:
@@ -8657,6 +8660,40 @@
   FuncInfo.MF->getFrameInfo().setHasStackMap();
 }
 
+static bool isPatchpointInTailCallPosition(ImmutableCallSite CS) {
+  const Instruction *I = CS.getInstruction();
+  const BasicBlock *ExitBB = I->getParent();
+  const Instruction *Term = ExitBB->getTerminator();
+  // Copy from isInTailCallPosition.
+  // If I will have a chain, make sure no other instruction that will have a
+  // chain interposes between I and the return.
+  if (I->mayHaveSideEffects() || I->mayReadFromMemory() ||
+      !isSafeToSpeculativelyExecute(I))
+    for (BasicBlock::const_iterator BBI = std::prev(ExitBB->end(), 2);; --BBI) {
+      if (&*BBI == I)
+        break;
+      // Debug info intrinsics do not get in the way of tail call optimization.
+      if (isa<DbgInfoIntrinsic>(BBI))
+        continue;
+      if (BBI->mayHaveSideEffects() || BBI->mayReadFromMemory() ||
+          !isSafeToSpeculativelyExecute(&*BBI))
+        return false;
+    }
+  if (isa<ReturnInst>(Term))
+    return true;
+  // Terminator must be a branch inst here
+  if (!isa<BranchInst>(Term))
+    return false;
+  const BranchInst *br = dyn_cast<BranchInst>(Term);
+  if (!br->isUnconditional())
+    return false;
+  const BasicBlock *bb = br->getSuccessor(0);
+  // This bb must contains only ret
+  if (bb->size() == 1 && isa<ReturnInst>(bb->getTerminator()))
+    return true;
+  return false;
+}
+
 /// Lower llvm.experimental.patchpoint directly to its target opcode.
 void SelectionDAGBuilder::visitPatchpoint(ImmutableCallSite CS,
                                           const BasicBlock *EHPadBB) {
@@ -8700,17 +8737,25 @@
   TargetLowering::CallLoweringInfo CLI(DAG);
   populateCallLoweringInfo(CLI, cast<CallBase>(CS.getInstruction()),
                            NumMetaOpers, NumCallArgs, Callee, ReturnTy, true);
+  CLI.IsTailCall = isPatchpointInTailCallPosition(CS);
+  assert((!CLI.IsTailCall || !HasDef) &&
+         "TailCall should not has a return type");
   std::pair<SDValue, SDValue> Result = lowerInvokable(CLI, EHPadBB);
 
-  SDNode *CallEnd = Result.second.getNode();
-  if (HasDef && (CallEnd->getOpcode() == ISD::CopyFromReg))
-    CallEnd = CallEnd->getOperand(0).getNode();
+  SDNode *Call;
+  if (!CLI.IsTailCall) {
+    SDNode *CallEnd = Result.second.getNode();
+    if (HasDef && (CallEnd->getOpcode() == ISD::CopyFromReg))
+      CallEnd = CallEnd->getOperand(0).getNode();
+    /// Get a call instruction from the call sequence chain.
+    /// Tail calls are not allowed.
+    assert(CallEnd->getOpcode() == ISD::CALLSEQ_END &&
+           "Expected a callseq node.");
+    Call = CallEnd->getOperand(0).getNode();
+  } else {
+    Call = CLI.Chain.getNode();
+  }
 
-  /// Get a call instruction from the call sequence chain.
-  /// Tail calls are not allowed.
-  assert(CallEnd->getOpcode() == ISD::CALLSEQ_END &&
-         "Expected a callseq node.");
-  SDNode *Call = CallEnd->getOperand(0).getNode();
   bool HasGlue = Call->getGluedNode();
 
   // Replace the target specific call node with the patchable intrinsic.
@@ -8781,8 +8826,9 @@
     NodeTys = DAG.getVTList(MVT::Other, MVT::Glue);
 
   // Replace the target specific call node with a PATCHPOINT node.
-  MachineSDNode *MN = DAG.getMachineNode(TargetOpcode::PATCHPOINT,
-                                         dl, NodeTys, Ops);
+  MachineSDNode *MN = DAG.getMachineNode(
+      CLI.IsTailCall ? TargetOpcode::TCPATCHPOINT : TargetOpcode::PATCHPOINT,
+      dl, NodeTys, Ops);
 
   // Update the NodeMap.
   if (HasDef) {
Index: lib/CodeGen/SelectionDAG/SelectionDAGBuilder.h
===================================================================
--- lib/CodeGen/SelectionDAG/SelectionDAGBuilder.h	(revision 375507)
+++ lib/CodeGen/SelectionDAG/SelectionDAGBuilder.h	(working copy)
@@ -759,6 +759,7 @@
   // These two are implemented in StatepointLowering.cpp
   void visitGCRelocate(const GCRelocateInst &Relocate);
   void visitGCResult(const GCResultInst &I);
+  void visitGCException(const GCExceptionInst &I);
 
   void visitVectorReduce(const CallInst &I, unsigned Intrinsic);
 
Index: lib/CodeGen/SelectionDAG/StatepointLowering.cpp
===================================================================
--- lib/CodeGen/SelectionDAG/StatepointLowering.cpp	(revision 375507)
+++ lib/CodeGen/SelectionDAG/StatepointLowering.cpp	(working copy)
@@ -562,14 +562,15 @@
   // arrays interwoven with each (lowered) base pointer immediately followed by
   // it's (lowered) derived pointer.  i.e
   // (base[0], ptr[0], base[1], ptr[1], ...)
+  bool LiveInOnly = SI.CLI.CallConv == CallingConv::V8CC;
   for (unsigned i = 0; i < SI.Bases.size(); ++i) {
     const Value *Base = SI.Bases[i];
-    lowerIncomingStatepointValue(Builder.getValue(Base), /*LiveInOnly*/ false,
-                                 Ops, MemRefs, Builder);
+    lowerIncomingStatepointValue(Builder.getValue(Base), LiveInOnly, Ops,
+                                 MemRefs, Builder);
 
     const Value *Ptr = SI.Ptrs[i];
-    lowerIncomingStatepointValue(Builder.getValue(Ptr), /*LiveInOnly*/ false,
-                                 Ops, MemRefs, Builder);
+    lowerIncomingStatepointValue(Builder.getValue(Ptr), LiveInOnly, Ops,
+                                 MemRefs, Builder);
   }
 
   // If there are any explicit spill slots passed to the statepoint, record
@@ -854,6 +855,11 @@
 
     unsigned AS = ISP.getCalledValue()->getType()->getPointerAddressSpace();
     ActualCallee = DAG.getConstant(0, getCurSDLoc(), TLI.getPointerTy(DL, AS));
+    if (auto *ConstCallee =
+            dyn_cast<ConstantSDNode>(getValue(ISP.getCalledValue())))
+      ActualCallee =
+          DAG.getIntPtrConstant(ConstCallee->getZExtValue(), getCurSDLoc(),
+                                /*isTarget=*/true);
   } else {
     ActualCallee = getValue(ISP.getCalledValue());
   }
@@ -982,6 +988,24 @@
   }
 }
 
+void SelectionDAGBuilder::visitGCException(const GCExceptionInst &CI) {
+  const TargetLowering &TLI = DAG.getTargetLoweringInfo();
+  const Constant *PersonalityFn = FuncInfo.Fn->getPersonalityFn();
+  if (TLI.getExceptionPointerRegister(PersonalityFn) == 0 &&
+      TLI.getExceptionSelectorRegister(PersonalityFn) == 0)
+    return;
+  assert(FuncInfo.MBB->isEHPad() && "Call to landingpad not in landing pad!");
+  SDValue Op;
+  SDLoc dl = getCurSDLoc();
+  Type *RetTy = CI.getFunctionType()->getReturnType();
+  Op = DAG.getZExtOrTrunc(
+      DAG.getCopyFromReg(DAG.getEntryNode(), dl,
+                         FuncInfo.ExceptionPointerVirtReg,
+                         TLI.getPointerTy(DAG.getDataLayout())),
+      dl, TLI.getValueType(DAG.getDataLayout(), RetTy));
+  setValue(&CI, Op);
+}
+
 void SelectionDAGBuilder::visitGCRelocate(const GCRelocateInst &Relocate) {
 #ifndef NDEBUG
   // Consistency check
Index: lib/CodeGen/SelectionDAG/TargetLowering.cpp
===================================================================
--- lib/CodeGen/SelectionDAG/TargetLowering.cpp	(revision 375507)
+++ lib/CodeGen/SelectionDAG/TargetLowering.cpp	(working copy)
@@ -88,6 +88,9 @@
     // (We look for a CopyFromReg reading a virtual register that is used
     //  for the function live-in value of register Reg)
     SDValue Value = OutVals[I];
+    // Ignore the undefined input.
+    if (Value->getOpcode() == ISD::UNDEF)
+      continue;
     if (Value->getOpcode() != ISD::CopyFromReg)
       return false;
     unsigned ArgReg = cast<RegisterSDNode>(Value->getOperand(1))->getReg();
Index: lib/CodeGen/ShrinkWrap.cpp
===================================================================
--- lib/CodeGen/ShrinkWrap.cpp	(revision 375507)
+++ lib/CodeGen/ShrinkWrap.cpp	(working copy)
@@ -272,6 +272,8 @@
     LLVM_DEBUG(dbgs() << "Frame instruction: " << MI << '\n');
     return true;
   }
+  if (MI.isReturn())
+    return false;
   for (const MachineOperand &MO : MI.operands()) {
     bool UseOrDefCSR = false;
     if (MO.isReg()) {
@@ -288,8 +290,8 @@
       // separately. An SP mentioned by a call instruction, we can ignore,
       // though, as it's harmless and we do not want to effectively disable tail
       // calls by forcing the restore point to post-dominate them.
-      UseOrDefCSR = (!MI.isCall() && PhysReg == SP) ||
-                    RCI.getLastCalleeSavedAlias(PhysReg);
+      UseOrDefCSR =
+          (!MI.isCall() && PhysReg == SP) || getCurrentCSRs(RS).count(PhysReg);
     } else if (MO.isRegMask()) {
       // Check if this regmask clobbers any of the CSRs.
       for (unsigned Reg : getCurrentCSRs(RS)) {
Index: lib/CodeGen/StackMaps.cpp
===================================================================
--- lib/CodeGen/StackMaps.cpp	(revision 375507)
+++ lib/CodeGen/StackMaps.cpp	(working copy)
@@ -29,6 +29,7 @@
 #include "llvm/Support/ErrorHandling.h"
 #include "llvm/Support/MathExtras.h"
 #include "llvm/Support/raw_ostream.h"
+#include "llvm/Target/TargetMachine.h"
 #include <algorithm>
 #include <cassert>
 #include <cstdint>
@@ -370,7 +371,9 @@
 }
 
 void StackMaps::recordPatchPoint(const MachineInstr &MI) {
-  assert(MI.getOpcode() == TargetOpcode::PATCHPOINT && "expected patchpoint");
+  assert(((MI.getOpcode() == TargetOpcode::PATCHPOINT) ||
+          (MI.getOpcode() == TargetOpcode::TCPATCHPOINT)) &&
+         "expected patchpoint");
 
   PatchPointOpers opers(&MI);
   const int64_t ID = opers.getID();
@@ -442,7 +445,7 @@
     LLVM_DEBUG(dbgs() << WSMP << "function addr: " << FR.first
                       << " frame size: " << FR.second.StackSize
                       << " callsite count: " << FR.second.RecordCount << '\n');
-    OS.EmitSymbolValue(FR.first, 8);
+    OS.EmitSymbolValue(FR.first, AP.TM.getProgramPointerSize());
     OS.EmitIntValue(FR.second.StackSize, 8);
     OS.EmitIntValue(FR.second.RecordCount, 8);
   }
Index: lib/CodeGen/TargetInstrInfo.cpp
===================================================================
--- lib/CodeGen/TargetInstrInfo.cpp	(revision 375507)
+++ lib/CodeGen/TargetInstrInfo.cpp	(working copy)
@@ -475,6 +475,7 @@
     StartIdx = StackMapOpers(&MI).getVarIdx();
     break;
   }
+  case TargetOpcode::TCPATCHPOINT:
   case TargetOpcode::PATCHPOINT: {
     // For PatchPoint, the call args are not foldable (even if reported in the
     // stackmap e.g. via anyregcc).
@@ -570,6 +571,7 @@
 
   if (MI.getOpcode() == TargetOpcode::STACKMAP ||
       MI.getOpcode() == TargetOpcode::PATCHPOINT ||
+      MI.getOpcode() == TargetOpcode::TCPATCHPOINT ||
       MI.getOpcode() == TargetOpcode::STATEPOINT) {
     // Fold stackmap/patchpoint.
     NewMI = foldPatchpoint(MF, MI, Ops, FI, *this);
@@ -634,6 +636,7 @@
   int FrameIndex = 0;
 
   if ((MI.getOpcode() == TargetOpcode::STACKMAP ||
+       MI.getOpcode() == TargetOpcode::TCPATCHPOINT ||
        MI.getOpcode() == TargetOpcode::PATCHPOINT ||
        MI.getOpcode() == TargetOpcode::STATEPOINT) &&
       isLoadFromStackSlot(LoadMI, FrameIndex)) {
Index: lib/CodeGen/TargetPassConfig.cpp
===================================================================
--- lib/CodeGen/TargetPassConfig.cpp	(revision 375507)
+++ lib/CodeGen/TargetPassConfig.cpp	(working copy)
@@ -951,6 +951,7 @@
   if (getOptLevel() != CodeGenOpt::None)
     addBlockPlacement();
 
+  addPass(&StackMapLivenessID, false);
   addPreEmitPass();
 
   if (TM->Options.EnableIPRA)
@@ -960,7 +961,6 @@
 
   addPass(&FuncletLayoutID, false);
 
-  addPass(&StackMapLivenessID, false);
   addPass(&LiveDebugValuesID, false);
 
   // Insert before XRay Instrumentation.
Index: lib/ExecutionEngine/ExecutionEngineBindings.cpp
===================================================================
--- lib/ExecutionEngine/ExecutionEngineBindings.cpp	(revision 375507)
+++ lib/ExecutionEngine/ExecutionEngineBindings.cpp	(working copy)
@@ -199,6 +199,7 @@
          .setErrorStr(&Error)
          .setOptLevel((CodeGenOpt::Level)options.OptLevel)
          .setTargetOptions(targetOptions);
+  builder.setRelocationModel(Reloc::PIC_);
   bool JIT;
   if (Optional<CodeModel::Model> CM = unwrap(options.CodeModel, JIT))
     builder.setCodeModel(*CM);
Index: lib/IR/AsmWriter.cpp
===================================================================
--- lib/IR/AsmWriter.cpp	(revision 375507)
+++ lib/IR/AsmWriter.cpp	(working copy)
@@ -348,6 +348,12 @@
   case CallingConv::Cold:          Out << "coldcc"; break;
   case CallingConv::WebKit_JS:     Out << "webkit_jscc"; break;
   case CallingConv::AnyReg:        Out << "anyregcc"; break;
+  case CallingConv::V8CC:
+    Out << "v8cc";
+    break;
+  case CallingConv::V8SBCC:
+    Out << "v8sbcc";
+    break;
   case CallingConv::PreserveMost:  Out << "preserve_mostcc"; break;
   case CallingConv::PreserveAll:   Out << "preserve_allcc"; break;
   case CallingConv::CXX_FAST_TLS:  Out << "cxx_fast_tlscc"; break;
Index: lib/IR/Verifier.cpp
===================================================================
--- lib/IR/Verifier.cpp	(revision 375507)
+++ lib/IR/Verifier.cpp	(working copy)
@@ -2032,7 +2032,8 @@
     Assert(UserCall, "illegal use of statepoint token", Call, U);
     if (!UserCall)
       continue;
-    Assert(isa<GCRelocateInst>(UserCall) || isa<GCResultInst>(UserCall),
+    Assert(isa<GCRelocateInst>(UserCall) || isa<GCResultInst>(UserCall) ||
+               isa<GCExceptionInst>(UserCall),
            "gc.result or gc.relocate are the only value uses "
            "of a gc.statepoint",
            Call, U);
@@ -2042,6 +2043,9 @@
     } else if (isa<GCRelocateInst>(Call)) {
       Assert(UserCall->getArgOperand(0) == &Call,
              "gc.relocate connected to wrong gc.statepoint", Call, UserCall);
+    } else if (isa<GCExceptionInst>(UserCall)) {
+      Assert(UserCall->getArgOperand(0) == &Call,
+             "gc.exception connected to wrong gc.statepoint", Call, UserCall);
     }
   }
 
Index: lib/MC/MCAsmStreamer.cpp
===================================================================
--- lib/MC/MCAsmStreamer.cpp	(revision 375507)
+++ lib/MC/MCAsmStreamer.cpp	(working copy)
@@ -945,7 +945,7 @@
   if (!Directive) {
     int64_t IntValue;
     if (!Value->evaluateAsAbsolute(IntValue))
-      report_fatal_error("Don't know how to emit this value.");
+      IntValue = -1;
 
     // We couldn't handle the requested integer size so we fallback by breaking
     // the request down into several, smaller, integers.
Index: lib/Target/ARM/ARMAsmPrinter.cpp
===================================================================
--- lib/Target/ARM/ARMAsmPrinter.cpp	(revision 375507)
+++ lib/Target/ARM/ARMAsmPrinter.cpp	(working copy)
@@ -55,7 +55,7 @@
 ARMAsmPrinter::ARMAsmPrinter(TargetMachine &TM,
                              std::unique_ptr<MCStreamer> Streamer)
     : AsmPrinter(TM, std::move(Streamer)), AFI(nullptr), MCP(nullptr),
-      InConstantPool(false), OptimizationGoals(-1) {}
+      InConstantPool(false), OptimizationGoals(-1), SM(*this) {}
 
 void ARMAsmPrinter::EmitFunctionBodyEnd() {
   // Make sure to terminate any constant pools that were at the end
@@ -548,6 +548,7 @@
       OutStreamer->AddBlankLine();
     }
 
+    SM.serializeToStackMapSection();
     // Funny Darwin hack: This flag tells the linker that no global symbols
     // contain code that falls through to other global symbols (e.g. the obvious
     // implementation of multiple entry points).  If this doesn't occur, the
@@ -567,6 +568,15 @@
   OptimizationGoals = -1;
 
   ATS.finishAttributeSection();
+  if (TT.isOSBinFormatCOFF()) {
+    SM.serializeToStackMapSection();
+    return;
+  }
+
+  if (TT.isOSBinFormatELF()) {
+    SM.serializeToStackMapSection();
+    return;
+  }
 }
 
 //===----------------------------------------------------------------------===//
@@ -2129,6 +2139,13 @@
   case ARM::PATCHABLE_TAIL_CALL:
     LowerPATCHABLE_TAIL_CALL(*MI);
     return;
+  case TargetOpcode::STACKMAP:
+    return LowerSTACKMAP(*OutStreamer, SM, *MI);
+  case TargetOpcode::PATCHPOINT:
+  case TargetOpcode::TCPATCHPOINT:
+    return LowerPATCHPOINT(*OutStreamer, SM, *MI);
+  case TargetOpcode::STATEPOINT:
+    return LowerSTATEPOINT(*OutStreamer, SM, *MI);
   }
 
   MCInst TmpInst;
@@ -2137,6 +2154,107 @@
   EmitToStreamer(*OutStreamer, TmpInst);
 }
 
+static unsigned roundUpTo4ByteAligned(unsigned n) {
+  unsigned mask = 3;
+  unsigned rev = ~3;
+  n = (n & rev) + (((n & mask) + mask) & rev);
+  return n;
+}
+
+void ARMAsmPrinter::LowerSTACKMAP(MCStreamer &OutStreamer, StackMaps &SM,
+                                  const MachineInstr &MI) {
+  assert(!AFI->isThumbFunction());
+  unsigned NumNOPBytes =
+      roundUpTo4ByteAligned(StackMapOpers(&MI).getNumPatchBytes());
+
+  SM.recordStackMap(MI);
+  assert(NumNOPBytes % 4 == 0 && "Invalid number of NOP bytes requested!");
+
+  // Scan ahead to trim the shadow.
+  const MachineBasicBlock &MBB = *MI.getParent();
+  MachineBasicBlock::const_iterator MII(MI);
+  ++MII;
+  while (NumNOPBytes > 0) {
+    if (MII == MBB.end() || MII->isCall() ||
+        MII->getOpcode() == ARM::DBG_VALUE ||
+        MII->getOpcode() == TargetOpcode::PATCHPOINT ||
+        MII->getOpcode() == TargetOpcode::TCPATCHPOINT ||
+        MII->getOpcode() == TargetOpcode::STACKMAP)
+      break;
+    ++MII;
+    NumNOPBytes -= 4;
+  }
+
+  // Emit nops.
+  MCInst Noop;
+  Subtarget->getInstrInfo()->getNoop(Noop);
+  for (unsigned i = 0; i < NumNOPBytes; i += 4)
+    EmitToStreamer(OutStreamer, Noop);
+}
+
+// Lower a patchpoint of the form:
+// [<def>], <id>, <numBytes>, <target>, <numArgs>
+void ARMAsmPrinter::LowerPATCHPOINT(MCStreamer &OutStreamer, StackMaps &SM,
+                                    const MachineInstr &MI) {
+  assert(!AFI->isThumbFunction());
+  SM.recordPatchPoint(MI);
+
+  PatchPointOpers Opers(&MI);
+
+  int64_t CallTarget = Opers.getCallTarget().getImm();
+  unsigned EncodedBytes = 0;
+  if (CallTarget) {
+    assert((CallTarget & 0xFFFFFFFFLL) == CallTarget &&
+           "High 32 bits of call target should be zero.");
+    unsigned ScratchReg = MI.getOperand(Opers.getNextScratchIdx()).getReg();
+    EncodedBytes = 16;
+    EmitToStreamer(OutStreamer, MCInstBuilder(ARM::MOVTi16)
+                                    .addReg(ScratchReg)
+                                    .addReg(ScratchReg)
+                                    .addImm((CallTarget >> 16) & 0xFFFF)
+                                    .addImm(ARMCC::AL)
+                                    .addImm(0));
+    EmitToStreamer(OutStreamer, MCInstBuilder(ARM::MOVi16)
+                                    .addReg(ScratchReg)
+                                    .addImm(CallTarget & 0xFFFF)
+                                    .addImm(ARMCC::AL)
+                                    .addImm(0));
+    EmitToStreamer(OutStreamer, MCInstBuilder(ARM::BLX).addReg(ScratchReg));
+  }
+  // Emit padding.
+  unsigned NumBytes = roundUpTo4ByteAligned(Opers.getNumPatchBytes());
+  assert(NumBytes >= EncodedBytes &&
+         "Patchpoint can't request size less than the length of a call.");
+  assert((NumBytes - EncodedBytes) % 4 == 0 &&
+         "Invalid number of NOP bytes requested!");
+  MCInst Noop;
+  Subtarget->getInstrInfo()->getNoop(Noop);
+  for (unsigned i = EncodedBytes; i < NumBytes; i += 4)
+    EmitToStreamer(OutStreamer, Noop);
+}
+
+void ARMAsmPrinter::LowerSTATEPOINT(MCStreamer &OutStreamer, StackMaps &SM,
+                                    const MachineInstr &MI) {
+  assert(!AFI->isThumbFunction());
+  SM.recordStatepoint(MI);
+
+  StatepointOpers SOpers(&MI);
+  MCInst Noop;
+  Subtarget->getInstrInfo()->getNoop(Noop);
+  if (unsigned PatchBytes = SOpers.getNumPatchBytes()) {
+    unsigned NumBytes = roundUpTo4ByteAligned(PatchBytes);
+    unsigned EncodedBytes = 0;
+    assert(NumBytes >= EncodedBytes &&
+           "Patchpoint can't request size less than the length of a call.");
+    assert((NumBytes - EncodedBytes) % 4 == 0 &&
+           "Invalid number of NOP bytes requested!");
+    MCInst Noop;
+    Subtarget->getInstrInfo()->getNoop(Noop);
+    for (unsigned i = EncodedBytes; i < NumBytes; i += 4)
+      EmitToStreamer(OutStreamer, Noop);
+  }
+}
+
 //===----------------------------------------------------------------------===//
 // Target Registry Stuff
 //===----------------------------------------------------------------------===//
Index: lib/Target/ARM/ARMAsmPrinter.h
===================================================================
--- lib/Target/ARM/ARMAsmPrinter.h	(revision 375507)
+++ lib/Target/ARM/ARMAsmPrinter.h	(working copy)
@@ -11,6 +11,7 @@
 
 #include "ARMSubtarget.h"
 #include "llvm/CodeGen/AsmPrinter.h"
+#include "llvm/CodeGen/StackMaps.h"
 #include "llvm/Target/TargetMachine.h"
 
 namespace llvm {
@@ -63,8 +64,10 @@
   /// Set of globals in PromotedGlobals that we've emitted labels for.
   /// We need to emit labels even for promoted globals so that DWARF
   /// debug info can link properly.
-  SmallPtrSet<const GlobalVariable*,2> EmittedPromotedGlobalLabels;
+  SmallPtrSet<const GlobalVariable *, 2> EmittedPromotedGlobalLabels;
 
+  StackMaps SM;
+
 public:
   explicit ARMAsmPrinter(TargetMachine &TM,
                          std::unique_ptr<MCStreamer> Streamer);
@@ -129,6 +132,15 @@
   bool emitPseudoExpansionLowering(MCStreamer &OutStreamer,
                                    const MachineInstr *MI);
 
+  void LowerSTACKMAP(MCStreamer &OutStreamer, StackMaps &SM,
+                     const MachineInstr &MI);
+
+  void LowerPATCHPOINT(MCStreamer &OutStreamer, StackMaps &SM,
+                       const MachineInstr &MI);
+
+  void LowerSTATEPOINT(MCStreamer &OutStreamer, StackMaps &SM,
+                       const MachineInstr &MI);
+
 public:
   unsigned getISAEncoding() override {
     // ARM/Darwin adds ISA to the DWARF info for each function.
Index: lib/Target/ARM/ARMBaseInstrInfo.cpp
===================================================================
--- lib/Target/ARM/ARMBaseInstrInfo.cpp	(revision 375507)
+++ lib/Target/ARM/ARMBaseInstrInfo.cpp	(working copy)
@@ -712,6 +712,10 @@
     return 0;
   case TargetOpcode::BUNDLE:
     return getInstBundleLength(MI);
+  case TargetOpcode::TCPATCHPOINT:
+  case TargetOpcode::PATCHPOINT:
+  case TargetOpcode::STATEPOINT:
+    return MI.getOperand(1).getImm();
   case ARM::MOVi16_ga_pcrel:
   case ARM::MOVTi16_ga_pcrel:
   case ARM::t2MOVi16_ga_pcrel:
@@ -1560,6 +1564,72 @@
   BB->erase(MI);
 }
 
+void ARMBaseInstrInfo::expandRESTORESP(MachineBasicBlock::iterator MI) const {
+  MachineBasicBlock *BB = MI->getParent();
+  bool ShouldExpand = true;
+  MachineBasicBlock::iterator it = MI;
+  ++it;
+  MachineBasicBlock::iterator end = MI->getParent()->end();
+  for (; it != end; ++it) {
+    bool IsDef = false, IsUse = false;
+    if (it->isCall()) {
+      ShouldExpand = true;
+      break;
+    }
+    for (unsigned i = 0, e = it->getNumOperands(); i != e; ++i) {
+      const MachineOperand &MO = it->getOperand(i);
+      if (!MO.isReg())
+        continue;
+      unsigned MOReg = MO.getReg();
+      if (MOReg != ARM::SP)
+        continue;
+      // Define new sp
+      if (MO.isDef())
+        IsDef = true;
+      else
+        IsUse = true;
+    }
+    // Should expand now.
+    if (IsUse)
+      break;
+    if (IsDef) {
+      ShouldExpand = false;
+      break;
+    }
+  }
+  if (ShouldExpand) {
+    MachineFunction *MF = BB->getParent();
+    DebugLoc dl = MI->getDebugLoc();
+
+    ARMFunctionInfo *AFI = MF->getInfo<ARMFunctionInfo>();
+    BuildMI(*BB, MI, dl, get(ARM::SUBri), ARM::SP)
+        .addReg(ARM::R11)
+        .addImm(AFI->getFramePtrSpillOffset())
+        .add(predOps(ARMCC::AL))
+        .add(condCodeOp());
+  }
+  BB->erase(MI);
+}
+
+int ARMBaseInstrInfo::getSPAdjust(const MachineInstr &MI) const {
+
+  const MachineFunction *MF = MI.getParent()->getParent();
+  const ARMFunctionInfo *AFI = MF->getInfo<ARMFunctionInfo>();
+  if (!AFI->isJSFunction() && !AFI->isJSStub())
+    return ARMGenInstrInfo::getSPAdjust(MI);
+  if (MI.isCall()) {
+    if (AFI->isJSStub() || AFI->isJSFunction())
+      return -AFI->popLastSPAdjust();
+  }
+  unsigned FrameSetupOpcode = getCallFrameSetupOpcode();
+  if (MI.getOpcode() == FrameSetupOpcode) {
+    int SPAdj = getFrameSize(MI);
+    AFI->pushLastSPAdjust(SPAdj);
+    return SPAdj;
+  }
+  return 0;
+}
+
 bool ARMBaseInstrInfo::expandPostRAPseudo(MachineInstr &MI) const {
   if (MI.getOpcode() == TargetOpcode::LOAD_STACK_GUARD) {
     assert(getSubtarget().getTargetTriple().isOSBinFormatMachO() &&
@@ -1574,6 +1644,11 @@
     return true;
   }
 
+  if (MI.getOpcode() == ARM::RESTORESP) {
+    expandRESTORESP(MI);
+    return true;
+  }
+
   // This hook gets to expand COPY instructions before they become
   // copyPhysReg() calls.  Look for VMOVS instructions that can legally be
   // widened to VMOVD.  We prefer the VMOVD when possible because it may be
Index: lib/Target/ARM/ARMBaseInstrInfo.h
===================================================================
--- lib/Target/ARM/ARMBaseInstrInfo.h	(revision 375507)
+++ lib/Target/ARM/ARMBaseInstrInfo.h	(working copy)
@@ -397,6 +397,8 @@
   virtual void expandLoadStackGuard(MachineBasicBlock::iterator MI) const = 0;
 
   void expandMEMCPY(MachineBasicBlock::iterator) const;
+  void expandRESTORESP(MachineBasicBlock::iterator) const;
+  int getSPAdjust(const MachineInstr &MI) const final override;
 
   /// Identify instructions that can be folded into a MOVCC instruction, and
   /// return the defining instruction.
Index: lib/Target/ARM/ARMBaseRegisterInfo.cpp
===================================================================
--- lib/Target/ARM/ARMBaseRegisterInfo.cpp	(revision 375507)
+++ lib/Target/ARM/ARMBaseRegisterInfo.cpp	(working copy)
@@ -89,6 +89,10 @@
       // exception handling.
       return CSR_GenericInt_SaveList;
     }
+  } else if (F.getCallingConv() == CallingConv::V8CC) {
+    return CSR_V8CC_SaveList;
+  } else if (F.getCallingConv() == CallingConv::V8SBCC) {
+    return CSR_V8SBCC_SaveList;
   }
 
   if (STI.getTargetLowering()->supportSwiftError() &&
@@ -123,6 +127,10 @@
   if (CC == CallingConv::GHC)
     // This is academic because all GHC calls are (supposed to be) tail calls
     return CSR_NoRegs_RegMask;
+  if (CC == CallingConv::V8CC)
+    return CSR_V8CC_RegMask;
+  if (CC == CallingConv::V8SBCC)
+    return CSR_V8SBCC_RegMask;
 
   if (STI.getTargetLowering()->supportSwiftError() &&
       MF.getFunction().getAttributes().hasAttrSomewhere(Attribute::SwiftError))
@@ -206,6 +214,19 @@
   // For v8.1m architecture
   markSuperRegs(Reserved, ARM::ZR);
 
+  const Function &F = MF.getFunction();
+  if (F.getCallingConv() == CallingConv::V8SBCC) {
+    markSuperRegs(Reserved, ARM::R5);
+    markSuperRegs(Reserved, ARM::R6);
+    markSuperRegs(Reserved, ARM::R7);
+    markSuperRegs(Reserved, ARM::R8);
+    markSuperRegs(Reserved, ARM::R9);
+    markSuperRegs(Reserved, ARM::R10);
+    markSuperRegs(Reserved, ARM::R11);
+  } else if (F.getCallingConv() == CallingConv::V8CC) {
+    markSuperRegs(Reserved, ARM::R10);
+    markSuperRegs(Reserved, ARM::R11);
+  }
   assert(checkAllSuperRegsMarked(Reserved));
   return Reserved;
 }
@@ -370,6 +391,8 @@
   const ARMFunctionInfo *AFI = MF.getInfo<ARMFunctionInfo>();
   const ARMFrameLowering *TFI = getFrameLowering(MF);
 
+  if (MF.getFunction().getCallingConv() == CallingConv::V8CC)
+    return false;
   // If we have stack realignment and VLAs, we have no pointer to use to
   // access the stack. If we have stack realignment, and a large call frame,
   // we have no place to allocate the emergency spill slot.
@@ -410,6 +433,8 @@
   // 2. There are VLAs in the function and the base pointer is disabled.
   if (!TargetRegisterInfo::canRealignStack(MF))
     return false;
+  if (MF.getFunction().getCallingConv() == CallingConv::V8CC)
+    return true;
   // Stack realignment requires a frame pointer.  If we already started
   // register allocation with frame pointer elimination, it is too late now.
   if (!MRI->canReserveReg(getFramePointerReg(MF.getSubtarget<ARMSubtarget>())))
@@ -869,3 +894,18 @@
   }
   return false;
 }
+
+TargetRegisterInfo::VirtRegToFixSlotMap
+ARMBaseRegisterInfo::getHoistToFixStackSlotMap(MachineFunction &MF) const {
+  const MachineRegisterInfo &MRI = MF.getRegInfo();
+  const ARMFunctionInfo *AFI = MF.getInfo<ARMFunctionInfo>();
+  MachineFrameInfo &MFI = MF.getFrameInfo();
+  VirtRegToFixSlotMap result;
+  for (auto &livein : MRI.liveins()) {
+    if (AFI->isWASM() && (livein.first == ARM::R3)) {
+      result.emplace_back(livein.second,
+                          MFI.CreateFixedSpillStackObject(4, -16));
+    }
+  }
+  return result;
+}
Index: lib/Target/ARM/ARMBaseRegisterInfo.h
===================================================================
--- lib/Target/ARM/ARMBaseRegisterInfo.h	(revision 375507)
+++ lib/Target/ARM/ARMBaseRegisterInfo.h	(working copy)
@@ -209,6 +209,8 @@
                       unsigned DstSubReg,
                       const TargetRegisterClass *NewRC,
                       LiveIntervals &LIS) const override;
+
+  VirtRegToFixSlotMap getHoistToFixStackSlotMap(MachineFunction &) const final;
 };
 
 } // end namespace llvm
Index: lib/Target/ARM/ARMCallingConv.h
===================================================================
--- lib/Target/ARM/ARMCallingConv.h	(revision 375507)
+++ lib/Target/ARM/ARMCallingConv.h	(working copy)
@@ -29,6 +29,12 @@
 bool CC_ARM_APCS_GHC(unsigned ValNo, MVT ValVT, MVT LocVT,
                      CCValAssign::LocInfo LocInfo, ISD::ArgFlagsTy ArgFlags,
                      CCState &State);
+bool CC_ARM_V8(unsigned ValNo, MVT ValVT,
+                     MVT LocVT, CCValAssign::LocInfo LocInfo,
+                     ISD::ArgFlagsTy ArgFlags, CCState &State);
+bool CC_ARM_V8SB(unsigned ValNo, MVT ValVT,
+                       MVT LocVT, CCValAssign::LocInfo LocInfo,
+                       ISD::ArgFlagsTy ArgFlags, CCState &State);
 bool FastCC_ARM_APCS(unsigned ValNo, MVT ValVT, MVT LocVT,
                      CCValAssign::LocInfo LocInfo, ISD::ArgFlagsTy ArgFlags,
                      CCState &State);
@@ -41,6 +47,12 @@
 bool RetCC_ARM_APCS(unsigned ValNo, MVT ValVT, MVT LocVT,
                     CCValAssign::LocInfo LocInfo, ISD::ArgFlagsTy ArgFlags,
                     CCState &State);
+bool RetCC_ARM_V8(unsigned ValNo, MVT ValVT,
+                        MVT LocVT, CCValAssign::LocInfo LocInfo,
+                        ISD::ArgFlagsTy ArgFlags, CCState &State);
+bool RetCC_ARM_V8SB(unsigned ValNo, MVT ValVT,
+                          MVT LocVT, CCValAssign::LocInfo LocInfo,
+                          ISD::ArgFlagsTy ArgFlags, CCState &State);
 bool RetFastCC_ARM_APCS(unsigned ValNo, MVT ValVT, MVT LocVT,
                         CCValAssign::LocInfo LocInfo, ISD::ArgFlagsTy ArgFlags,
                         CCState &State);
Index: lib/Target/ARM/ARMCallingConv.td
===================================================================
--- lib/Target/ARM/ARMCallingConv.td	(revision 375507)
+++ lib/Target/ARM/ARMCallingConv.td	(working copy)
@@ -247,6 +247,65 @@
 ]>;
 
 //===----------------------------------------------------------------------===//
+// V8 Calling Conventions
+//===----------------------------------------------------------------------===//
+
+let Entry = 1 in
+def CC_ARM_V8: CallingConv<[
+  CCIfType<[i1, i8, i16], CCPromoteToType<i32>>,
+
+  // i64/f64 is passed in even pairs of GPRs
+  // i64 is 8-aligned i32 here, so we may need to eat R1 as a pad register
+  // (and the same is true for f64 if VFP is not enabled)
+  CCIfType<[i32], CCIfAlign<"8", CCAssignToRegWithShadow<[R0, R2], [R0, R1]>>>,
+  CCIfType<[i32], CCIf<"ArgFlags.getOrigAlign() != 8",
+                       CCAssignToReg<[R0, R1, R2, R3, R4, R5, R6, R7, R8, R9, R10, R11]>>>,
+
+  CCIfType<[i32], CCIfAlign<"8", CCAssignToStackWithShadow<4, 8, [R0, R1, R2, R3, R4, R5, R6, R7, R8, R9, R10, R11]>>>,
+  CCIfType<[i32], CCAssignToStackWithShadow<4, 4, [R0, R1, R2, R3, R4, R5, R6, R7, R8, R9, R10, R11]>>,
+  CCIfType<[f32], CCAssignToReg<[S0, S1, S2, S3, S4, S5, S6, S7, S8,
+                                 S9, S10, S11, S12, S13, S14, S15]>>,
+  CCIfType<[f64], CCAssignToReg<[D0, D1, D2, D3, D4, D5, D6, D7]>>
+]>;
+
+let Entry = 1 in
+def RetCC_ARM_V8: CallingConv<[
+  CCIfType<[i1, i8, i16], CCPromoteToType<i32>>,
+  CCIfType<[i32], CCAssignToReg<[R0, R1, R2, R3]>>,
+  CCIfType<[i64], CCAssignToRegWithShadow<[R0, R2], [R1, R3]>>,
+  CCIfType<[f64], CCAssignToReg<[D0, D1, D2, D3, D4, D5, D6, D7]>>,
+  CCIfType<[f32], CCAssignToReg<[S0, S1, S2, S3, S4, S5, S6, S7, S8,
+                                 S9, S10, S11, S12, S13, S14, S15]>>
+]>;
+
+let Entry = 1 in
+def CC_ARM_V8SB: CallingConv<[
+  CCIfType<[i1, i8, i16], CCPromoteToType<i32>>,
+
+  // i64/f64 is passed in even pairs of GPRs
+  // i64 is 8-aligned i32 here, so we may need to eat R1 as a pad register
+  // (and the same is true for f64 if VFP is not enabled)
+  CCIfType<[i32], CCIfAlign<"8", CCAssignToRegWithShadow<[R0, R2], [R0, R1]>>>,
+  CCIfType<[i32], CCIf<"ArgFlags.getOrigAlign() != 8",
+                       CCAssignToReg<[R0, R1, R2, R3, R4, R10, R11, R12]>>>,
+
+  CCIfType<[i32], CCIfAlign<"8", CCAssignToStackWithShadow<4, 8, [R0, R1, R2, R3, R4, R10, R11, R12]>>>,
+  CCIfType<[i32], CCAssignToStackWithShadow<4, 4, [R0, R1, R2, R3, R4, R10, R11, R12]>>,
+  CCIfType<[f32], CCAssignToStackWithShadow<4, 4, [Q0, Q1, Q2, Q3]>>,
+  CCIfType<[f64], CCAssignToStackWithShadow<8, 8, [Q0, Q1, Q2, Q3]>>,
+  CCIfType<[v2f64], CCIfAlign<"16",
+           CCAssignToStackWithShadow<16, 16, [Q0, Q1, Q2, Q3]>>>,
+  CCIfType<[v2f64], CCAssignToStackWithShadow<16, 8, [Q0, Q1, Q2, Q3]>>
+]>;
+
+let Entry = 1 in
+def RetCC_ARM_V8SB: CallingConv<[
+  CCIfType<[i1, i8, i16], CCPromoteToType<i32>>,
+  CCIfType<[i32], CCAssignToReg<[R0, R1, R2, R3]>>,
+  CCIfType<[i64], CCAssignToRegWithShadow<[R0, R2], [R1, R3]>>
+]>;
+
+//===----------------------------------------------------------------------===//
 // Callee-saved register lists.
 //===----------------------------------------------------------------------===//
 
@@ -256,6 +315,11 @@
 def CSR_AAPCS : CalleeSavedRegs<(add LR, R11, R10, R9, R8, R7, R6, R5, R4,
                                      (sequence "D%u", 15, 8))>;
 
+def CSR_V8CC : CalleeSavedRegs<(add LR, R11)>;
+
+def CSR_V8SBCC : CalleeSavedRegs<(add LR, R11, R10, R9, R8, R7, R6, R5,
+                                     (sequence "D%u", 31, 0))>;
+
 // R8 is used to pass swifterror, remove it from CSR.
 def CSR_AAPCS_SwiftError : CalleeSavedRegs<(sub CSR_AAPCS, R8)>;
 
Index: lib/Target/ARM/ARMConstantIslandPass.cpp
===================================================================
--- lib/Target/ARM/ARMConstantIslandPass.cpp	(revision 375507)
+++ lib/Target/ARM/ARMConstantIslandPass.cpp	(working copy)
@@ -240,6 +240,7 @@
     CPEntry *findConstPoolEntry(unsigned CPI, const MachineInstr *CPEMI);
     unsigned getCPELogAlign(const MachineInstr *CPEMI);
     void scanFunctionJumpTables();
+    bool replaceInlineAsm();
     void initializeFunctionInfo(const std::vector<MachineInstr*> &CPEMIs);
     MachineBasicBlock *splitBlockBeforeInstr(MachineInstr *MI);
     void updateForInsertedWaterBlock(MachineBasicBlock *NewBB);
@@ -366,7 +367,7 @@
 
   // Try to reorder and otherwise adjust the block layout to make good use
   // of the TB[BH] instructions.
-  bool MadeChange = false;
+  bool MadeChange = replaceInlineAsm();
   if (GenerateTBB && AdjustJumpTableBlocks) {
     scanFunctionJumpTables();
     MadeChange |= reorderThumb2JumpTables();
@@ -677,6 +678,49 @@
   }
 }
 
+bool ARMConstantIslands::replaceInlineAsm() {
+  bool MadeChange = false;
+  std::vector<MachineInstr *> toRemove;
+  for (MachineBasicBlock &MBB : *MF) {
+    for (MachineInstr &MI : MBB) {
+      if (MI.getOpcode() == TargetOpcode::INLINEASM) {
+        // handle const pool load
+        if (!strcmp(MI.getOperand(0).getSymbolName(), "ldr $0, =${1:c}")) {
+          MachineFunction *MF = MBB.getParent();
+          MachineConstantPool *ConstantPool = MF->getConstantPool();
+          Type *Int32Ty = Type::getInt32Ty(MF->getFunction().getContext());
+          const Constant *C =
+              ConstantInt::get(Int32Ty, MI.getOperand(5).getImm());
+
+          // MachineConstantPool wants an explicit alignment.
+          unsigned Align = MF->getDataLayout().getPrefTypeAlignment(Int32Ty);
+          if (Align == 0)
+            Align = MF->getDataLayout().getTypeAllocSize(C->getType());
+          unsigned Idx = ConstantPool->getConstantPoolIndex(C, Align);
+
+          if (isThumb)
+            BuildMI(MBB, MI, MI.getDebugLoc(), TII->get(ARM::tLDRpci))
+                .addReg(MI.getOperand(3).getReg(), RegState::Define)
+                .addConstantPoolIndex(Idx)
+                .add(predOps(ARMCC::AL));
+          else
+            BuildMI(MBB, MI, MI.getDebugLoc(), TII->get(ARM::LDRcp))
+                .addReg(MI.getOperand(3).getReg(), RegState::Define)
+                .addConstantPoolIndex(Idx)
+                .addImm(0)
+                .add(predOps(ARMCC::AL));
+          toRemove.push_back(&MI);
+          MadeChange = true;
+        }
+      }
+    }
+  }
+  for (MachineInstr *MI : toRemove) {
+    MI->eraseFromParent();
+  }
+  return MadeChange;
+}
+
 /// initializeFunctionInfo - Do the initial scan of the function, building up
 /// information about the sizes of each block, the location of all the water,
 /// and finding all of the constant pool users.
Index: lib/Target/ARM/ARMFastISel.cpp
===================================================================
--- lib/Target/ARM/ARMFastISel.cpp	(revision 375507)
+++ lib/Target/ARM/ARMFastISel.cpp	(working copy)
@@ -1889,6 +1889,10 @@
       report_fatal_error("Can't return in GHC call convention");
     else
       return CC_ARM_APCS_GHC;
+  case CallingConv::V8CC:
+    return (Return ? RetCC_ARM_V8 : CC_ARM_V8);
+  case CallingConv::V8SBCC:
+    return (Return ? RetCC_ARM_V8SB : CC_ARM_V8SB);
   }
 }
 
Index: lib/Target/ARM/ARMFrameLowering.cpp
===================================================================
--- lib/Target/ARM/ARMFrameLowering.cpp	(revision 375507)
+++ lib/Target/ARM/ARMFrameLowering.cpp	(working copy)
@@ -122,6 +122,9 @@
 /// included as part of the stack frame.
 bool ARMFrameLowering::hasReservedCallFrame(const MachineFunction &MF) const {
   const MachineFrameInfo &MFI = MF.getFrameInfo();
+  const ARMFunctionInfo *AFI = MF.getInfo<ARMFunctionInfo>();
+  if (AFI->isJSStub() || AFI->isJSFunction())
+    return false;
   unsigned CFSize = MFI.getMaxCallFrameSize();
   // It's not always a good idea to include the call frame as part of the
   // stack frame. ARM (especially Thumb) has small immediate offset to
@@ -355,9 +358,13 @@
 
 void ARMFrameLowering::emitPrologue(MachineFunction &MF,
                                     MachineBasicBlock &MBB) const {
+  ARMFunctionInfo *AFI = MF.getInfo<ARMFunctionInfo>();
+  if (AFI->isJSStub() || AFI->isJSFunction()) {
+    emitJSPrologue(MF, MBB);
+    return;
+  }
   MachineBasicBlock::iterator MBBI = MBB.begin();
   MachineFrameInfo  &MFI = MF.getFrameInfo();
-  ARMFunctionInfo *AFI = MF.getInfo<ARMFunctionInfo>();
   MachineModuleInfo &MMI = MF.getMMI();
   MCContext &Context = MMI.getContext();
   const TargetMachine &TM = MF.getTarget();
@@ -769,8 +776,12 @@
 
 void ARMFrameLowering::emitEpilogue(MachineFunction &MF,
                                     MachineBasicBlock &MBB) const {
+  ARMFunctionInfo *AFI = MF.getInfo<ARMFunctionInfo>();
+  if (AFI->isJSFunction() || AFI->isJSStub()) {
+    emitJSEpilogue(MF, MBB);
+    return;
+  }
   MachineFrameInfo &MFI = MF.getFrameInfo();
-  ARMFunctionInfo *AFI = MF.getInfo<ARMFunctionInfo>();
   const TargetRegisterInfo *RegInfo = MF.getSubtarget().getRegisterInfo();
   const ARMBaseInstrInfo &TII =
       *static_cast<const ARMBaseInstrInfo *>(MF.getSubtarget().getInstrInfo());
@@ -910,6 +921,9 @@
     if (isFixed) {
       FrameReg = RegInfo->getFrameRegister(MF);
       Offset = FPOffset;
+    } else if (hasMovingSP &&
+               MF.getFunction().getCallingConv() == CallingConv::V8CC) {
+      return Offset;
     } else if (hasMovingSP) {
       assert(RegInfo->hasBasePointer(MF) &&
              "VLAs and dynamic stack alignment, but missing base pointer!");
@@ -955,6 +969,9 @@
       // Otherwise, use SP or FP, whichever is closer to the stack slot.
       FrameReg = RegInfo->getFrameRegister(MF);
       return FPOffset;
+    } else if (AFI->isJSStub() || AFI->isJSFunction()) {
+      FrameReg = RegInfo->getFrameRegister(MF);
+      return FPOffset;
     }
   }
   // Use the base pointer if we have one.
@@ -1063,7 +1080,9 @@
   if (MBB.end() != MI) {
     DL = MI->getDebugLoc();
     unsigned RetOpcode = MI->getOpcode();
-    isTailCall = (RetOpcode == ARM::TCRETURNdi || RetOpcode == ARM::TCRETURNri);
+    isTailCall =
+        (RetOpcode == ARM::TCRETURNdi || RetOpcode == ARM::TCRETURNri ||
+         RetOpcode == TargetOpcode::TCPATCHPOINT);
     isInterrupt =
         RetOpcode == ARM::SUBS_PC_LR || RetOpcode == ARM::t2SUBS_PC_LR;
     isTrap =
@@ -1422,6 +1441,47 @@
   std::prev(MI)->addRegisterKilled(ARM::R4, TRI);
 }
 
+static inline bool isV8Area1Register(unsigned Reg, bool isIOS) {
+  using namespace ARM;
+
+  switch (Reg) {
+  case R0:
+  case R1:
+  case R7:
+  case R11:
+  case LR:
+  case SP:
+  case PC:
+    return true;
+  default:
+    return false;
+  }
+}
+
+static inline bool isV8Area2Register(unsigned Reg, bool isIOS) {
+  using namespace ARM;
+  switch (Reg) {
+  case R3:
+  case R12:
+    return true;
+  default:
+    return false;
+  }
+}
+
+static inline bool isV8Area3Register(unsigned Reg, bool isIOS) {
+  using namespace ARM;
+  switch (Reg) {
+  case R5:
+  case R6:
+  case R8:
+  case R9:
+    return true;
+  default:
+    return false;
+  }
+}
+
 bool ARMFrameLowering::spillCalleeSavedRegisters(MachineBasicBlock &MBB,
                                         MachineBasicBlock::iterator MI,
                                         const std::vector<CalleeSavedInfo> &CSI,
@@ -1437,10 +1497,19 @@
     ARM::t2STR_PRE : ARM::STR_PRE_IMM;
   unsigned FltOpc = ARM::VSTMDDB_UPD;
   unsigned NumAlignedDPRCS2Regs = AFI->getNumAlignedDPRCS2Regs();
-  emitPushInst(MBB, MI, CSI, PushOpc, PushOneOpc, false, &isARMArea1Register, 0,
-               MachineInstr::FrameSetup);
-  emitPushInst(MBB, MI, CSI, PushOpc, PushOneOpc, false, &isARMArea2Register, 0,
-               MachineInstr::FrameSetup);
+  if (AFI->isJSStub() || AFI->isJSFunction()) {
+    emitPushInst(MBB, MI, CSI, PushOpc, PushOneOpc, false, &isV8Area1Register,
+                 0, MachineInstr::FrameSetup);
+    emitPushInst(MBB, MI, CSI, PushOpc, PushOneOpc, false, &isV8Area2Register,
+                 0, MachineInstr::FrameSetup);
+    emitPushInst(MBB, MI, CSI, PushOpc, PushOneOpc, false, &isV8Area3Register,
+                 0, MachineInstr::FrameSetup);
+  } else {
+    emitPushInst(MBB, MI, CSI, PushOpc, PushOneOpc, false, &isARMArea1Register,
+                 0, MachineInstr::FrameSetup);
+    emitPushInst(MBB, MI, CSI, PushOpc, PushOneOpc, false, &isARMArea2Register,
+                 0, MachineInstr::FrameSetup);
+  }
   emitPushInst(MBB, MI, CSI, FltOpc, 0, true, &isARMArea3Register,
                NumAlignedDPRCS2Regs, MachineInstr::FrameSetup);
 
@@ -1469,16 +1538,23 @@
   // registers. Do that here instead.
   if (NumAlignedDPRCS2Regs)
     emitAlignedDPRCS2Restores(MBB, MI, NumAlignedDPRCS2Regs, CSI, TRI);
-
   unsigned PopOpc = AFI->isThumbFunction() ? ARM::t2LDMIA_UPD : ARM::LDMIA_UPD;
   unsigned LdrOpc = AFI->isThumbFunction() ? ARM::t2LDR_POST :ARM::LDR_POST_IMM;
   unsigned FltOpc = ARM::VLDMDIA_UPD;
   emitPopInst(MBB, MI, CSI, FltOpc, 0, isVarArg, true, &isARMArea3Register,
               NumAlignedDPRCS2Regs);
-  emitPopInst(MBB, MI, CSI, PopOpc, LdrOpc, isVarArg, false,
-              &isARMArea2Register, 0);
-  emitPopInst(MBB, MI, CSI, PopOpc, LdrOpc, isVarArg, false,
-              &isARMArea1Register, 0);
+  if (AFI->isJSStub() || AFI->isJSFunction()) {
+    emitPopInst(MBB, MI, CSI, PopOpc, LdrOpc, isVarArg, false,
+                &isV8Area3Register, 0);
+    // Just need one more pop as place holder.
+    emitPopInst(MBB, MI, CSI, PopOpc, LdrOpc, isVarArg, false,
+                &isV8Area1Register, 0);
+  } else {
+    emitPopInst(MBB, MI, CSI, PopOpc, LdrOpc, isVarArg, false,
+                &isARMArea2Register, 0);
+    emitPopInst(MBB, MI, CSI, PopOpc, LdrOpc, isVarArg, false,
+                &isARMArea1Register, 0);
+  }
 
   return true;
 }
@@ -1623,6 +1699,7 @@
   MachineRegisterInfo &MRI = MF.getRegInfo();
   const TargetRegisterInfo *TRI = MF.getSubtarget().getRegisterInfo();
   (void)TRI;  // Silence unused warning in non-assert builds.
+
   unsigned FramePtr = RegInfo->getFrameRegister(MF);
 
   // Spill R4 if Thumb2 function requires stack realignment - it will be used as
@@ -1736,6 +1813,21 @@
     }
   }
 
+  if (!CanEliminateFrame) {
+    if (AFI->isJSFunction() || AFI->isJSStub()) {
+      SavedRegs.set(ARM::R11);
+      SavedRegs.set(ARM::LR);
+    }
+
+    if (AFI->isJSFunction()) {
+      SavedRegs.set(ARM::R0);
+      SavedRegs.set(ARM::R1);
+      SavedRegs.set(ARM::R7);
+    } else if (AFI->isWASM()) {
+      SavedRegs.set(ARM::R3);
+    }
+  }
+
   bool ForceLRSpill = false;
   if (!LRSpilled && AFI->isThumb1OnlyFunction()) {
     unsigned FnSize = EstimateFunctionSizeInBytes(MF, TII);
@@ -1841,7 +1933,10 @@
     AFI->setHasStackFrame(true);
 
     if (HasFP) {
-      SavedRegs.set(FramePtr);
+      if (!SavedRegs.test(FramePtr)) {
+        SavedRegs.set(FramePtr);
+        NumGPRSpills++;
+      }
       // If the frame pointer is required by the ABI, also spill LR so that we
       // emit a complete frame record.
       if (MF.getTarget().Options.DisableFramePointerElim(MF) && !LRSpilled) {
@@ -1855,7 +1950,6 @@
       auto FPPos = llvm::find(UnspilledCS1GPRs, FramePtr);
       if (FPPos != UnspilledCS1GPRs.end())
         UnspilledCS1GPRs.erase(FPPos);
-      NumGPRSpills++;
       if (FramePtr == ARM::R7)
         CS1Spilled = true;
     }
@@ -2112,12 +2206,13 @@
     DebugLoc dl = Old.getDebugLoc();
     unsigned Amount = TII.getFrameSize(Old);
     if (Amount != 0) {
+      ARMFunctionInfo *AFI = MF.getInfo<ARMFunctionInfo>();
       // We need to keep the stack aligned properly.  To do this, we round the
       // amount of space needed for the outgoing arguments up to the next
       // alignment boundary.
-      Amount = alignSPAdjust(Amount);
+      if (!AFI->isJSFunction() && !AFI->isJSStub())
+        Amount = alignSPAdjust(Amount);
 
-      ARMFunctionInfo *AFI = MF.getInfo<ARMFunctionInfo>();
       assert(!AFI->isThumb1OnlyFunction() &&
              "This eliminateCallFramePseudoInstr does not support Thumb1!");
       bool isARM = !AFI->isThumbFunction();
@@ -2132,7 +2227,7 @@
       if (Opc == ARM::ADJCALLSTACKDOWN || Opc == ARM::tADJCALLSTACKDOWN) {
         emitSPUpdate(isARM, MBB, I, dl, TII, -Amount, MachineInstr::NoFlags,
                      Pred, PredReg);
-      } else {
+      } else if (!AFI->isJSFunction() && !AFI->isJSStub()) {
         assert(Opc == ARM::ADJCALLSTACKUP || Opc == ARM::tADJCALLSTACKUP);
         emitSPUpdate(isARM, MBB, I, dl, TII, Amount, MachineInstr::NoFlags,
                      Pred, PredReg);
@@ -2580,3 +2675,470 @@
   MF.verify();
 #endif
 }
+
+bool ARMFrameLowering::assignCalleeSavedSpillSlots(
+    MachineFunction &MF, const TargetRegisterInfo *TRI,
+    std::vector<CalleeSavedInfo> &CSI) const {
+  ARMFunctionInfo *AFI = MF.getInfo<ARMFunctionInfo>();
+  // default handle.
+  if (!AFI->isJSStub() && !AFI->isJSFunction())
+    return false;
+  MachineFrameInfo &MFI = MF.getFrameInfo();
+  // No need to do anything.
+  if (CSI.empty())
+    return true;
+  int FixedOffset;
+  if (AFI->isJSStub()) {
+    MFI.CreateFixedSpillStackObject(4, -12);
+    FixedOffset = -12;
+    if (AFI->isWASM()) {
+      // wasm instance for v8.
+      CSI.emplace_back(ARM::R3);
+      CSI.back().setFrameIdx(MFI.CreateFixedSpillStackObject(4, -16));
+      FixedOffset = -16;
+    }
+  } else {
+    int CPFI = MFI.CreateFixedSpillStackObject(4, -12);  // cp
+    int FunFI = MFI.CreateFixedSpillStackObject(4, -16); // function
+    int ArgFI = MFI.CreateFixedSpillStackObject(4, -20); // arg count.
+    CSI.emplace_back(ARM::R7);
+    CSI.back().setFrameIdx(CPFI);
+    CSI.emplace_back(ARM::R1);
+    CSI.back().setFrameIdx(FunFI);
+    CSI.emplace_back(ARM::R0);
+    CSI.back().setFrameIdx(ArgFI);
+    FixedOffset = -20;
+  }
+
+  for (auto &i : CSI) {
+    if (i.getReg() == ARM::LR) {
+      i.setFrameIdx(MFI.CreateFixedSpillStackObject(4, -4));
+    } else if (i.getReg() == ARM::R11) {
+      i.setFrameIdx(MFI.CreateFixedSpillStackObject(4, -8));
+    } else if (i.getReg() >= ARM::R5 && i.getReg() < ARM::R9) {
+      FixedOffset -= 4;
+      i.setFrameIdx(MFI.CreateFixedSpillStackObject(4, FixedOffset));
+    }
+  }
+  
+  unsigned Align = STI.getFrameLowering()->getStackAlignment();
+  FixedOffset &= ~(Align - 1);
+  for (auto &i : CSI) {
+    if (i.getReg() >= ARM::D0 && i.getReg() <= ARM::D31) {
+      FixedOffset -= 8;
+      i.setFrameIdx(MFI.CreateFixedSpillStackObject(8, FixedOffset));
+    }
+  }
+  return true;
+}
+
+void ARMFrameLowering::emitJSPrologue(MachineFunction &MF,
+                                      MachineBasicBlock &MBB) const {
+  ARMFunctionInfo *AFI = MF.getInfo<ARMFunctionInfo>();
+  MachineBasicBlock::iterator MBBI = MBB.begin();
+  MachineFrameInfo &MFI = MF.getFrameInfo();
+  MachineModuleInfo &MMI = MF.getMMI();
+  MCContext &Context = MMI.getContext();
+  const MCRegisterInfo *MRI = Context.getRegisterInfo();
+  const ARMBaseInstrInfo &TII = *STI.getInstrInfo();
+  bool isARM = !AFI->isThumbFunction();
+  unsigned Align = STI.getFrameLowering()->getStackAlignment();
+  unsigned NumBytes = MFI.getStackSize();
+  const std::vector<CalleeSavedInfo> &CSI = MFI.getCalleeSavedInfo();
+  const ARMBaseRegisterInfo *RegInfo = STI.getRegisterInfo();
+
+  // Debug location must be unknown since the first debug location is used
+  // to determine the end of the prologue.
+  DebugLoc dl;
+
+  unsigned FramePtr = ARM::R11;
+  // Determine the sizes of each callee-save spill areas and record which frame
+  // belongs to which callee-save spill areas.
+  unsigned GPRCS1Size = 0, GPRCS2Size = 0, GPRCS3Size = 0, DPRCSSize = 0;
+  int FramePtrSpillFI = 0;
+  int D8SpillFI = 0;
+
+  if (AFI->isJSStub()) {
+    if (MF.getRegInfo().isLiveIn(ARM::R9)) {
+      MachineBasicBlock &MBB = MF.front();
+      BuildMI(MBB, MBB.instr_begin(), dl, TII.get(ARM::MOVr), ARM::R9)
+          .addReg(ARM::R11)
+          .add(predOps(ARMCC::AL))
+          .add(condCodeOp());
+    }
+    // Push LR as marker.
+    GPRCS1Size = 4;
+  }
+
+  StackAdjustingInsts DefCFAOffsetCandidates;
+  bool HasFP = hasFP(MF);
+
+  if (!AFI->hasStackFrame()) {
+    if (NumBytes != 0) {
+      emitSPUpdate(isARM, MBB, MBBI, dl, TII, -NumBytes,
+                   MachineInstr::FrameSetup);
+      DefCFAOffsetCandidates.addInst(std::prev(MBBI), NumBytes, true);
+    }
+    DefCFAOffsetCandidates.emitDefCFAOffsets(MBB, dl, TII, HasFP);
+    return;
+  }
+
+  // Determine spill area sizes.
+  for (unsigned i = 0, e = CSI.size(); i != e; ++i) {
+    unsigned Reg = CSI[i].getReg();
+    int FI = CSI[i].getFrameIdx();
+    switch (Reg) {
+    case ARM::R3:
+      GPRCS2Size += 4;
+      break;
+    case ARM::R0:
+    case ARM::R1:
+    case ARM::R7:
+    case ARM::R11:
+    case ARM::LR:
+      if (Reg == ARM::R11)
+        FramePtrSpillFI = FI;
+      GPRCS1Size += 4;
+      break;
+    case ARM::R5:
+    case ARM::R6:
+    case ARM::R8:
+    case ARM::R9:
+      GPRCS3Size += 4;
+      break;
+    default:
+      // This is a DPR. Exclude the aligned DPRCS2 spills.
+      if (Reg == ARM::D8)
+        D8SpillFI = FI;
+      if ((Reg <= ARM::D31) && (Reg >= ARM::D0))
+        DPRCSSize += 8;
+    }
+  }
+
+  // Move past area 1.
+  MachineBasicBlock::iterator LastPush = MBB.end(), GPRCS1Push, GPRCS2Push,
+                              GPRCS3Push;
+  if (GPRCS1Size > 0) {
+    GPRCS1Push = LastPush = MBBI++;
+    DefCFAOffsetCandidates.addInst(LastPush, GPRCS1Size, true);
+  }
+
+  // Determine starting offsets of spill areas.
+  unsigned GPRCS1Offset = NumBytes - GPRCS1Size;
+  unsigned GPRCS2Offset = GPRCS1Offset - GPRCS2Size;
+  unsigned GPRCS3Offset = GPRCS2Offset - GPRCS3Size;
+  unsigned DPRAlign = DPRCSSize ? std::min(8U, Align) : 4U;
+  unsigned DPRGapSize = (GPRCS1Size + GPRCS2Size + GPRCS3Size) % DPRAlign;
+  unsigned DPRCSOffset = GPRCS3Offset - DPRGapSize - DPRCSSize;
+  int FramePtrOffsetInPush = 0;
+  if (FramePtrSpillFI) {
+    int FPOffset = MFI.getObjectOffset(FramePtrSpillFI);
+    assert(getMaxFPOffset(MF.getFunction(), *AFI) <= FPOffset &&
+           "Max FP estimation is wrong");
+    FramePtrOffsetInPush = FPOffset;
+    AFI->setFramePtrSpillOffset(MFI.getObjectOffset(FramePtrSpillFI) +
+                                NumBytes);
+  }
+  AFI->setGPRCalleeSavedArea1Offset(GPRCS1Offset);
+  AFI->setGPRCalleeSavedArea2Offset(GPRCS2Offset);
+  AFI->setDPRCalleeSavedAreaOffset(DPRCSOffset);
+
+  // Move past area 2.
+  if (GPRCS2Size > 0) {
+    GPRCS2Push = LastPush = MBBI++;
+    DefCFAOffsetCandidates.addInst(LastPush, GPRCS2Size);
+  }
+
+  if (GPRCS3Size > 0) {
+    GPRCS3Push = LastPush = MBBI++;
+    DefCFAOffsetCandidates.addInst(LastPush, GPRCS3Size);
+  }
+
+  // Prolog/epilog inserter assumes we correctly align DPRs on the stack, so our
+  // .cfi_offset operations will reflect that.
+  if (DPRGapSize) {
+    assert(DPRGapSize == 4 && "unexpected alignment requirements for DPRs");
+    emitSPUpdate(isARM, MBB, MBBI, dl, TII, -DPRGapSize,
+                 MachineInstr::FrameSetup);
+    DefCFAOffsetCandidates.addInst(std::prev(MBBI), DPRGapSize);
+  }
+
+  // Move past area 3.
+  if (DPRCSSize > 0) {
+    // Since vpush register list cannot have gaps, there may be multiple vpush
+    // instructions in the prologue.
+    while (MBBI != MBB.end() && MBBI->getOpcode() == ARM::VSTMDDB_UPD) {
+      DefCFAOffsetCandidates.addInst(MBBI, sizeOfSPAdjustment(*MBBI));
+      LastPush = MBBI++;
+    }
+  }
+
+  // Move past the aligned DPRCS2 area.
+  if (AFI->getNumAlignedDPRCS2Regs() > 0) {
+    MBBI = skipAlignedDPRCS2Spills(MBBI, AFI->getNumAlignedDPRCS2Regs());
+    // The code inserted by emitAlignedDPRCS2Spills realigns the stack, and
+    // leaves the stack pointer pointing to the DPRCS2 area.
+    //
+    // Adjust NumBytes to represent the stack slots below the DPRCS2 area.
+    NumBytes += MFI.getObjectOffset(D8SpillFI);
+  } else
+    NumBytes = DPRCSOffset;
+
+  if (NumBytes) {
+    emitSPUpdate(isARM, MBB, MBBI, dl, TII, -NumBytes,
+                 MachineInstr::FrameSetup);
+    DefCFAOffsetCandidates.addInst(std::prev(MBBI), NumBytes);
+
+    if (HasFP && isARM)
+      // Restore from fp only in ARM mode: e.g. sub sp, r7, #24
+      // Note it's not safe to do this in Thumb2 mode because it would have
+      // taken two instructions:
+      // mov sp, r7
+      // sub sp, #24
+      // If an interrupt is taken between the two instructions, then sp is in
+      // an inconsistent state (pointing to the middle of callee-saved area).
+      // The interrupt handler can end up clobbering the registers.
+      AFI->setShouldRestoreSPFromFP(true);
+  }
+
+  // Set FP to point to the stack slot that contains the previous FP.
+  // For iOS, FP is R7, which has now been stored in spill area 1.
+  // Otherwise, if this is not iOS, all the callee-saved registers go
+  // into spill area 1, including the FP in R11.  In either case, it
+  // is in area one and the adjustment needs to take place just after
+  // that push.
+  MachineBasicBlock::iterator AfterPush = std::next(GPRCS1Push);
+  unsigned PushSize = sizeOfSPAdjustment(*GPRCS1Push);
+  emitRegPlusImmediate(!AFI->isThumbFunction(), MBB, AfterPush, dl, TII,
+                       FramePtr, ARM::SP, PushSize + FramePtrOffsetInPush,
+                       MachineInstr::FrameSetup);
+  if (FramePtrOffsetInPush + PushSize != 0) {
+    unsigned CFIIndex = MF.addFrameInst(MCCFIInstruction::createDefCfa(
+        nullptr, MRI->getDwarfRegNum(FramePtr, true), FramePtrOffsetInPush));
+    BuildMI(MBB, AfterPush, dl, TII.get(TargetOpcode::CFI_INSTRUCTION))
+        .addCFIIndex(CFIIndex)
+        .setMIFlags(MachineInstr::FrameSetup);
+  } else {
+    unsigned CFIIndex = MF.addFrameInst(MCCFIInstruction::createDefCfaRegister(
+        nullptr, MRI->getDwarfRegNum(FramePtr, true)));
+    BuildMI(MBB, AfterPush, dl, TII.get(TargetOpcode::CFI_INSTRUCTION))
+        .addCFIIndex(CFIIndex)
+        .setMIFlags(MachineInstr::FrameSetup);
+  }
+  // Push marker
+  if (AFI->isJSStub()) {
+    int Marker;
+    MF.getFunction()
+        .getFnAttribute("js-stub-call")
+        .getValueAsString()
+        .getAsInteger(10, Marker);
+    BuildMI(MBB, AfterPush, dl, TII.get(ARM::MOVi), ARM::LR)
+        .addImm(Marker)
+        .add(predOps(ARMCC::AL))
+        .add(condCodeOp());
+    BuildMI(MBB, AfterPush, dl, TII.get(ARM::STR_PRE_IMM), ARM::SP)
+        .addReg(ARM::LR, RegState::Kill)
+        .addReg(ARM::SP)
+        .setMIFlags(MachineInstr::NoFlags)
+        .addImm(-4)
+        .add(predOps(ARMCC::AL));
+  }
+
+  // Now that the prologue's actual instructions are finalised, we can insert
+  // the necessary DWARF cf instructions to describe the situation. Start by
+  // recording where each register ended up:
+  if (GPRCS1Size > 0) {
+    MachineBasicBlock::iterator Pos = std::next(GPRCS1Push);
+    int CFIIndex;
+    for (const auto &Entry : CSI) {
+      unsigned Reg = Entry.getReg();
+      int FI = Entry.getFrameIdx();
+      switch (Reg) {
+      case ARM::R0:
+      case ARM::R1:
+      case ARM::R7:
+      case ARM::LR:
+        CFIIndex = MF.addFrameInst(MCCFIInstruction::createOffset(
+            nullptr, MRI->getDwarfRegNum(Reg, true), MFI.getObjectOffset(FI)));
+        BuildMI(MBB, Pos, dl, TII.get(TargetOpcode::CFI_INSTRUCTION))
+            .addCFIIndex(CFIIndex)
+            .setMIFlags(MachineInstr::FrameSetup);
+        break;
+      }
+    }
+  }
+
+  if (GPRCS2Size > 0) {
+    MachineBasicBlock::iterator Pos = std::next(GPRCS2Push);
+    for (const auto &Entry : CSI) {
+      unsigned Reg = Entry.getReg();
+      int FI = Entry.getFrameIdx();
+      switch (Reg) {
+      case ARM::R3:
+        unsigned DwarfReg = MRI->getDwarfRegNum(Reg, true);
+        unsigned Offset = MFI.getObjectOffset(FI);
+        unsigned CFIIndex = MF.addFrameInst(
+            MCCFIInstruction::createOffset(nullptr, DwarfReg, Offset));
+        BuildMI(MBB, Pos, dl, TII.get(TargetOpcode::CFI_INSTRUCTION))
+            .addCFIIndex(CFIIndex)
+            .setMIFlags(MachineInstr::FrameSetup);
+        break;
+      }
+    }
+  }
+
+  if (GPRCS3Size > 0) {
+    MachineBasicBlock::iterator Pos = std::next(GPRCS3Push);
+    for (const auto &Entry : CSI) {
+      unsigned Reg = Entry.getReg();
+      int FI = Entry.getFrameIdx();
+      switch (Reg) {
+      case ARM::R5:
+      case ARM::R6:
+      case ARM::R8:
+      case ARM::R9:
+        unsigned DwarfReg = MRI->getDwarfRegNum(Reg, true);
+        unsigned Offset = MFI.getObjectOffset(FI);
+        unsigned CFIIndex = MF.addFrameInst(
+            MCCFIInstruction::createOffset(nullptr, DwarfReg, Offset));
+        BuildMI(MBB, Pos, dl, TII.get(TargetOpcode::CFI_INSTRUCTION))
+            .addCFIIndex(CFIIndex)
+            .setMIFlags(MachineInstr::FrameSetup);
+        break;
+      }
+    }
+  }
+
+  if (DPRCSSize > 0) {
+    // Since vpush register list cannot have gaps, there may be multiple vpush
+    // instructions in the prologue.
+    MachineBasicBlock::iterator Pos = std::next(LastPush);
+    for (const auto &Entry : CSI) {
+      unsigned Reg = Entry.getReg();
+      int FI = Entry.getFrameIdx();
+      if ((Reg >= ARM::D0 && Reg <= ARM::D31) &&
+          (Reg < ARM::D8 || Reg >= ARM::D8 + AFI->getNumAlignedDPRCS2Regs())) {
+        unsigned DwarfReg = MRI->getDwarfRegNum(Reg, true);
+        unsigned Offset = MFI.getObjectOffset(FI);
+        unsigned CFIIndex = MF.addFrameInst(
+            MCCFIInstruction::createOffset(nullptr, DwarfReg, Offset));
+        BuildMI(MBB, Pos, dl, TII.get(TargetOpcode::CFI_INSTRUCTION))
+            .addCFIIndex(CFIIndex)
+            .setMIFlags(MachineInstr::FrameSetup);
+      }
+    }
+  }
+
+  // Now we can emit descriptions of where the canonical frame address was
+  // throughout the process. If we have a frame pointer, it takes over the job
+  // half-way through, so only the first few .cfi_def_cfa_offset instructions
+  // actually get emitted.
+  DefCFAOffsetCandidates.emitDefCFAOffsets(MBB, dl, TII, HasFP);
+
+  if (STI.isTargetELF() && hasFP(MF))
+    MFI.setOffsetAdjustment(MFI.getOffsetAdjustment() -
+                            AFI->getFramePtrSpillOffset());
+
+  AFI->setGPRCalleeSavedArea1Size(GPRCS1Size);
+  AFI->setGPRCalleeSavedArea2Size(GPRCS2Size);
+  AFI->setGPRCalleeSavedArea3Size(GPRCS3Size);
+  AFI->setDPRCalleeSavedGapSize(DPRGapSize);
+  AFI->setDPRCalleeSavedAreaSize(DPRCSSize);
+
+  // If we need dynamic stack realignment, do it here. Be paranoid and make
+  // sure if we also have VLAs, we have a base pointer for frame access.
+  // If aligned NEON registers were spilled, the stack has already been
+  // realigned.
+  if (RegInfo->needsStackRealignment(MF)) {
+    unsigned MaxAlign = MFI.getMaxAlignment();
+    assert(!AFI->isThumbFunction());
+    emitAligningInstructions(MF, AFI, TII, MBB, MBBI, dl, ARM::SP, MaxAlign,
+                             false);
+    AFI->setShouldRestoreSPFromFP(true);
+  }
+
+  assert(!MFI.hasVarSizedObjects());
+}
+
+void ARMFrameLowering::emitJSEpilogue(MachineFunction &MF,
+                                      MachineBasicBlock &MBB) const {
+  ARMFunctionInfo *AFI = MF.getInfo<ARMFunctionInfo>();
+  MachineFrameInfo &MFI = MF.getFrameInfo();
+  const ARMBaseInstrInfo &TII =
+      *static_cast<const ARMBaseInstrInfo *>(MF.getSubtarget().getInstrInfo());
+  assert(!AFI->isThumb1OnlyFunction() &&
+         "This emitEpilogue does not support Thumb1!");
+  bool isARM = !AFI->isThumbFunction();
+  assert(isARM);
+
+  int NumBytes = (int)MFI.getStackSize();
+  unsigned FramePtr = ARM::R11;
+
+  // First put ourselves on the first (from top) terminator instructions.
+  MachineBasicBlock::iterator MBBI = MBB.getFirstTerminator();
+  DebugLoc dl = MBBI != MBB.end() ? MBBI->getDebugLoc() : DebugLoc();
+
+  if (!AFI->hasStackFrame()) {
+    if (NumBytes != 0)
+      emitSPUpdate(isARM, MBB, MBBI, dl, TII, NumBytes);
+  } else {
+    // Unwind MBBI to point to first LDR / VLDRD.
+    if (MBBI != MBB.begin()) {
+      do {
+        --MBBI;
+      } while (MBBI != MBB.begin() && isPopOpcode(MBBI->getOpcode()));
+      if (!isPopOpcode(MBBI->getOpcode()))
+        ++MBBI;
+    }
+
+    // Move SP to start of FP callee save spill area.
+    NumBytes -=
+        (AFI->getGPRCalleeSavedArea1Size() + AFI->getGPRCalleeSavedArea2Size() +
+         AFI->getGPRCalleeSavedArea3Size() + AFI->getDPRCalleeSavedGapSize() +
+         AFI->getDPRCalleeSavedAreaSize());
+
+    if (AFI->getDPRCalleeSavedAreaSize() + AFI->getGPRCalleeSavedArea3Size()) {
+      // Reset SP based on frame pointer only if the stack frame extends beyond
+      // frame pointer stack slot or target is ELF and the function has FP.
+      if (AFI->shouldRestoreSPFromFP()) {
+        NumBytes = AFI->getFramePtrSpillOffset() - NumBytes;
+        if (NumBytes) {
+          emitARMRegPlusImmediate(MBB, MBBI, dl, ARM::SP, FramePtr, -NumBytes,
+                                  ARMCC::AL, 0, TII);
+        } else {
+          // Thumb2 or ARM.
+          BuildMI(MBB, MBBI, dl, TII.get(ARM::MOVr), ARM::SP)
+              .addReg(FramePtr)
+              .add(predOps(ARMCC::AL))
+              .add(condCodeOp());
+        }
+      } else if (NumBytes)
+        emitSPUpdate(isARM, MBB, MBBI, dl, TII, NumBytes);
+    }
+
+    // Increment past our save areas.
+    if (MBBI != MBB.end() && AFI->getDPRCalleeSavedAreaSize()) {
+      MBBI++;
+      // Since vpop register list cannot have gaps, there may be multiple vpop
+      // instructions in the epilogue.
+      while (MBBI != MBB.end() && MBBI->getOpcode() == ARM::VLDMDIA_UPD)
+        MBBI++;
+    }
+    // Skip one more MI when area 3 exists.
+    if (AFI->getGPRCalleeSavedArea3Size()) {
+      MBBI++;
+    }
+
+    BuildMI(MBB, MBBI, dl, TII.get(ARM::MOVr), ARM::SP)
+        .addReg(FramePtr)
+        .add(predOps(ARMCC::AL))
+        .add(condCodeOp());
+    MachineInstrBuilder MIB =
+        BuildMI(MBB, MBBI, dl, TII.get(ARM::LDMIA_UPD), ARM::SP)
+            .addReg(ARM::SP)
+            .add(predOps(ARMCC::AL));
+    MIB.addReg(ARM::R11, getDefRegState(true));
+    MIB.addReg(ARM::LR, getDefRegState(true));
+    MBBI->eraseFromParent();
+  }
+}
Index: lib/Target/ARM/ARMFrameLowering.h
===================================================================
--- lib/Target/ARM/ARMFrameLowering.h	(revision 375507)
+++ lib/Target/ARM/ARMFrameLowering.h	(working copy)
@@ -30,6 +30,8 @@
   /// the function.
   void emitPrologue(MachineFunction &MF, MachineBasicBlock &MBB) const override;
   void emitEpilogue(MachineFunction &MF, MachineBasicBlock &MBB) const override;
+  void emitJSPrologue(MachineFunction &MF, MachineBasicBlock &MBB) const;
+  void emitJSEpilogue(MachineFunction &MF, MachineBasicBlock &MBB) const;
 
   bool spillCalleeSavedRegisters(MachineBasicBlock &MBB,
                                  MachineBasicBlock::iterator MI,
@@ -64,6 +66,11 @@
     return true;
   }
 
+  bool
+  assignCalleeSavedSpillSlots(MachineFunction &MF,
+                              const TargetRegisterInfo *TRI,
+                              std::vector<CalleeSavedInfo> &CSI) const override;
+
 private:
   void emitPushInst(MachineBasicBlock &MBB, MachineBasicBlock::iterator MI,
                     const std::vector<CalleeSavedInfo> &CSI, unsigned StmOpc,
Index: lib/Target/ARM/ARMISelDAGToDAG.cpp
===================================================================
--- lib/Target/ARM/ARMISelDAGToDAG.cpp	(revision 375507)
+++ lib/Target/ARM/ARMISelDAGToDAG.cpp	(working copy)
@@ -12,6 +12,7 @@
 
 #include "ARM.h"
 #include "ARMBaseInstrInfo.h"
+#include "ARMMachineFunctionInfo.h"
 #include "ARMTargetMachine.h"
 #include "MCTargetDesc/ARMAddressingModes.h"
 #include "Utils/ARMBaseInfo.h"
@@ -59,6 +60,15 @@
       : SelectionDAGISel(tm, OptLevel) {}
 
   bool runOnMachineFunction(MachineFunction &MF) override {
+    const Function &F = MF.getFunction();
+    ARMFunctionInfo *AFI = MF.getInfo<ARMFunctionInfo>();
+    if (F.hasFnAttribute("js-function-call"))
+      AFI->setJSFunction(true);
+    else if (F.hasFnAttribute("js-stub-call"))
+      AFI->setJSStub(true);
+    if (F.hasFnAttribute("js-wasm-call"))
+      AFI->setWASM(true);
+
     // Reset the subtarget each time through.
     Subtarget = &MF.getSubtarget<ARMSubtarget>();
     SelectionDAGISel::runOnMachineFunction(MF);
Index: lib/Target/ARM/ARMISelLowering.cpp
===================================================================
--- lib/Target/ARM/ARMISelLowering.cpp	(revision 375507)
+++ lib/Target/ARM/ARMISelLowering.cpp	(working copy)
@@ -1566,6 +1566,7 @@
   case ARMISD::VBICIMM:       return "ARMISD::VBICIMM";
   case ARMISD::VBSL:          return "ARMISD::VBSL";
   case ARMISD::MEMCPY:        return "ARMISD::MEMCPY";
+  case ARMISD::RESTORESP:     return "ARMISD::RESTORESP";
   case ARMISD::VLD1DUP:       return "ARMISD::VLD1DUP";
   case ARMISD::VLD2DUP:       return "ARMISD::VLD2DUP";
   case ARMISD::VLD3DUP:       return "ARMISD::VLD3DUP";
@@ -1781,6 +1782,10 @@
   switch (CC) {
   default:
     report_fatal_error("Unsupported calling convention");
+  case CallingConv::V8CC:
+    return CallingConv::V8CC;
+  case CallingConv::V8SBCC:
+    return CallingConv::V8SBCC;
   case CallingConv::ARM_AAPCS:
   case CallingConv::ARM_APCS:
   case CallingConv::GHC:
@@ -1831,6 +1836,10 @@
   switch (getEffectiveCallingConv(CC, isVarArg)) {
   default:
     report_fatal_error("Unsupported calling convention");
+  case CallingConv::V8SBCC:
+    return (Return ? RetCC_ARM_V8SB : CC_ARM_V8SB);
+  case CallingConv::V8CC:
+    return (Return ? RetCC_ARM_V8 : CC_ARM_V8);
   case CallingConv::ARM_APCS:
     return (Return ? RetCC_ARM_APCS : CC_ARM_APCS);
   case CallingConv::ARM_AAPCS:
@@ -1990,6 +1999,10 @@
   MachineFunction &MF = DAG.getMachineFunction();
   bool isStructRet = (Outs.empty()) ? false : Outs[0].Flags.isSRet();
   bool isThisReturn = false;
+  bool hasJSCCall     = false;
+  CallingConv::ID CallerCC = MF.getFunction().getCallingConv();
+  bool CallerIsJS =
+      ((CallerCC == CallingConv::V8CC) || (CallerCC == CallingConv::V8SBCC));
   auto Attr = MF.getFunction().getFnAttribute("disable-tail-calls");
   bool PreferIndirect = false;
 
@@ -2013,10 +2026,10 @@
   }
   if (isTailCall) {
     // Check if it's really possible to do a tail call.
-    isTailCall = IsEligibleForTailCallOptimization(
-        Callee, CallConv, isVarArg, isStructRet,
-        MF.getFunction().hasStructRetAttr(), Outs, OutVals, Ins, DAG,
-        PreferIndirect);
+    isTailCall = CallerIsJS || IsEligibleForTailCallOptimization(
+                                   Callee, CallConv, isVarArg, isStructRet,
+                                   MF.getFunction().hasStructRetAttr(), Outs,
+                                   OutVals, Ins, DAG, PreferIndirect);
     if (!isTailCall && CLI.CS && CLI.CS.isMustTailCall())
       report_fatal_error("failed to perform tail call elimination on a call "
                          "site marked musttail");
@@ -2042,9 +2055,32 @@
     // Adjust the stack pointer for the new arguments...
     // These operations are automatically eliminated by the prolog/epilog pass
     Chain = DAG.getCALLSEQ_START(Chain, NumBytes, 0, dl);
+    if (CLI.CS.isCall()) {
+      if (CallerIsJS && (CLI.CallConv == CallingConv::C) &&
+          !MF.getFunction().hasFnAttribute(Attribute::StackAlignment)) {
+        hasJSCCall = true;
+      }
+    }
   }
 
-  SDValue StackPtr =
+  SDValue StackPtr;
+
+  if (hasJSCCall) {
+    SDValue OldStackPtr = DAG.getCopyFromReg(Chain, dl, ARM::SP,
+                                             getPointerTy(DAG.getDataLayout()));
+    Chain = OldStackPtr.getValue(1);
+    SDValue InFlag;
+    SDValue NewStackPtr = DAG.getNode(
+        ISD::AND, dl, OldStackPtr.getValueType(), OldStackPtr.getValue(0),
+        DAG.getConstant(-8, dl, OldStackPtr.getValueType()));
+    Chain =
+        DAG.getCopyToReg(Chain, dl, ARM::SP, NewStackPtr.getValue(0), InFlag);
+    MachineFunction &MF = DAG.getMachineFunction();
+    MachineFrameInfo &MFI = MF.getFrameInfo();
+    MFI.setFrameAddressIsTaken(true);
+  }
+
+  StackPtr =
       DAG.getCopyFromReg(Chain, dl, ARM::SP, getPointerTy(DAG.getDataLayout()));
 
   RegsToPassVector RegsToPass;
@@ -2352,6 +2388,12 @@
 
   // Returns a chain and a flag for retval copy to use.
   Chain = DAG.getNode(CallOpc, dl, NodeTys, Ops);
+
+  if (hasJSCCall) {
+    Chain = DAG.getNode(ARMISD::RESTORESP, dl,
+                        DAG.getVTList(MVT::Other, MVT::Glue), Chain);
+  }
+
   InFlag = Chain.getValue(1);
 
   Chain = DAG.getCALLSEQ_END(Chain, DAG.getIntPtrConstant(NumBytes, dl, true),
@@ -9631,6 +9673,17 @@
     llvm_unreachable("Unexpected instr type to insert");
   }
 
+  case TargetOpcode::STATEPOINT:
+    // As an implementation detail, STATEPOINT shares the STACKMAP format at
+    // this point in the process.  We diverge later.
+    return emitPatchPoint(MI, BB);
+
+  case TargetOpcode::STACKMAP:
+  case TargetOpcode::PATCHPOINT:
+    return emitPatchPoint(MI, BB);
+  case TargetOpcode::TCPATCHPOINT:
+    return TargetLoweringBase::emitPatchPoint(MI, BB);
+
   // Thumb1 post-indexed loads are really just single-register LDMs.
   case ARM::tLDR_postidx: {
     MachineOperand Def(MI.getOperand(1));
@@ -11380,6 +11433,10 @@
   unsigned Mask = MaskC->getZExtValue();
   if (Mask == 0xffff)
     return SDValue();
+  // Memory Operator can address constant, save one register.
+  auto UI = dyn_cast<MemSDNode>(*(N->use_begin()));
+  if (UI)
+    return SDValue();
   SDValue Res;
   // Case (1): or (and A, mask), val => ARMbfi A, val, mask
   ConstantSDNode *N1C = dyn_cast<ConstantSDNode>(N1);
@@ -13682,6 +13739,23 @@
   return -1;
 }
 
+const MCPhysReg *
+ARMTargetLowering::getScratchRegisters(CallingConv::ID CC) const {
+  if (CC == CallingConv::V8SBCC)
+    return nullptr;
+  static const MCPhysReg ScratchRegs[] = {ARM::R12, 0};
+  return ScratchRegs;
+}
+
+MachineBasicBlock *
+ARMTargetLowering::emitPatchPoint(MachineInstr &MI,
+                                  MachineBasicBlock *MBB) const {
+  MachineBasicBlock *MBB2 = TargetLoweringBase::emitPatchPoint(MI, MBB);
+  MachineFunction &MF = *MI.getMF();
+  MI.addOperand(MF, MachineOperand::CreateReg(ARM::LR, true, true));
+  return MBB2;
+}
+
 static bool isLegalT1AddressImmediate(int64_t V, EVT VT) {
   if (V < 0)
     return false;
Index: lib/Target/ARM/ARMISelLowering.h
===================================================================
--- lib/Target/ARM/ARMISelLowering.h	(revision 375507)
+++ lib/Target/ARM/ARMISelLowering.h	(working copy)
@@ -242,6 +242,8 @@
       // Pseudo-instruction representing a memory copy using ldm/stm
       // instructions.
       MEMCPY,
+      // Pseudo-instruction representing restoring sp from fp.
+      RESTORESP,
 
       // Vector load N-element structure to all lanes:
       VLD1DUP = ISD::FIRST_TARGET_MEMORY_OPCODE,
@@ -516,6 +518,11 @@
     bool functionArgumentNeedsConsecutiveRegisters(
         Type *Ty, CallingConv::ID CallConv, bool isVarArg) const override;
 
+    const MCPhysReg *getScratchRegisters(CallingConv::ID CC) const override;
+
+    MachineBasicBlock *emitPatchPoint(MachineInstr &MI,
+                                      MachineBasicBlock *MBB) const;
+
     /// If a physical register, this returns the register that receives the
     /// exception address on entry to an EH pad.
     unsigned
Index: lib/Target/ARM/ARMInstrInfo.td
===================================================================
--- lib/Target/ARM/ARMInstrInfo.td	(revision 375507)
+++ lib/Target/ARM/ARMInstrInfo.td	(working copy)
@@ -78,6 +78,7 @@
 def SDT_ARMMEMCPY  : SDTypeProfile<2, 3, [SDTCisVT<0, i32>, SDTCisVT<1, i32>,
                                           SDTCisVT<2, i32>, SDTCisVT<3, i32>,
                                           SDTCisVT<4, i32>]>;
+def SDT_ARMRESTORESP: SDTypeProfile<0, 0, []>;
 
 def SDTBinaryArithWithFlags : SDTypeProfile<2, 2,
                                             [SDTCisSameAs<0, 2>,
@@ -222,6 +223,10 @@
                         [SDNPHasChain, SDNPInGlue, SDNPOutGlue,
                          SDNPMayStore, SDNPMayLoad]>;
 
+def ARMrestoresp : SDNode<"ARMISD::RESTORESP", SDT_ARMRESTORESP,
+                        [SDNPHasChain, SDNPOutGlue,
+                         SDNPMayStore, SDNPMayLoad]>;
+
 def ARMsmulwb       : SDNode<"ARMISD::SMULWB", SDTIntBinOp, []>;
 def ARMsmulwt       : SDNode<"ARMISD::SMULWT", SDTIntBinOp, []>;
 def ARMsmlalbb      : SDNode<"ARMISD::SMLALBB", SDT_LongMac, []>;
@@ -4897,6 +4902,14 @@
             (ARMmemcopy GPR:$dst, GPR:$src, imm:$nreg))]>;
 }
 
+let hasPostISelHook = 1, hasNoSchedulingInfo = 1 in {
+    def RESTORESP : PseudoInst<
+      (outs),
+      (ins),
+      NoItinerary,
+      [(ARMrestoresp)]>;
+}
+
 def ldrex_1 : PatFrag<(ops node:$ptr), (int_arm_ldrex node:$ptr), [{
   return cast<MemIntrinsicSDNode>(N)->getMemoryVT() == MVT::i8;
 }]>;
Index: lib/Target/ARM/ARMMachineFunctionInfo.h
===================================================================
--- lib/Target/ARM/ARMMachineFunctionInfo.h	(revision 375507)
+++ lib/Target/ARM/ARMMachineFunctionInfo.h	(working copy)
@@ -88,6 +88,7 @@
   /// areas.
   unsigned GPRCS1Size = 0;
   unsigned GPRCS2Size = 0;
+  unsigned GPRCS3Size = 0;
   unsigned DPRCSAlignGapSize = 0;
   unsigned DPRCSSize = 0;
 
@@ -130,6 +131,11 @@
   /// The amount the literal pool has been increasedby due to promoted globals.
   int PromotedGlobalsIncrease = 0;
 
+  mutable int LastSPAdjust = 0;
+  bool IsJSFunction = false;
+  bool IsJSStub = false;
+  bool IsWASM = false;
+
 public:
   ARMFunctionInfo() = default;
 
@@ -176,11 +182,13 @@
 
   unsigned getGPRCalleeSavedArea1Size() const { return GPRCS1Size; }
   unsigned getGPRCalleeSavedArea2Size() const { return GPRCS2Size; }
+  unsigned getGPRCalleeSavedArea3Size() const { return GPRCS3Size; }
   unsigned getDPRCalleeSavedGapSize() const   { return DPRCSAlignGapSize; }
   unsigned getDPRCalleeSavedAreaSize()  const { return DPRCSSize; }
 
   void setGPRCalleeSavedArea1Size(unsigned s) { GPRCS1Size = s; }
   void setGPRCalleeSavedArea2Size(unsigned s) { GPRCS2Size = s; }
+  void setGPRCalleeSavedArea3Size(unsigned s) { GPRCS3Size = s; }
   void setDPRCalleeSavedGapSize(unsigned s)   { DPRCSAlignGapSize = s; }
   void setDPRCalleeSavedAreaSize(unsigned s)  { DPRCSSize = s; }
 
@@ -246,6 +254,21 @@
     PromotedGlobalsIncrease = Sz;
   }
 
+  bool isJSFunction() const { return IsJSFunction; }
+  bool isJSStub() const { return IsJSStub; }
+  bool isWASM() const { return IsWASM; }
+
+  void setJSFunction(bool s) { IsJSFunction = s; }
+  void setJSStub(bool s) { IsJSStub = s; }
+  void setWASM(bool s) { IsWASM = s; }
+
+  void pushLastSPAdjust(int n) const { LastSPAdjust = n; }
+  int popLastSPAdjust() const {
+    int r = LastSPAdjust;
+    LastSPAdjust = 0;
+    return r;
+  }
+
   DenseMap<unsigned, unsigned> EHPrologueRemappedRegs;
 };
 
Index: lib/Target/ARM/ARMTargetTransformInfo.h
===================================================================
--- lib/Target/ARM/ARMTargetTransformInfo.h	(revision 375507)
+++ lib/Target/ARM/ARMTargetTransformInfo.h	(working copy)
@@ -46,6 +46,7 @@
 
   const ARMSubtarget *ST;
   const ARMTargetLowering *TLI;
+  const Function *Fn;
 
   // Currently the following features are excluded from InlineFeatureWhitelist.
   // ModeThumb, FeatureNoARM, ModeSoftFloat, FeatureFP64, FeatureD32
@@ -86,7 +87,7 @@
 public:
   explicit ARMTTIImpl(const ARMBaseTargetMachine *TM, const Function &F)
       : BaseT(TM, F.getParent()->getDataLayout()), ST(TM->getSubtargetImpl(F)),
-        TLI(ST->getTargetLowering()) {}
+        TLI(ST->getTargetLowering()), Fn(&F) {}
 
   bool areInlineCompatible(const Function *Caller,
                            const Function *Callee) const;
@@ -190,6 +191,8 @@
                                TTI::UnrollingPreferences &UP);
 
   bool shouldBuildLookupTablesForConstant(Constant *C) const {
+    if (Fn->getCallingConv() == CallingConv::V8CC)
+      return false;
     // In the ROPI and RWPI relocation models we can't have pointers to global
     // variables or functions in constant data, so don't convert switches to
     // lookup tables if any of the values would need relocation.
Index: lib/Target/Target.cpp
===================================================================
--- lib/Target/Target.cpp	(revision 375507)
+++ lib/Target/Target.cpp	(working copy)
@@ -65,6 +65,12 @@
   unwrap(PM)->add(new TargetLibraryInfoWrapperPass(*unwrap(TLI)));
 }
 
+LLVMTargetLibraryInfoRef LLVMCreateEmptyTargetLibraryInfo() {
+  TargetLibraryInfoImpl *TLII = new TargetLibraryInfoImpl();
+  TLII->disableAllFunctions();
+  return wrap(TLII);
+}
+
 char *LLVMCopyStringRepOfTargetData(LLVMTargetDataRef TD) {
   std::string StringRep = unwrap(TD)->getStringRepresentation();
   return strdup(StringRep.c_str());
Index: lib/Transforms/IPO/PassManagerBuilder.cpp
===================================================================
--- lib/Transforms/IPO/PassManagerBuilder.cpp	(revision 375507)
+++ lib/Transforms/IPO/PassManagerBuilder.cpp	(working copy)
@@ -34,6 +34,7 @@
 #include "llvm/Transforms/IPO/ForceFunctionAttrs.h"
 #include "llvm/Transforms/IPO/FunctionAttrs.h"
 #include "llvm/Transforms/IPO/InferFunctionAttrs.h"
+#include "llvm/Transforms/IPO/SampleProfile.h"
 #include "llvm/Transforms/InstCombine/InstCombine.h"
 #include "llvm/Transforms/Instrumentation.h"
 #include "llvm/Transforms/Scalar.h"
@@ -268,6 +269,8 @@
 // Do PGO instrumentation generation or use pass as the option specified.
 void PassManagerBuilder::addPGOInstrPasses(legacy::PassManagerBase &MPM,
                                            bool IsCS = false) {
+  if (PGOSampleUse.empty())
+    PGOSampleUse = SampleProfileLoaderPass::SampleProfileFileFromOption();
   if (IsCS) {
     if (!EnablePGOCSInstrGen && !EnablePGOCSInstrUse)
       return;
@@ -425,6 +428,8 @@
 
 void PassManagerBuilder::populateModulePassManager(
     legacy::PassManagerBase &MPM) {
+  if (PGOSampleUse.empty())
+    PGOSampleUse = SampleProfileLoaderPass::SampleProfileFileFromOption();
   // Whether this is a default or *LTO pre-link pipeline. The FullLTO post-link
   // is handled separately, so just check this is not the ThinLTO post-link.
   bool DefaultOrPreLinkPipeline = !PerformThinLTO;
@@ -786,6 +791,8 @@
 }
 
 void PassManagerBuilder::addLTOOptimizationPasses(legacy::PassManagerBase &PM) {
+  if (PGOSampleUse.empty())
+    PGOSampleUse = SampleProfileLoaderPass::SampleProfileFileFromOption();
   // Load sample profile before running the LTO optimization pipeline.
   if (!PGOSampleUse.empty()) {
     PM.add(createPruneEHPass());
Index: lib/Transforms/IPO/SampleProfile.cpp
===================================================================
--- lib/Transforms/IPO/SampleProfile.cpp	(revision 375507)
+++ lib/Transforms/IPO/SampleProfile.cpp	(working copy)
@@ -1707,3 +1707,7 @@
 
   return PreservedAnalyses::none();
 }
+
+std::string SampleProfileLoaderPass::SampleProfileFileFromOption() {
+  return SampleProfileFile;
+}
Index: lib/Transforms/Vectorize/LoopVectorize.cpp
===================================================================
--- lib/Transforms/Vectorize/LoopVectorize.cpp	(revision 375507)
+++ lib/Transforms/Vectorize/LoopVectorize.cpp	(working copy)
@@ -7645,6 +7645,14 @@
   }
 
   // Process each loop nest in the function.
+  if (Changed) {
+    // Add alignstack attribute to function.
+    if (F.hasFnAttribute("js-wasm-call")) {
+      F.addAttribute(
+          AttributeList::FunctionIndex,
+          Attribute::get(F.getContext(), Attribute::StackAlignment, 4));
+    }
+  }
   return Changed;
 }
 
Index: tools/llvm-shlib/CMakeLists.txt
===================================================================
--- tools/llvm-shlib/CMakeLists.txt	(revision 375507)
+++ tools/llvm-shlib/CMakeLists.txt	(working copy)
@@ -67,6 +67,7 @@
   endif()
 
   target_link_libraries(LLVM PRIVATE ${LIB_NAMES})
+  set_target_properties(LLVM PROPERTIES LINK_FLAGS "-static-libstdc++")
 
   if (APPLE)
     set_property(TARGET LLVM APPEND_STRING PROPERTY
Index: tools/llvm-shlib/simple_version_script.map.in
===================================================================
--- tools/llvm-shlib/simple_version_script.map.in	(revision 375507)
+++ tools/llvm-shlib/simple_version_script.map.in	(working copy)
@@ -1 +1 @@
-LLVM_@LLVM_VERSION_MAJOR@ { global: *; };
+LLVM_@LLVM_VERSION_MAJOR@ { global: LLVM*; __jit_debug_descriptor; local:*; };
