Index: include/llvm/CodeGen/TargetRegisterInfo.h
===================================================================
--- include/llvm/CodeGen/TargetRegisterInfo.h	(revision 359070)
+++ include/llvm/CodeGen/TargetRegisterInfo.h	(working copy)
@@ -968,6 +968,13 @@
     return StringRef(getName(Reg));
   }
 
+  using VirtRegToFixSlotMap = std::vector<std::pair<unsigned, int>>;
+
+  virtual VirtRegToFixSlotMap
+  getHoistToFixStackSlotMap(MachineFunction &) const {
+    return VirtRegToFixSlotMap();
+  }
+
   //===--------------------------------------------------------------------===//
   /// Subtarget Hooks
 
Index: include/llvm/IR/CallingConv.h
===================================================================
--- include/llvm/IR/CallingConv.h	(revision 359070)
+++ include/llvm/IR/CallingConv.h	(working copy)
@@ -220,8 +220,14 @@
     /// shader if tessellation is in use, or otherwise the vertex shader.
     AMDGPU_ES = 96,
 
+    // Calling convention for v8
+    V8CC = 97,
+
+    // Calling convention for v8 store barrier stub
+    V8SBCC = 98,
+
     // Calling convention between AArch64 Advanced SIMD functions
-    AArch64_VectorCall = 97,
+    AArch64_VectorCall = 99,
 
     /// The highest possible calling convention ID. Must be some 2^k - 1.
     MaxID = 1023
Index: include/llvm/Support/TargetOpcodes.def
===================================================================
--- include/llvm/Support/TargetOpcodes.def	(revision 359070)
+++ include/llvm/Support/TargetOpcodes.def	(working copy)
@@ -120,6 +120,7 @@
 /// rewrite calls to runtimes with more efficient code sequences.
 /// This also implies a stack map.
 HANDLE_TARGET_OPCODE(PATCHPOINT)
+HANDLE_TARGET_OPCODE(TCPATCHPOINT)
 
 /// This pseudo-instruction loads the stack guard value. Targets which need
 /// to prevent the stack guard value or address from being spilled to the
Index: include/llvm/Target/Target.td
===================================================================
--- include/llvm/Target/Target.td	(revision 359070)
+++ include/llvm/Target/Target.td	(working copy)
@@ -1068,6 +1068,17 @@
   let mayLoad = 1;
   let usesCustomInserter = 1;
 }
+def TCPATCHPOINT : StandardPseudoInstruction {
+  let OutOperandList = (outs);
+  let InOperandList = (ins i64imm:$id, i32imm:$nbytes, unknown:$callee,
+                       i32imm:$nargs, i32imm:$cc, variable_ops);
+  let hasSideEffects = 1;
+  let isCall = 1;
+  let mayLoad = 1;
+  let usesCustomInserter = 1;
+  let isReturn = 1;
+  let isTerminator = 1;
+}
 def STATEPOINT : StandardPseudoInstruction {
   let OutOperandList = (outs);
   let InOperandList = (ins variable_ops);
Index: include/llvm/Transforms/IPO/SampleProfile.h
===================================================================
--- include/llvm/Transforms/IPO/SampleProfile.h	(revision 359070)
+++ include/llvm/Transforms/IPO/SampleProfile.h	(working copy)
@@ -31,6 +31,7 @@
         IsThinLTOPreLink(IsThinLTOPreLink) {}
 
   PreservedAnalyses run(Module &M, ModuleAnalysisManager &AM);
+  static std::string SampleProfileFileFromOption();
 
 private:
   std::string ProfileFileName;
Index: include/llvm-c/Core.h
===================================================================
--- include/llvm-c/Core.h	(revision 359070)
+++ include/llvm-c/Core.h	(working copy)
@@ -245,7 +245,9 @@
   LLVMAMDGPUHSCallConv      = 93,
   LLVMMSP430BUILTINCallConv = 94,
   LLVMAMDGPULSCallConv      = 95,
-  LLVMAMDGPUESCallConv      = 96
+  LLVMAMDGPUESCallConv      = 96,
+  LLVMV8CallConv            = 97,
+  LLVMV8SBCallConv          = 98
 } LLVMCallConv;
 
 typedef enum {
Index: include/llvm-c/Target.h
===================================================================
--- include/llvm-c/Target.h	(revision 359070)
+++ include/llvm-c/Target.h	(working copy)
@@ -211,6 +211,9 @@
 void LLVMAddTargetLibraryInfo(LLVMTargetLibraryInfoRef TLI,
                               LLVMPassManagerRef PM);
 
+/** Create empty TargetLibraryInfoImpl, with all functions disabled */
+LLVMTargetLibraryInfoRef LLVMCreateEmptyTargetLibraryInfo();
+
 /** Converts target data to a target layout string. The string must be disposed
     with LLVMDisposeMessage.
     See the constructor llvm::DataLayout::DataLayout. */
Index: lib/Analysis/BranchProbabilityInfo.cpp
===================================================================
--- lib/Analysis/BranchProbabilityInfo.cpp	(revision 359070)
+++ lib/Analysis/BranchProbabilityInfo.cpp	(working copy)
@@ -302,7 +302,13 @@
   assert(WeightSum <= UINT32_MAX &&
          "Expected weights to scale down to 32 bits");
 
-  if (WeightSum == 0 || ReachableIdxs.size() == 0) {
+  bool NoUnreachableHeuristic = false;
+  CallingConv::ID CC = BB->getParent()->getCallingConv();
+  if ((CC == CallingConv::V8CC) || (CC == CallingConv::V8SBCC))
+    NoUnreachableHeuristic = true;
+
+  if (!NoUnreachableHeuristic &&
+      (WeightSum == 0 || ReachableIdxs.size() == 0)) {
     for (unsigned i = 0, e = TI->getNumSuccessors(); i != e; ++i)
       Weights[i] = 1;
     WeightSum = TI->getNumSuccessors();
@@ -315,7 +321,8 @@
 
   // Examine the metadata against unreachable heuristic.
   // If the unreachable heuristic is more strong then we use it for this edge.
-  if (UnreachableIdxs.size() > 0 && ReachableIdxs.size() > 0) {
+  if (!NoUnreachableHeuristic &&
+      (UnreachableIdxs.size() > 0 && ReachableIdxs.size() > 0)) {
     auto ToDistribute = BranchProbability::getZero();
     auto UnreachableProb = UR_TAKEN_PROB;
     for (auto i : UnreachableIdxs)
Index: lib/AsmParser/LLLexer.cpp
===================================================================
--- lib/AsmParser/LLLexer.cpp	(revision 359070)
+++ lib/AsmParser/LLLexer.cpp	(working copy)
@@ -607,6 +607,8 @@
   KEYWORD(webkit_jscc);
   KEYWORD(swiftcc);
   KEYWORD(anyregcc);
+  KEYWORD(v8cc);
+  KEYWORD(v8sbcc);
   KEYWORD(preserve_mostcc);
   KEYWORD(preserve_allcc);
   KEYWORD(ghccc);
Index: lib/AsmParser/LLParser.cpp
===================================================================
--- lib/AsmParser/LLParser.cpp	(revision 359070)
+++ lib/AsmParser/LLParser.cpp	(working copy)
@@ -1931,6 +1931,12 @@
   case lltok::kw_win64cc:        CC = CallingConv::Win64; break;
   case lltok::kw_webkit_jscc:    CC = CallingConv::WebKit_JS; break;
   case lltok::kw_anyregcc:       CC = CallingConv::AnyReg; break;
+  case lltok::kw_v8cc:
+    CC = CallingConv::V8CC;
+    break;
+  case lltok::kw_v8sbcc:
+    CC = CallingConv::V8SBCC;
+    break;
   case lltok::kw_preserve_mostcc:CC = CallingConv::PreserveMost; break;
   case lltok::kw_preserve_allcc: CC = CallingConv::PreserveAll; break;
   case lltok::kw_ghccc:          CC = CallingConv::GHC; break;
Index: lib/AsmParser/LLToken.h
===================================================================
--- lib/AsmParser/LLToken.h	(revision 359070)
+++ lib/AsmParser/LLToken.h	(working copy)
@@ -151,6 +151,8 @@
   kw_win64cc,
   kw_webkit_jscc,
   kw_anyregcc,
+  kw_v8cc,
+  kw_v8sbcc,
   kw_swiftcc,
   kw_preserve_mostcc,
   kw_preserve_allcc,
Index: lib/CodeGen/InlineSpiller.cpp
===================================================================
--- lib/CodeGen/InlineSpiller.cpp	(revision 359070)
+++ lib/CodeGen/InlineSpiller.cpp	(working copy)
@@ -57,6 +57,7 @@
 #include <cassert>
 #include <iterator>
 #include <tuple>
+#include <unordered_set>
 #include <utility>
 #include <vector>
 
@@ -213,6 +214,7 @@
   bool isSibling(unsigned Reg);
   bool hoistSpillInsideBB(LiveInterval &SpillLI, MachineInstr &CopyMI);
   void eliminateRedundantSpills(LiveInterval &LI, VNInfo *VNI);
+  void foldStatePoints(unsigned Reg);
 
   void markValueUsed(LiveInterval*, VNInfo*);
   bool reMaterializeFor(LiveInterval &, MachineInstr &MI);
@@ -924,11 +926,66 @@
     HSpiller.addToMergeableSpills(*std::next(MI), StackSlot, Original);
 }
 
+void InlineSpiller::foldStatePoints(unsigned InputReg) {
+  SmallVector<unsigned, 8> WorkList;
+  std::unordered_set<unsigned> VisitedSet;
+  // Don't add Reg to WorkList, spillAroundUses will handle it.
+  for (MachineRegisterInfo::reg_bundle_iterator
+           RegI = MRI.reg_bundle_begin(InputReg),
+           E = MRI.reg_bundle_end();
+       RegI != E;) {
+    MachineInstr &MI = *RegI++;
+    unsigned SibReg = isFullCopyOf(MI, InputReg);
+    if (SibReg && isSibling(SibReg) && SibReg != InputReg) {
+      WorkList.push_back(SibReg);
+      VisitedSet.emplace(SibReg);
+    }
+  }
+  VisitedSet.emplace(InputReg);
+  LiveInterval &OldLI = LIS.getInterval(InputReg);
+  while (!WorkList.empty()) {
+    unsigned Reg = WorkList.pop_back_val();
+    // Fold the use of state point.
+    for (MachineRegisterInfo::use_instr_nodbg_iterator
+             UI = MRI.use_instr_nodbg_begin(Reg),
+             E = MRI.use_instr_nodbg_end();
+         UI != E;) {
+      MachineInstr &MI = *UI++;
+      if (unsigned DstReg = isFullCopyOf(MI, Reg)) {
+        if (isSibling(DstReg)) {
+          auto pair = VisitedSet.emplace(DstReg);
+          if (pair.second)
+            WorkList.push_back(DstReg);
+        }
+        continue;
+      }
+      // Handle STATEPOINT, PATCHPOINT
+      switch (MI.getOpcode()) {
+      case TargetOpcode::STATEPOINT:
+      case TargetOpcode::PATCHPOINT: {
+        // Ignore if the input reg's live range does not contain
+        // the MI.
+        SlotIndex Idx = LIS.getInstructionIndex(MI).getRegSlot();
+        if (OldLI.end() == OldLI.FindSegmentContaining(Idx))
+          continue;
+        // Analyze instruction.
+        SmallVector<std::pair<MachineInstr *, unsigned>, 8> Ops;
+        MIBundleOperands(MI).analyzeVirtReg(Reg, &Ops);
+        foldMemoryOperand(Ops);
+      } break;
+      default:
+        break;
+      }
+    }
+  }
+}
+
 /// spillAroundUses - insert spill code around each use of Reg.
 void InlineSpiller::spillAroundUses(unsigned Reg) {
   LLVM_DEBUG(dbgs() << "spillAroundUses " << printReg(Reg) << '\n');
   LiveInterval &OldLI = LIS.getInterval(Reg);
 
+  foldStatePoints(Reg);
   // Iterate over instructions using Reg.
   for (MachineRegisterInfo::reg_bundle_iterator
        RegI = MRI.reg_bundle_begin(Reg), E = MRI.reg_bundle_end();
Index: lib/CodeGen/LocalStackSlotAllocation.cpp
===================================================================
--- lib/CodeGen/LocalStackSlotAllocation.cpp	(revision 359070)
+++ lib/CodeGen/LocalStackSlotAllocation.cpp	(working copy)
@@ -299,6 +299,7 @@
       // range, so they don't need any updates.
       if (MI.isDebugInstr() || MI.getOpcode() == TargetOpcode::STATEPOINT ||
           MI.getOpcode() == TargetOpcode::STACKMAP ||
+          MI.getOpcode() == TargetOpcode::TCPATCHPOINT ||
           MI.getOpcode() == TargetOpcode::PATCHPOINT)
         continue;
 
Index: lib/CodeGen/MachineVerifier.cpp
===================================================================
--- lib/CodeGen/MachineVerifier.cpp	(revision 359070)
+++ lib/CodeGen/MachineVerifier.cpp	(working copy)
@@ -1203,6 +1203,8 @@
   unsigned NumDefs = MCID.getNumDefs();
   if (MCID.getOpcode() == TargetOpcode::PATCHPOINT)
     NumDefs = (MONum == 0 && MO->isReg()) ? NumDefs : 0;
+  else if (MCID.getOpcode() == TargetOpcode::TCPATCHPOINT)
+    NumDefs = 0;
 
   // The first MCID.NumDefs operands must be explicit register defines
   if (MONum < NumDefs) {
Index: lib/CodeGen/RegAllocBase.cpp
===================================================================
--- lib/CodeGen/RegAllocBase.cpp	(revision 359070)
+++ lib/CodeGen/RegAllocBase.cpp	(working copy)
@@ -30,6 +30,7 @@
 #include "llvm/Support/Timer.h"
 #include "llvm/Support/raw_ostream.h"
 #include <cassert>
+#include <unordered_map>
 
 using namespace llvm;
 
@@ -166,4 +167,39 @@
     DeadInst->eraseFromParent();
   }
   DeadRemats.clear();
+  tryHoistToStackSlot();
 }
+
+void RegAllocBase::tryHoistToStackSlot() {
+  MachineFunction &MF = VRM->getMachineFunction();
+  auto RegMapping = TRI->getHoistToFixStackSlotMap(MF);
+  if (RegMapping.empty())
+    return;
+  std::unordered_map<int, int> SlotMapping;
+  for (auto &Entry : RegMapping) {
+    int Slot = VRM->getStackSlot(Entry.first);
+    if (Slot == VirtRegMap::NO_STACK_SLOT)
+      continue;
+    SlotMapping.emplace(Slot, Entry.second);
+  }
+  if (SlotMapping.empty())
+    return;
+  for (MachineBasicBlock &MBB : MF) {
+    for (MachineInstr &MI : MBB)
+      for (unsigned i = 0, ee = MI.getNumOperands(); i != ee; ++i) {
+        MachineOperand &MO = MI.getOperand(i);
+        if (!MO.isFI())
+          continue;
+        int OldFI = MO.getIndex();
+        if (OldFI < 0)
+          continue;
+        auto FoundFI = SlotMapping.find(OldFI);
+        if (FoundFI == SlotMapping.end())
+          continue;
+        int NewFI = FoundFI->second;
+        if (NewFI == OldFI)
+          continue;
+        MO.setIndex(NewFI);
+      }
+  }
+}
Index: lib/CodeGen/RegAllocBase.h
===================================================================
--- lib/CodeGen/RegAllocBase.h	(revision 359070)
+++ lib/CodeGen/RegAllocBase.h	(working copy)
@@ -118,6 +118,7 @@
 
 private:
   void seedLiveRegs();
+  void tryHoistToStackSlot();
 };
 
 } // end namespace llvm
Index: lib/CodeGen/SelectionDAG/InstrEmitter.cpp
===================================================================
--- lib/CodeGen/SelectionDAG/InstrEmitter.cpp	(revision 359070)
+++ lib/CodeGen/SelectionDAG/InstrEmitter.cpp	(working copy)
@@ -832,7 +832,8 @@
   const MCPhysReg *ScratchRegs = nullptr;
 
   // Handle STACKMAP and PATCHPOINT specially and then use the generic code.
-  if (Opc == TargetOpcode::STACKMAP || Opc == TargetOpcode::PATCHPOINT) {
+  if (Opc == TargetOpcode::STACKMAP || Opc == TargetOpcode::PATCHPOINT ||
+      Opc == TargetOpcode::TCPATCHPOINT) {
     // Stackmaps do not have arguments and do not preserve their calling
     // convention. However, to simplify runtime support, they clobber the same
     // scratch registers as AnyRegCC.
Index: lib/CodeGen/SelectionDAG/ScheduleDAGSDNodes.cpp
===================================================================
--- lib/CodeGen/SelectionDAG/ScheduleDAGSDNodes.cpp	(revision 359070)
+++ lib/CodeGen/SelectionDAG/ScheduleDAGSDNodes.cpp	(working copy)
@@ -538,6 +538,10 @@
     NodeNumDefs = 0;
     return;
   }
+  if (POpc == TargetOpcode::TCPATCHPOINT) {
+    NodeNumDefs = 0;
+    return;
+  }
   if (POpc == TargetOpcode::PATCHPOINT &&
       Node->getValueType(0) == MVT::Other) {
     // PATCHPOINT is defined to have one result, but it might really have none
Index: lib/CodeGen/SelectionDAG/SelectionDAGBuilder.cpp
===================================================================
--- lib/CodeGen/SelectionDAG/SelectionDAGBuilder.cpp	(revision 359070)
+++ lib/CodeGen/SelectionDAG/SelectionDAGBuilder.cpp	(working copy)
@@ -1221,6 +1221,16 @@
   SDValue &N = NodeMap[V];
   if (N.getNode()) return N;
 
+  // Special handling BitCastInst with a constant.
+  if (const BitCastInst *I = dyn_cast<BitCastInst>(V)) {
+    if (const Constant *C = dyn_cast<Constant>(I->getOperand(0))) {
+      SDValue Val = getValueImpl(C);
+      NodeMap[V] = Val;
+      resolveDanglingDebugInfo(V, Val);
+      return Val;
+    }
+  }
+
   // If there's a virtual register allocated and initialized for this
   // value, use it.
   if (SDValue copyFromReg = getCopyFromRegs(V, V->getType()))
@@ -8187,6 +8197,32 @@
   FuncInfo.MF->getFrameInfo().setHasStackMap();
 }
 
+static bool isPatchpointInTailCallPosition(ImmutableCallSite CS) {
+  const Instruction *I = CS.getInstruction();
+  const BasicBlock *ExitBB = I->getParent();
+  const Instruction *Term = ExitBB->getTerminator();
+  const ReturnInst *Ret = dyn_cast<ReturnInst>(Term);
+  // If Term is not a ReturnInst, then it must be a UnreachableInst.
+  if (!Ret && !isa<UnreachableInst>(Term))
+    return false;
+  // Copy from isInTailCallPosition.
+  // If I will have a chain, make sure no other instruction that will have a
+  // chain interposes between I and the return.
+  if (I->mayHaveSideEffects() || I->mayReadFromMemory() ||
+      !isSafeToSpeculativelyExecute(I))
+    for (BasicBlock::const_iterator BBI = std::prev(ExitBB->end(), 2);; --BBI) {
+      if (&*BBI == I)
+        break;
+      // Debug info intrinsics do not get in the way of tail call optimization.
+      if (isa<DbgInfoIntrinsic>(BBI))
+        continue;
+      if (BBI->mayHaveSideEffects() || BBI->mayReadFromMemory() ||
+          !isSafeToSpeculativelyExecute(&*BBI))
+        return false;
+    }
+  return true;
+}
+
 /// Lower llvm.experimental.patchpoint directly to its target opcode.
 void SelectionDAGBuilder::visitPatchpoint(ImmutableCallSite CS,
                                           const BasicBlock *EHPadBB) {
@@ -8230,17 +8266,24 @@
   TargetLowering::CallLoweringInfo CLI(DAG);
   populateCallLoweringInfo(CLI, CS, NumMetaOpers, NumCallArgs, Callee, ReturnTy,
                            true);
+  CLI.IsTailCall = isPatchpointInTailCallPosition(CS);
+  assert((!CLI.IsTailCall || !HasDef) &&
+         "TailCall should not has a return type");
   std::pair<SDValue, SDValue> Result = lowerInvokable(CLI, EHPadBB);
+  SDNode *Call;
+  if (!CLI.IsTailCall) {
+    SDNode *CallEnd = Result.second.getNode();
+    if (HasDef && (CallEnd->getOpcode() == ISD::CopyFromReg))
+      CallEnd = CallEnd->getOperand(0).getNode();
 
-  SDNode *CallEnd = Result.second.getNode();
-  if (HasDef && (CallEnd->getOpcode() == ISD::CopyFromReg))
-    CallEnd = CallEnd->getOperand(0).getNode();
-
-  /// Get a call instruction from the call sequence chain.
-  /// Tail calls are not allowed.
-  assert(CallEnd->getOpcode() == ISD::CALLSEQ_END &&
-         "Expected a callseq node.");
-  SDNode *Call = CallEnd->getOperand(0).getNode();
+    /// Get a call instruction from the call sequence chain.
+    /// Tail calls are not allowed.
+    assert(CallEnd->getOpcode() == ISD::CALLSEQ_END &&
+           "Expected a callseq node.");
+    Call = CallEnd->getOperand(0).getNode();
+  } else {
+    Call = CLI.Chain.getNode();
+  }
   bool HasGlue = Call->getGluedNode();
 
   // Replace the target specific call node with the patchable intrinsic.
@@ -8311,8 +8354,9 @@
     NodeTys = DAG.getVTList(MVT::Other, MVT::Glue);
 
   // Replace the target specific call node with a PATCHPOINT node.
-  MachineSDNode *MN = DAG.getMachineNode(TargetOpcode::PATCHPOINT,
-                                         dl, NodeTys, Ops);
+  MachineSDNode *MN = DAG.getMachineNode(
+      CLI.IsTailCall ? TargetOpcode::TCPATCHPOINT : TargetOpcode::PATCHPOINT,
+      dl, NodeTys, Ops);
 
   // Update the NodeMap.
   if (HasDef) {
Index: lib/CodeGen/SelectionDAG/StatepointLowering.cpp
===================================================================
--- lib/CodeGen/SelectionDAG/StatepointLowering.cpp	(revision 359070)
+++ lib/CodeGen/SelectionDAG/StatepointLowering.cpp	(working copy)
@@ -541,14 +541,15 @@
   // arrays interwoven with each (lowered) base pointer immediately followed by
   // it's (lowered) derived pointer.  i.e
   // (base[0], ptr[0], base[1], ptr[1], ...)
+  bool LiveInOnly = SI.CLI.CallConv == CallingConv::V8CC;
   for (unsigned i = 0; i < SI.Bases.size(); ++i) {
     const Value *Base = SI.Bases[i];
-    lowerIncomingStatepointValue(Builder.getValue(Base), /*LiveInOnly*/ false,
-                                 Ops, Builder);
+    lowerIncomingStatepointValue(Builder.getValue(Base), LiveInOnly, Ops,
+                                 Builder);
 
     const Value *Ptr = SI.Ptrs[i];
-    lowerIncomingStatepointValue(Builder.getValue(Ptr), /*LiveInOnly*/ false,
-                                 Ops, Builder);
+    lowerIncomingStatepointValue(Builder.getValue(Ptr), LiveInOnly, Ops,
+                                 Builder);
   }
 
   // If there are any explicit spill slots passed to the statepoint, record
@@ -827,6 +828,11 @@
 
     unsigned AS = ISP.getCalledValue()->getType()->getPointerAddressSpace();
     ActualCallee = DAG.getConstant(0, getCurSDLoc(), TLI.getPointerTy(DL, AS));
+    if (auto *ConstCallee =
+            dyn_cast<ConstantSDNode>(getValue(ISP.getCalledValue())))
+      ActualCallee =
+          DAG.getIntPtrConstant(ConstCallee->getZExtValue(), getCurSDLoc(),
+                                /*isTarget=*/true);
   } else {
     ActualCallee = getValue(ISP.getCalledValue());
   }
Index: lib/CodeGen/SelectionDAG/TargetLowering.cpp
===================================================================
--- lib/CodeGen/SelectionDAG/TargetLowering.cpp	(revision 359070)
+++ lib/CodeGen/SelectionDAG/TargetLowering.cpp	(working copy)
@@ -89,6 +89,9 @@
     // (We look for a CopyFromReg reading a virtual register that is used
     //  for the function live-in value of register Reg)
     SDValue Value = OutVals[I];
+    // Ignore the undefined input.
+    if (Value->getOpcode() == ISD::UNDEF)
+      continue;
     if (Value->getOpcode() != ISD::CopyFromReg)
       return false;
     unsigned ArgReg = cast<RegisterSDNode>(Value->getOperand(1))->getReg();
Index: lib/CodeGen/ShrinkWrap.cpp
===================================================================
--- lib/CodeGen/ShrinkWrap.cpp	(revision 359070)
+++ lib/CodeGen/ShrinkWrap.cpp	(working copy)
@@ -264,6 +264,8 @@
     LLVM_DEBUG(dbgs() << "Frame instruction: " << MI << '\n');
     return true;
   }
+  if (MI.isReturn())
+    return false;
   for (const MachineOperand &MO : MI.operands()) {
     bool UseOrDefCSR = false;
     if (MO.isReg()) {
@@ -280,8 +282,8 @@
       // separately. An SP mentioned by a call instruction, we can ignore,
       // though, as it's harmless and we do not want to effectively disable tail
       // calls by forcing the restore point to post-dominate them.
-      UseOrDefCSR = (!MI.isCall() && PhysReg == SP) ||
-                    RCI.getLastCalleeSavedAlias(PhysReg);
+      UseOrDefCSR =
+          (!MI.isCall() && PhysReg == SP) || getCurrentCSRs(RS).count(PhysReg);
     } else if (MO.isRegMask()) {
       // Check if this regmask clobbers any of the CSRs.
       for (unsigned Reg : getCurrentCSRs(RS)) {
Index: lib/CodeGen/StackMaps.cpp
===================================================================
--- lib/CodeGen/StackMaps.cpp	(revision 359070)
+++ lib/CodeGen/StackMaps.cpp	(working copy)
@@ -30,6 +30,7 @@
 #include "llvm/Support/ErrorHandling.h"
 #include "llvm/Support/MathExtras.h"
 #include "llvm/Support/raw_ostream.h"
+#include "llvm/Target/TargetMachine.h"
 #include <algorithm>
 #include <cassert>
 #include <cstdint>
@@ -371,7 +372,9 @@
 }
 
 void StackMaps::recordPatchPoint(const MachineInstr &MI) {
-  assert(MI.getOpcode() == TargetOpcode::PATCHPOINT && "expected patchpoint");
+  assert(((MI.getOpcode() == TargetOpcode::PATCHPOINT) ||
+          (MI.getOpcode() == TargetOpcode::TCPATCHPOINT)) &&
+         "expected patchpoint");
 
   PatchPointOpers opers(&MI);
   const int64_t ID = opers.getID();
@@ -443,7 +446,7 @@
     LLVM_DEBUG(dbgs() << WSMP << "function addr: " << FR.first
                       << " frame size: " << FR.second.StackSize
                       << " callsite count: " << FR.second.RecordCount << '\n');
-    OS.EmitSymbolValue(FR.first, 8);
+    OS.EmitSymbolValue(FR.first, AP.TM.getProgramPointerSize());
     OS.EmitIntValue(FR.second.StackSize, 8);
     OS.EmitIntValue(FR.second.RecordCount, 8);
   }
Index: lib/CodeGen/TargetInstrInfo.cpp
===================================================================
--- lib/CodeGen/TargetInstrInfo.cpp	(revision 359070)
+++ lib/CodeGen/TargetInstrInfo.cpp	(working copy)
@@ -468,6 +468,7 @@
     StartIdx = StackMapOpers(&MI).getVarIdx();
     break;
   }
+  case TargetOpcode::TCPATCHPOINT:
   case TargetOpcode::PATCHPOINT: {
     // For PatchPoint, the call args are not foldable (even if reported in the
     // stackmap e.g. via anyregcc).
@@ -562,6 +563,7 @@
 
   if (MI.getOpcode() == TargetOpcode::STACKMAP ||
       MI.getOpcode() == TargetOpcode::PATCHPOINT ||
+      MI.getOpcode() == TargetOpcode::TCPATCHPOINT ||
       MI.getOpcode() == TargetOpcode::STATEPOINT) {
     // Fold stackmap/patchpoint.
     NewMI = foldPatchpoint(MF, MI, Ops, FI, *this);
@@ -626,6 +628,7 @@
   int FrameIndex = 0;
 
   if ((MI.getOpcode() == TargetOpcode::STACKMAP ||
+       MI.getOpcode() == TargetOpcode::TCPATCHPOINT ||
        MI.getOpcode() == TargetOpcode::PATCHPOINT ||
        MI.getOpcode() == TargetOpcode::STATEPOINT) &&
       isLoadFromStackSlot(LoadMI, FrameIndex)) {
Index: lib/CodeGen/TargetPassConfig.cpp
===================================================================
--- lib/CodeGen/TargetPassConfig.cpp	(revision 359070)
+++ lib/CodeGen/TargetPassConfig.cpp	(working copy)
@@ -954,6 +954,7 @@
   if (getOptLevel() != CodeGenOpt::None)
     addBlockPlacement();
 
+  addPass(&StackMapLivenessID, false);
   addPreEmitPass();
 
   if (TM->Options.EnableIPRA)
@@ -963,7 +964,6 @@
 
   addPass(&FuncletLayoutID, false);
 
-  addPass(&StackMapLivenessID, false);
   addPass(&LiveDebugValuesID, false);
 
   // Insert before XRay Instrumentation.
Index: lib/ExecutionEngine/ExecutionEngineBindings.cpp
===================================================================
--- lib/ExecutionEngine/ExecutionEngineBindings.cpp	(revision 359070)
+++ lib/ExecutionEngine/ExecutionEngineBindings.cpp	(working copy)
@@ -200,6 +200,14 @@
          .setErrorStr(&Error)
          .setOptLevel((CodeGenOpt::Level)options.OptLevel)
          .setTargetOptions(targetOptions);
+  builder.setRelocationModel(Reloc::PIC_);
+  std::vector<std::string> AttrList;
+  AttrList.emplace_back("+armv7-a");
+  AttrList.emplace_back("+dsp");
+  AttrList.emplace_back("+neon");
+  AttrList.emplace_back("+vfp3");
+  AttrList.emplace_back("+hwdiv-arm");
+  builder.setMAttrs(AttrList);
   bool JIT;
   if (Optional<CodeModel::Model> CM = unwrap(options.CodeModel, JIT))
     builder.setCodeModel(*CM);
Index: lib/IR/AsmWriter.cpp
===================================================================
--- lib/IR/AsmWriter.cpp	(revision 359070)
+++ lib/IR/AsmWriter.cpp	(working copy)
@@ -349,6 +349,12 @@
   case CallingConv::Cold:          Out << "coldcc"; break;
   case CallingConv::WebKit_JS:     Out << "webkit_jscc"; break;
   case CallingConv::AnyReg:        Out << "anyregcc"; break;
+  case CallingConv::V8CC:
+    Out << "v8cc";
+    break;
+  case CallingConv::V8SBCC:
+    Out << "v8sbcc";
+    break;
   case CallingConv::PreserveMost:  Out << "preserve_mostcc"; break;
   case CallingConv::PreserveAll:   Out << "preserve_allcc"; break;
   case CallingConv::CXX_FAST_TLS:  Out << "cxx_fast_tlscc"; break;
Index: lib/MC/MCAsmStreamer.cpp
===================================================================
--- lib/MC/MCAsmStreamer.cpp	(revision 359070)
+++ lib/MC/MCAsmStreamer.cpp	(working copy)
@@ -941,7 +941,7 @@
   if (!Directive) {
     int64_t IntValue;
     if (!Value->evaluateAsAbsolute(IntValue))
-      report_fatal_error("Don't know how to emit this value.");
+      IntValue = -1;
 
     // We couldn't handle the requested integer size so we fallback by breaking
     // the request down into several, smaller, integers.
Index: lib/Target/ARM/ARMAsmPrinter.cpp
===================================================================
--- lib/Target/ARM/ARMAsmPrinter.cpp	(revision 359070)
+++ lib/Target/ARM/ARMAsmPrinter.cpp	(working copy)
@@ -55,7 +55,7 @@
 ARMAsmPrinter::ARMAsmPrinter(TargetMachine &TM,
                              std::unique_ptr<MCStreamer> Streamer)
     : AsmPrinter(TM, std::move(Streamer)), AFI(nullptr), MCP(nullptr),
-      InConstantPool(false), OptimizationGoals(-1) {}
+      InConstantPool(false), OptimizationGoals(-1), SM(*this) {}
 
 void ARMAsmPrinter::EmitFunctionBodyEnd() {
   // Make sure to terminate any constant pools that were at the end
@@ -559,6 +559,7 @@
       OutStreamer->AddBlankLine();
     }
 
+    SM.serializeToStackMapSection();
     // Funny Darwin hack: This flag tells the linker that no global symbols
     // contain code that falls through to other global symbols (e.g. the obvious
     // implementation of multiple entry points).  If this doesn't occur, the
@@ -578,6 +579,15 @@
   OptimizationGoals = -1;
 
   ATS.finishAttributeSection();
+  if (TT.isOSBinFormatCOFF()) {
+    SM.serializeToStackMapSection();
+    return;
+  }
+
+  if (TT.isOSBinFormatELF()) {
+    SM.serializeToStackMapSection();
+    return;
+  }
 }
 
 //===----------------------------------------------------------------------===//
@@ -2064,6 +2074,13 @@
   case ARM::PATCHABLE_TAIL_CALL:
     LowerPATCHABLE_TAIL_CALL(*MI);
     return;
+  case TargetOpcode::STACKMAP:
+    return LowerSTACKMAP(*OutStreamer, SM, *MI);
+  case TargetOpcode::PATCHPOINT:
+  case TargetOpcode::TCPATCHPOINT:
+    return LowerPATCHPOINT(*OutStreamer, SM, *MI);
+  case TargetOpcode::STATEPOINT:
+    return LowerSTATEPOINT(*OutStreamer, SM, *MI);
   }
 
   MCInst TmpInst;
@@ -2072,6 +2089,107 @@
   EmitToStreamer(*OutStreamer, TmpInst);
 }
 
+static unsigned roundUpTo4ByteAligned(unsigned n) {
+  unsigned mask = 3;
+  unsigned rev = ~3;
+  n = (n & rev) + (((n & mask) + mask) & rev);
+  return n;
+}
+
+void ARMAsmPrinter::LowerSTACKMAP(MCStreamer &OutStreamer, StackMaps &SM,
+                                  const MachineInstr &MI) {
+  assert(!AFI->isThumbFunction());
+  unsigned NumNOPBytes =
+      roundUpTo4ByteAligned(StackMapOpers(&MI).getNumPatchBytes());
+
+  SM.recordStackMap(MI);
+  assert(NumNOPBytes % 4 == 0 && "Invalid number of NOP bytes requested!");
+
+  // Scan ahead to trim the shadow.
+  const MachineBasicBlock &MBB = *MI.getParent();
+  MachineBasicBlock::const_iterator MII(MI);
+  ++MII;
+  while (NumNOPBytes > 0) {
+    if (MII == MBB.end() || MII->isCall() ||
+        MII->getOpcode() == ARM::DBG_VALUE ||
+        MII->getOpcode() == TargetOpcode::PATCHPOINT ||
+        MII->getOpcode() == TargetOpcode::TCPATCHPOINT ||
+        MII->getOpcode() == TargetOpcode::STACKMAP)
+      break;
+    ++MII;
+    NumNOPBytes -= 4;
+  }
+
+  // Emit nops.
+  MCInst Noop;
+  Subtarget->getInstrInfo()->getNoop(Noop);
+  for (unsigned i = 0; i < NumNOPBytes; i += 4)
+    EmitToStreamer(OutStreamer, Noop);
+}
+
+// Lower a patchpoint of the form:
+// [<def>], <id>, <numBytes>, <target>, <numArgs>
+void ARMAsmPrinter::LowerPATCHPOINT(MCStreamer &OutStreamer, StackMaps &SM,
+                                    const MachineInstr &MI) {
+  assert(!AFI->isThumbFunction());
+  SM.recordPatchPoint(MI);
+
+  PatchPointOpers Opers(&MI);
+
+  int64_t CallTarget = Opers.getCallTarget().getImm();
+  unsigned EncodedBytes = 0;
+  if (CallTarget) {
+    assert((CallTarget & 0xFFFFFFFFLL) == CallTarget &&
+           "High 32 bits of call target should be zero.");
+    unsigned ScratchReg = MI.getOperand(Opers.getNextScratchIdx()).getReg();
+    EncodedBytes = 16;
+    EmitToStreamer(OutStreamer, MCInstBuilder(ARM::MOVTi16)
+                                    .addReg(ScratchReg)
+                                    .addReg(ScratchReg)
+                                    .addImm((CallTarget >> 16) & 0xFFFF)
+                                    .addImm(ARMCC::AL)
+                                    .addImm(0));
+    EmitToStreamer(OutStreamer, MCInstBuilder(ARM::MOVi16)
+                                    .addReg(ScratchReg)
+                                    .addImm(CallTarget & 0xFFFF)
+                                    .addImm(ARMCC::AL)
+                                    .addImm(0));
+    EmitToStreamer(OutStreamer, MCInstBuilder(ARM::BLX).addReg(ScratchReg));
+  }
+  // Emit padding.
+  unsigned NumBytes = roundUpTo4ByteAligned(Opers.getNumPatchBytes());
+  assert(NumBytes >= EncodedBytes &&
+         "Patchpoint can't request size less than the length of a call.");
+  assert((NumBytes - EncodedBytes) % 4 == 0 &&
+         "Invalid number of NOP bytes requested!");
+  MCInst Noop;
+  Subtarget->getInstrInfo()->getNoop(Noop);
+  for (unsigned i = EncodedBytes; i < NumBytes; i += 4)
+    EmitToStreamer(OutStreamer, Noop);
+}
+
+void ARMAsmPrinter::LowerSTATEPOINT(MCStreamer &OutStreamer, StackMaps &SM,
+                                    const MachineInstr &MI) {
+  assert(!AFI->isThumbFunction());
+  SM.recordStatepoint(MI);
+
+  StatepointOpers SOpers(&MI);
+  MCInst Noop;
+  Subtarget->getInstrInfo()->getNoop(Noop);
+  if (unsigned PatchBytes = SOpers.getNumPatchBytes()) {
+    unsigned NumBytes = roundUpTo4ByteAligned(PatchBytes);
+    unsigned EncodedBytes = 0;
+    assert(NumBytes >= EncodedBytes &&
+           "Patchpoint can't request size less than the length of a call.");
+    assert((NumBytes - EncodedBytes) % 4 == 0 &&
+           "Invalid number of NOP bytes requested!");
+    MCInst Noop;
+    Subtarget->getInstrInfo()->getNoop(Noop);
+    for (unsigned i = EncodedBytes; i < NumBytes; i += 4)
+      EmitToStreamer(OutStreamer, Noop);
+  }
+}
+
 //===----------------------------------------------------------------------===//
 // Target Registry Stuff
 //===----------------------------------------------------------------------===//
Index: lib/Target/ARM/ARMAsmPrinter.h
===================================================================
--- lib/Target/ARM/ARMAsmPrinter.h	(revision 359070)
+++ lib/Target/ARM/ARMAsmPrinter.h	(working copy)
@@ -12,6 +12,7 @@
 
 #include "ARMSubtarget.h"
 #include "llvm/CodeGen/AsmPrinter.h"
+#include "llvm/CodeGen/StackMaps.h"
 #include "llvm/Target/TargetMachine.h"
 
 namespace llvm {
@@ -64,8 +65,10 @@
   /// Set of globals in PromotedGlobals that we've emitted labels for.
   /// We need to emit labels even for promoted globals so that DWARF
   /// debug info can link properly.
-  SmallPtrSet<const GlobalVariable*,2> EmittedPromotedGlobalLabels;
+  SmallPtrSet<const GlobalVariable *, 2> EmittedPromotedGlobalLabels;
 
+  StackMaps SM;
+
 public:
   explicit ARMAsmPrinter(TargetMachine &TM,
                          std::unique_ptr<MCStreamer> Streamer);
@@ -131,6 +134,15 @@
   bool emitPseudoExpansionLowering(MCStreamer &OutStreamer,
                                    const MachineInstr *MI);
 
+  void LowerSTACKMAP(MCStreamer &OutStreamer, StackMaps &SM,
+                     const MachineInstr &MI);
+
+  void LowerPATCHPOINT(MCStreamer &OutStreamer, StackMaps &SM,
+                       const MachineInstr &MI);
+
+  void LowerSTATEPOINT(MCStreamer &OutStreamer, StackMaps &SM,
+                       const MachineInstr &MI);
+
 public:
   unsigned getISAEncoding() override {
     // ARM/Darwin adds ISA to the DWARF info for each function.
Index: lib/Target/ARM/ARMBaseInstrInfo.cpp
===================================================================
--- lib/Target/ARM/ARMBaseInstrInfo.cpp	(revision 359070)
+++ lib/Target/ARM/ARMBaseInstrInfo.cpp	(working copy)
@@ -721,6 +721,10 @@
     return 0;
   case TargetOpcode::BUNDLE:
     return getInstBundleLength(MI);
+  case TargetOpcode::TCPATCHPOINT:
+  case TargetOpcode::PATCHPOINT:
+  case TargetOpcode::STATEPOINT:
+    return MI.getOperand(1).getImm();
   case ARM::MOVi16_ga_pcrel:
   case ARM::MOVTi16_ga_pcrel:
   case ARM::t2MOVi16_ga_pcrel:
@@ -1462,6 +1466,49 @@
   BB->erase(MI);
 }
 
+void ARMBaseInstrInfo::expandRESTORESP(MachineBasicBlock::iterator MI) const {
+  MachineBasicBlock *BB = MI->getParent();
+  bool ShouldExpand = true;
+  MachineBasicBlock::iterator it = MI;
+  ++it;
+  MachineBasicBlock::iterator end = MI->getParent()->end();
+  for (; it != end; ++it) {
+    bool IsDef = false, IsUse = false;
+    for (unsigned i = 0, e = it->getNumOperands(); i != e; ++i) {
+      const MachineOperand &MO = it->getOperand(i);
+      if (!MO.isReg())
+        continue;
+      unsigned MOReg = MO.getReg();
+      if (MOReg != ARM::SP)
+        continue;
+      // Define new sp
+      if (MO.isDef())
+        IsDef = true;
+      else
+        IsUse = true;
+    }
+    // Should expand now.
+    if (IsUse)
+      break;
+    if (IsDef) {
+      ShouldExpand = false;
+      break;
+    }
+  }
+  if (ShouldExpand) {
+    MachineFunction *MF = BB->getParent();
+    DebugLoc dl = MI->getDebugLoc();
+
+    ARMFunctionInfo *AFI = MF->getInfo<ARMFunctionInfo>();
+    BuildMI(*BB, MI, dl, get(ARM::SUBri), ARM::SP)
+        .addReg(ARM::R11)
+        .addImm(AFI->getFramePtrSpillOffset())
+        .add(predOps(ARMCC::AL))
+        .add(condCodeOp());
+  }
+  BB->erase(MI);
+}
+
 bool ARMBaseInstrInfo::expandPostRAPseudo(MachineInstr &MI) const {
   if (MI.getOpcode() == TargetOpcode::LOAD_STACK_GUARD) {
     assert(getSubtarget().getTargetTriple().isOSBinFormatMachO() &&
@@ -1476,6 +1523,11 @@
     return true;
   }
 
+  if (MI.getOpcode() == ARM::RESTORESP) {
+    expandRESTORESP(MI);
+    return true;
+  }
+
   // This hook gets to expand COPY instructions before they become
   // copyPhysReg() calls.  Look for VMOVS instructions that can legally be
   // widened to VMOVD.  We prefer the VMOVD when possible because it may be
Index: lib/Target/ARM/ARMBaseInstrInfo.h
===================================================================
--- lib/Target/ARM/ARMBaseInstrInfo.h	(revision 359070)
+++ lib/Target/ARM/ARMBaseInstrInfo.h	(working copy)
@@ -398,6 +398,7 @@
   virtual void expandLoadStackGuard(MachineBasicBlock::iterator MI) const = 0;
 
   void expandMEMCPY(MachineBasicBlock::iterator) const;
+  void expandRESTORESP(MachineBasicBlock::iterator) const;
 
 private:
   /// Modeling special VFP / NEON fp MLA / MLS hazards.
Index: lib/Target/ARM/ARMBaseRegisterInfo.cpp
===================================================================
--- lib/Target/ARM/ARMBaseRegisterInfo.cpp	(revision 359070)
+++ lib/Target/ARM/ARMBaseRegisterInfo.cpp	(working copy)
@@ -90,6 +90,10 @@
       // exception handling.
       return CSR_GenericInt_SaveList;
     }
+  } else if (F.getCallingConv() == CallingConv::V8CC) {
+    return CSR_V8CC_SaveList;
+  } else if (F.getCallingConv() == CallingConv::V8SBCC) {
+    return CSR_V8SBCC_SaveList;
   }
 
   if (STI.getTargetLowering()->supportSwiftError() &&
@@ -124,6 +128,10 @@
   if (CC == CallingConv::GHC)
     // This is academic because all GHC calls are (supposed to be) tail calls
     return CSR_NoRegs_RegMask;
+  if (CC == CallingConv::V8CC)
+    return CSR_V8CC_RegMask;
+  if (CC == CallingConv::V8SBCC)
+    return CSR_V8SBCC_RegMask;
 
   if (STI.getTargetLowering()->supportSwiftError() &&
       MF.getFunction().getAttributes().hasAttrSomewhere(Attribute::SwiftError))
@@ -205,6 +213,19 @@
       if (Reserved.test(*SI))
         markSuperRegs(Reserved, Reg);
 
+  const Function &F = MF.getFunction();
+  if (F.getCallingConv() == CallingConv::V8SBCC) {
+    markSuperRegs(Reserved, ARM::R5);
+    markSuperRegs(Reserved, ARM::R6);
+    markSuperRegs(Reserved, ARM::R7);
+    markSuperRegs(Reserved, ARM::R8);
+    markSuperRegs(Reserved, ARM::R9);
+    markSuperRegs(Reserved, ARM::R10);
+    markSuperRegs(Reserved, ARM::R11);
+  } else if (F.getCallingConv() == CallingConv::V8CC) {
+    markSuperRegs(Reserved, ARM::R10);
+    markSuperRegs(Reserved, ARM::R11);
+  }
   assert(checkAllSuperRegsMarked(Reserved));
   return Reserved;
 }
@@ -862,3 +883,18 @@
   }
   return false;
 }
+
+TargetRegisterInfo::VirtRegToFixSlotMap
+ARMBaseRegisterInfo::getHoistToFixStackSlotMap(MachineFunction &MF) const {
+  const MachineRegisterInfo &MRI = MF.getRegInfo();
+  const ARMFunctionInfo *AFI = MF.getInfo<ARMFunctionInfo>();
+  MachineFrameInfo &MFI = MF.getFrameInfo();
+  VirtRegToFixSlotMap result;
+  for (auto &livein : MRI.liveins()) {
+    if (AFI->isWASM() && (livein.first == ARM::R3)) {
+      result.emplace_back(livein.second,
+                          MFI.CreateFixedSpillStackObject(4, -16));
+    }
+  }
+  return result;
+}
Index: lib/Target/ARM/ARMBaseRegisterInfo.h
===================================================================
--- lib/Target/ARM/ARMBaseRegisterInfo.h	(revision 359070)
+++ lib/Target/ARM/ARMBaseRegisterInfo.h	(working copy)
@@ -210,6 +210,8 @@
                       unsigned DstSubReg,
                       const TargetRegisterClass *NewRC,
                       LiveIntervals &LIS) const override;
+
+  VirtRegToFixSlotMap getHoistToFixStackSlotMap(MachineFunction &) const final;
 };
 
 } // end namespace llvm
Index: lib/Target/ARM/ARMCallingConv.td
===================================================================
--- lib/Target/ARM/ARMCallingConv.td	(revision 359070)
+++ lib/Target/ARM/ARMCallingConv.td	(working copy)
@@ -239,6 +239,61 @@
 ]>;
 
 //===----------------------------------------------------------------------===//
+// V8 Calling Conventions
+//===----------------------------------------------------------------------===//
+
+def CC_ARM_V8: CallingConv<[
+  CCIfType<[i1, i8, i16], CCPromoteToType<i32>>,
+
+  // i64/f64 is passed in even pairs of GPRs
+  // i64 is 8-aligned i32 here, so we may need to eat R1 as a pad register
+  // (and the same is true for f64 if VFP is not enabled)
+  CCIfType<[i32], CCIfAlign<"8", CCAssignToRegWithShadow<[R0, R2], [R0, R1]>>>,
+  CCIfType<[i32], CCIf<"ArgFlags.getOrigAlign() != 8",
+                       CCAssignToReg<[R0, R1, R2, R3, R4, R5, R6, R7, R8, R9, R10, R11]>>>,
+
+  CCIfType<[i32], CCIfAlign<"8", CCAssignToStackWithShadow<4, 8, [R0, R1, R2, R3, R4, R5, R6, R7, R8, R9, R10, R11]>>>,
+  CCIfType<[i32], CCAssignToStackWithShadow<4, 4, [R0, R1, R2, R3, R4, R5, R6, R7, R8, R9, R10, R11]>>,
+  CCIfType<[f32], CCAssignToReg<[S0, S1, S2, S3, S4, S5, S6, S7, S8,
+                                 S9, S10, S11, S12, S13, S14, S15]>>,
+  CCIfType<[f64], CCAssignToReg<[D0, D1, D2, D3, D4, D5, D6, D7]>>
+]>;
+
+def RetCC_ARM_V8: CallingConv<[
+  CCIfType<[i1, i8, i16], CCPromoteToType<i32>>,
+  CCIfType<[i32], CCAssignToReg<[R0, R1, R2, R3]>>,
+  CCIfType<[i64], CCAssignToRegWithShadow<[R0, R2], [R1, R3]>>,
+  CCIfType<[f64], CCAssignToReg<[D0, D1, D2, D3, D4, D5, D6, D7]>>,
+  CCIfType<[f32], CCAssignToReg<[S0, S1, S2, S3, S4, S5, S6, S7, S8,
+                                 S9, S10, S11, S12, S13, S14, S15]>>
+]>;
+
+def CC_ARM_V8SB: CallingConv<[
+  CCIfType<[i1, i8, i16], CCPromoteToType<i32>>,
+
+  // i64/f64 is passed in even pairs of GPRs
+  // i64 is 8-aligned i32 here, so we may need to eat R1 as a pad register
+  // (and the same is true for f64 if VFP is not enabled)
+  CCIfType<[i32], CCIfAlign<"8", CCAssignToRegWithShadow<[R0, R2], [R0, R1]>>>,
+  CCIfType<[i32], CCIf<"ArgFlags.getOrigAlign() != 8",
+                       CCAssignToReg<[R0, R1, R2, R3, R4, R10, R11, R12]>>>,
+
+  CCIfType<[i32], CCIfAlign<"8", CCAssignToStackWithShadow<4, 8, [R0, R1, R2, R3, R4, R10, R11, R12]>>>,
+  CCIfType<[i32], CCAssignToStackWithShadow<4, 4, [R0, R1, R2, R3, R4, R10, R11, R12]>>,
+  CCIfType<[f32], CCAssignToStackWithShadow<4, 4, [Q0, Q1, Q2, Q3]>>,
+  CCIfType<[f64], CCAssignToStackWithShadow<8, 8, [Q0, Q1, Q2, Q3]>>,
+  CCIfType<[v2f64], CCIfAlign<"16",
+           CCAssignToStackWithShadow<16, 16, [Q0, Q1, Q2, Q3]>>>,
+  CCIfType<[v2f64], CCAssignToStackWithShadow<16, 8, [Q0, Q1, Q2, Q3]>>
+]>;
+
+def RetCC_ARM_V8SB: CallingConv<[
+  CCIfType<[i1, i8, i16], CCPromoteToType<i32>>,
+  CCIfType<[i32], CCAssignToReg<[R0, R1, R2, R3]>>,
+  CCIfType<[i64], CCAssignToRegWithShadow<[R0, R2], [R1, R3]>>
+]>;
+
+//===----------------------------------------------------------------------===//
 // Callee-saved register lists.
 //===----------------------------------------------------------------------===//
 
@@ -248,6 +303,11 @@
 def CSR_AAPCS : CalleeSavedRegs<(add LR, R11, R10, R9, R8, R7, R6, R5, R4,
                                      (sequence "D%u", 15, 8))>;
 
+def CSR_V8CC : CalleeSavedRegs<(add LR, R11)>;
+
+def CSR_V8SBCC : CalleeSavedRegs<(add LR, R11, R10, R9, R8, R7, R6, R5,
+                                     (sequence "D%u", 31, 0))>;
+
 // R8 is used to pass swifterror, remove it from CSR.
 def CSR_AAPCS_SwiftError : CalleeSavedRegs<(sub CSR_AAPCS, R8)>;
 
Index: lib/Target/ARM/ARMConstantIslandPass.cpp
===================================================================
--- lib/Target/ARM/ARMConstantIslandPass.cpp	(revision 359070)
+++ lib/Target/ARM/ARMConstantIslandPass.cpp	(working copy)
@@ -241,6 +241,7 @@
     CPEntry *findConstPoolEntry(unsigned CPI, const MachineInstr *CPEMI);
     unsigned getCPELogAlign(const MachineInstr *CPEMI);
     void scanFunctionJumpTables();
+    bool replaceInlineAsm();
     void initializeFunctionInfo(const std::vector<MachineInstr*> &CPEMIs);
     MachineBasicBlock *splitBlockBeforeInstr(MachineInstr *MI);
     void updateForInsertedWaterBlock(MachineBasicBlock *NewBB);
@@ -367,7 +368,7 @@
 
   // Try to reorder and otherwise adjust the block layout to make good use
   // of the TB[BH] instructions.
-  bool MadeChange = false;
+  bool MadeChange = replaceInlineAsm();
   if (GenerateTBB && AdjustJumpTableBlocks) {
     scanFunctionJumpTables();
     MadeChange |= reorderThumb2JumpTables();
@@ -678,6 +679,49 @@
   }
 }
 
+bool ARMConstantIslands::replaceInlineAsm() {
+  bool MadeChange = false;
+  std::vector<MachineInstr *> toRemove;
+  for (MachineBasicBlock &MBB : *MF) {
+    for (MachineInstr &MI : MBB) {
+      if (MI.getOpcode() == TargetOpcode::INLINEASM) {
+        // handle const pool load
+        if (!strcmp(MI.getOperand(0).getSymbolName(), "ldr $0, =${1:c}")) {
+          MachineFunction *MF = MBB.getParent();
+          MachineConstantPool *ConstantPool = MF->getConstantPool();
+          Type *Int32Ty = Type::getInt32Ty(MF->getFunction().getContext());
+          const Constant *C =
+              ConstantInt::get(Int32Ty, MI.getOperand(5).getImm());
+
+          // MachineConstantPool wants an explicit alignment.
+          unsigned Align = MF->getDataLayout().getPrefTypeAlignment(Int32Ty);
+          if (Align == 0)
+            Align = MF->getDataLayout().getTypeAllocSize(C->getType());
+          unsigned Idx = ConstantPool->getConstantPoolIndex(C, Align);
+
+          if (isThumb)
+            BuildMI(MBB, MI, MI.getDebugLoc(), TII->get(ARM::tLDRpci))
+                .addReg(MI.getOperand(3).getReg(), RegState::Define)
+                .addConstantPoolIndex(Idx)
+                .add(predOps(ARMCC::AL));
+          else
+            BuildMI(MBB, MI, MI.getDebugLoc(), TII->get(ARM::LDRcp))
+                .addReg(MI.getOperand(3).getReg(), RegState::Define)
+                .addConstantPoolIndex(Idx)
+                .addImm(0)
+                .add(predOps(ARMCC::AL));
+          toRemove.push_back(&MI);
+          MadeChange = true;
+        }
+      }
+    }
+  }
+  for (MachineInstr *MI : toRemove) {
+    MI->eraseFromParent();
+  }
+  return MadeChange;
+}
+
 /// initializeFunctionInfo - Do the initial scan of the function, building up
 /// information about the sizes of each block, the location of all the water,
 /// and finding all of the constant pool users.
Index: lib/Target/ARM/ARMFastISel.cpp
===================================================================
--- lib/Target/ARM/ARMFastISel.cpp	(revision 359070)
+++ lib/Target/ARM/ARMFastISel.cpp	(working copy)
@@ -1890,6 +1890,10 @@
       report_fatal_error("Can't return in GHC call convention");
     else
       return CC_ARM_APCS_GHC;
+  case CallingConv::V8CC:
+    return (Return ? RetCC_ARM_V8 : CC_ARM_V8);
+  case CallingConv::V8SBCC:
+    return (Return ? RetCC_ARM_V8SB : CC_ARM_V8SB);
   }
 }
 
Index: lib/Target/ARM/ARMFrameLowering.cpp
===================================================================
--- lib/Target/ARM/ARMFrameLowering.cpp	(revision 359070)
+++ lib/Target/ARM/ARMFrameLowering.cpp	(working copy)
@@ -122,6 +122,9 @@
 /// included as part of the stack frame.
 bool ARMFrameLowering::hasReservedCallFrame(const MachineFunction &MF) const {
   const MachineFrameInfo &MFI = MF.getFrameInfo();
+  const ARMFunctionInfo *AFI = MF.getInfo<ARMFunctionInfo>();
+  if (AFI->isJSStub() || AFI->isJSFunction())
+    return false;
   unsigned CFSize = MFI.getMaxCallFrameSize();
   // It's not always a good idea to include the call frame as part of the
   // stack frame. ARM (especially Thumb) has small immediate offset to
@@ -351,9 +354,13 @@
 
 void ARMFrameLowering::emitPrologue(MachineFunction &MF,
                                     MachineBasicBlock &MBB) const {
+  ARMFunctionInfo *AFI = MF.getInfo<ARMFunctionInfo>();
+  if (AFI->isJSStub() || AFI->isJSFunction()) {
+    emitJSPrologue(MF, MBB);
+    return;
+  }
   MachineBasicBlock::iterator MBBI = MBB.begin();
   MachineFrameInfo  &MFI = MF.getFrameInfo();
-  ARMFunctionInfo *AFI = MF.getInfo<ARMFunctionInfo>();
   MachineModuleInfo &MMI = MF.getMMI();
   MCContext &Context = MMI.getContext();
   const TargetMachine &TM = MF.getTarget();
@@ -765,8 +772,12 @@
 
 void ARMFrameLowering::emitEpilogue(MachineFunction &MF,
                                     MachineBasicBlock &MBB) const {
+  ARMFunctionInfo *AFI = MF.getInfo<ARMFunctionInfo>();
+  if (AFI->isJSFunction() || AFI->isJSStub()) {
+    emitJSEpilogue(MF, MBB);
+    return;
+  }
   MachineFrameInfo &MFI = MF.getFrameInfo();
-  ARMFunctionInfo *AFI = MF.getInfo<ARMFunctionInfo>();
   const TargetRegisterInfo *RegInfo = MF.getSubtarget().getRegisterInfo();
   const ARMBaseInstrInfo &TII =
       *static_cast<const ARMBaseInstrInfo *>(MF.getSubtarget().getInstrInfo());
@@ -951,6 +962,9 @@
       // Otherwise, use SP or FP, whichever is closer to the stack slot.
       FrameReg = RegInfo->getFrameRegister(MF);
       return FPOffset;
+    } else if (AFI->isJSStub() || AFI->isJSFunction()) {
+      FrameReg = RegInfo->getFrameRegister(MF);
+      return FPOffset;
     }
   }
   // Use the base pointer if we have one.
@@ -1055,7 +1069,9 @@
   if (MBB.end() != MI) {
     DL = MI->getDebugLoc();
     unsigned RetOpcode = MI->getOpcode();
-    isTailCall = (RetOpcode == ARM::TCRETURNdi || RetOpcode == ARM::TCRETURNri);
+    isTailCall =
+        (RetOpcode == ARM::TCRETURNdi || RetOpcode == ARM::TCRETURNri ||
+         RetOpcode == TargetOpcode::TCPATCHPOINT);
     isInterrupt =
         RetOpcode == ARM::SUBS_PC_LR || RetOpcode == ARM::t2SUBS_PC_LR;
     isTrap =
@@ -1414,6 +1430,47 @@
   std::prev(MI)->addRegisterKilled(ARM::R4, TRI);
 }
 
+static inline bool isV8Area1Register(unsigned Reg, bool isIOS) {
+  using namespace ARM;
+
+  switch (Reg) {
+  case R0:
+  case R1:
+  case R7:
+  case R11:
+  case LR:
+  case SP:
+  case PC:
+    return true;
+  default:
+    return false;
+  }
+}
+
+static inline bool isV8Area2Register(unsigned Reg, bool isIOS) {
+  using namespace ARM;
+  switch (Reg) {
+  case R3:
+  case R12:
+    return true;
+  default:
+    return false;
+  }
+}
+
+static inline bool isV8Area3Register(unsigned Reg, bool isIOS) {
+  using namespace ARM;
+  switch (Reg) {
+  case R5:
+  case R6:
+  case R8:
+  case R9:
+    return true;
+  default:
+    return false;
+  }
+}
+
 bool ARMFrameLowering::spillCalleeSavedRegisters(MachineBasicBlock &MBB,
                                         MachineBasicBlock::iterator MI,
                                         const std::vector<CalleeSavedInfo> &CSI,
@@ -1429,10 +1486,19 @@
     ARM::t2STR_PRE : ARM::STR_PRE_IMM;
   unsigned FltOpc = ARM::VSTMDDB_UPD;
   unsigned NumAlignedDPRCS2Regs = AFI->getNumAlignedDPRCS2Regs();
-  emitPushInst(MBB, MI, CSI, PushOpc, PushOneOpc, false, &isARMArea1Register, 0,
-               MachineInstr::FrameSetup);
-  emitPushInst(MBB, MI, CSI, PushOpc, PushOneOpc, false, &isARMArea2Register, 0,
-               MachineInstr::FrameSetup);
+  if (AFI->isJSStub() || AFI->isJSFunction()) {
+    emitPushInst(MBB, MI, CSI, PushOpc, PushOneOpc, false, &isV8Area1Register,
+                 0, MachineInstr::FrameSetup);
+    emitPushInst(MBB, MI, CSI, PushOpc, PushOneOpc, false, &isV8Area2Register,
+                 0, MachineInstr::FrameSetup);
+    emitPushInst(MBB, MI, CSI, PushOpc, PushOneOpc, false, &isV8Area3Register,
+                 0, MachineInstr::FrameSetup);
+  } else {
+    emitPushInst(MBB, MI, CSI, PushOpc, PushOneOpc, false, &isARMArea1Register,
+                 0, MachineInstr::FrameSetup);
+    emitPushInst(MBB, MI, CSI, PushOpc, PushOneOpc, false, &isARMArea2Register,
+                 0, MachineInstr::FrameSetup);
+  }
   emitPushInst(MBB, MI, CSI, FltOpc, 0, true, &isARMArea3Register,
                NumAlignedDPRCS2Regs, MachineInstr::FrameSetup);
 
@@ -1461,16 +1527,23 @@
   // registers. Do that here instead.
   if (NumAlignedDPRCS2Regs)
     emitAlignedDPRCS2Restores(MBB, MI, NumAlignedDPRCS2Regs, CSI, TRI);
-
   unsigned PopOpc = AFI->isThumbFunction() ? ARM::t2LDMIA_UPD : ARM::LDMIA_UPD;
   unsigned LdrOpc = AFI->isThumbFunction() ? ARM::t2LDR_POST :ARM::LDR_POST_IMM;
   unsigned FltOpc = ARM::VLDMDIA_UPD;
   emitPopInst(MBB, MI, CSI, FltOpc, 0, isVarArg, true, &isARMArea3Register,
               NumAlignedDPRCS2Regs);
-  emitPopInst(MBB, MI, CSI, PopOpc, LdrOpc, isVarArg, false,
-              &isARMArea2Register, 0);
-  emitPopInst(MBB, MI, CSI, PopOpc, LdrOpc, isVarArg, false,
-              &isARMArea1Register, 0);
+  if (AFI->isJSStub() || AFI->isJSFunction()) {
+    emitPopInst(MBB, MI, CSI, PopOpc, LdrOpc, isVarArg, false,
+                &isV8Area3Register, 0);
+    // Just need one more pop as place holder.
+    emitPopInst(MBB, MI, CSI, PopOpc, LdrOpc, isVarArg, false,
+                &isV8Area1Register, 0);
+  } else {
+    emitPopInst(MBB, MI, CSI, PopOpc, LdrOpc, isVarArg, false,
+                &isARMArea2Register, 0);
+    emitPopInst(MBB, MI, CSI, PopOpc, LdrOpc, isVarArg, false,
+                &isARMArea1Register, 0);
+  }
 
   return true;
 }
@@ -1611,6 +1684,21 @@
   MachineRegisterInfo &MRI = MF.getRegInfo();
   const TargetRegisterInfo *TRI = MF.getSubtarget().getRegisterInfo();
   (void)TRI;  // Silence unused warning in non-assert builds.
+
+  if (AFI->isJSFunction() || AFI->isJSStub()) {
+    SavedRegs.set(ARM::R11);
+    SavedRegs.set(ARM::LR);
+    CanEliminateFrame = false;
+  }
+
+  if (AFI->isJSFunction()) {
+    SavedRegs.set(ARM::R0);
+    SavedRegs.set(ARM::R1);
+    SavedRegs.set(ARM::R7);
+  } else if (AFI->isWASM()) {
+    SavedRegs.set(ARM::R3);
+  }
+
   unsigned FramePtr = RegInfo->getFrameRegister(MF);
 
   // Spill R4 if Thumb2 function requires stack realignment - it will be used as
@@ -1783,7 +1871,10 @@
     AFI->setHasStackFrame(true);
 
     if (HasFP) {
-      SavedRegs.set(FramePtr);
+      if (!SavedRegs.test(FramePtr)) {
+        SavedRegs.set(FramePtr);
+        NumGPRSpills++;
+      }
       // If the frame pointer is required by the ABI, also spill LR so that we
       // emit a complete frame record.
       if (MF.getTarget().Options.DisableFramePointerElim(MF) && !LRSpilled) {
@@ -1797,7 +1888,6 @@
       auto FPPos = llvm::find(UnspilledCS1GPRs, FramePtr);
       if (FPPos != UnspilledCS1GPRs.end())
         UnspilledCS1GPRs.erase(FPPos);
-      NumGPRSpills++;
       if (FramePtr == ARM::R7)
         CS1Spilled = true;
     }
@@ -2063,7 +2153,8 @@
       if (Opc == ARM::ADJCALLSTACKDOWN || Opc == ARM::tADJCALLSTACKDOWN) {
         emitSPUpdate(isARM, MBB, I, dl, TII, -Amount, MachineInstr::NoFlags,
                      Pred, PredReg);
-      } else {
+      } else if (!AFI->isJSFunction() && !AFI->isJSStub()) {
+        // JS is callee restore stack.
         assert(Opc == ARM::ADJCALLSTACKUP || Opc == ARM::tADJCALLSTACKUP);
         emitSPUpdate(isARM, MBB, I, dl, TII, Amount, MachineInstr::NoFlags,
                      Pred, PredReg);
@@ -2511,3 +2602,438 @@
   MF.verify();
 #endif
 }
+
+bool ARMFrameLowering::assignCalleeSavedSpillSlots(
+    MachineFunction &MF, const TargetRegisterInfo *TRI,
+    std::vector<CalleeSavedInfo> &CSI) const {
+  ARMFunctionInfo *AFI = MF.getInfo<ARMFunctionInfo>();
+  // default handle.
+  if (!AFI->isJSStub() && !AFI->isJSFunction())
+    return false;
+  MachineFrameInfo &MFI = MF.getFrameInfo();
+  int FixedOffset;
+  if (AFI->isJSStub()) {
+    MFI.CreateFixedSpillStackObject(4, -12);
+    FixedOffset = -16;
+    if (AFI->isWASM()) {
+      // wasm instance for v8.
+      CSI.emplace_back(ARM::R3);
+      CSI.back().setFrameIdx(MFI.CreateFixedSpillStackObject(4, -16));
+      FixedOffset = -20;
+    }
+  } else {
+    int CPFI = MFI.CreateFixedSpillStackObject(4, -12);  // cp
+    int FunFI = MFI.CreateFixedSpillStackObject(4, -16); // function
+    int ArgFI = MFI.CreateFixedSpillStackObject(4, -20); // arg count.
+    CSI.emplace_back(ARM::R7);
+    CSI.back().setFrameIdx(CPFI);
+    CSI.emplace_back(ARM::R1);
+    CSI.back().setFrameIdx(FunFI);
+    CSI.emplace_back(ARM::R0);
+    CSI.back().setFrameIdx(ArgFI);
+    FixedOffset = -24;
+  }
+
+  for (auto &i : CSI) {
+    if (i.getReg() == ARM::LR) {
+      i.setFrameIdx(MFI.CreateFixedSpillStackObject(4, -4));
+    } else if (i.getReg() == ARM::R11) {
+      i.setFrameIdx(MFI.CreateFixedSpillStackObject(4, -8));
+    } else if (i.getReg() >= ARM::R5 && i.getReg() < ARM::R9) {
+      i.setFrameIdx(MFI.CreateFixedSpillStackObject(4, FixedOffset));
+      FixedOffset -= 4;
+    }
+  }
+  for (auto &i : CSI) {
+    if (i.getReg() >= ARM::D0 && i.getReg() <= ARM::D15) {
+      i.setFrameIdx(MFI.CreateFixedSpillStackObject(8, FixedOffset));
+      FixedOffset -= 8;
+    }
+  }
+  return true;
+}
+
+void ARMFrameLowering::emitJSPrologue(MachineFunction &MF,
+                                      MachineBasicBlock &MBB) const {
+  ARMFunctionInfo *AFI = MF.getInfo<ARMFunctionInfo>();
+  MachineBasicBlock::iterator MBBI = MBB.begin();
+  MachineFrameInfo &MFI = MF.getFrameInfo();
+  MachineModuleInfo &MMI = MF.getMMI();
+  MCContext &Context = MMI.getContext();
+  const MCRegisterInfo *MRI = Context.getRegisterInfo();
+  const ARMBaseInstrInfo &TII = *STI.getInstrInfo();
+  bool isARM = !AFI->isThumbFunction();
+  unsigned Align = STI.getFrameLowering()->getStackAlignment();
+  unsigned NumBytes = MFI.getStackSize();
+  const std::vector<CalleeSavedInfo> &CSI = MFI.getCalleeSavedInfo();
+
+  // Debug location must be unknown since the first debug location is used
+  // to determine the end of the prologue.
+  DebugLoc dl;
+
+  unsigned FramePtr = ARM::R11;
+  // Determine the sizes of each callee-save spill areas and record which frame
+  // belongs to which callee-save spill areas.
+  unsigned GPRCS1Size = 0, GPRCS2Size = 0, GPRCS3Size = 0, DPRCSSize = 0;
+  int FramePtrSpillFI = 0;
+  int D8SpillFI = 0;
+
+  if (AFI->isJSStub()) {
+    if (MF.getRegInfo().isLiveIn(ARM::R9)) {
+      MachineBasicBlock &MBB = MF.front();
+      BuildMI(MBB, MBB.instr_begin(), dl, TII.get(ARM::MOVr), ARM::R9)
+          .addReg(ARM::R11)
+          .add(predOps(ARMCC::AL))
+          .add(condCodeOp());
+    }
+    // Push LR as marker.
+    GPRCS1Size = 4;
+  }
+
+  StackAdjustingInsts DefCFAOffsetCandidates;
+  bool HasFP = hasFP(MF);
+
+  // Determine spill area sizes.
+  for (unsigned i = 0, e = CSI.size(); i != e; ++i) {
+    unsigned Reg = CSI[i].getReg();
+    int FI = CSI[i].getFrameIdx();
+    switch (Reg) {
+    case ARM::R3:
+      GPRCS2Size += 4;
+      break;
+    case ARM::R0:
+    case ARM::R1:
+    case ARM::R7:
+    case ARM::R11:
+    case ARM::LR:
+      if (Reg == ARM::R11)
+        FramePtrSpillFI = FI;
+      GPRCS1Size += 4;
+      break;
+    case ARM::R5:
+    case ARM::R6:
+    case ARM::R8:
+    case ARM::R9:
+      GPRCS3Size += 4;
+      break;
+    default:
+      // This is a DPR. Exclude the aligned DPRCS2 spills.
+      if (Reg == ARM::D8)
+        D8SpillFI = FI;
+      if (Reg < ARM::D8 || Reg >= ARM::D8 + AFI->getNumAlignedDPRCS2Regs())
+        DPRCSSize += 8;
+    }
+  }
+
+  // Move past area 1.
+  MachineBasicBlock::iterator LastPush = MBB.end(), GPRCS1Push, GPRCS2Push,
+                              GPRCS3Push;
+  if (GPRCS1Size > 0) {
+    GPRCS1Push = LastPush = MBBI++;
+    DefCFAOffsetCandidates.addInst(LastPush, GPRCS1Size, true);
+  }
+
+  // Determine starting offsets of spill areas.
+  unsigned GPRCS1Offset = NumBytes - GPRCS1Size;
+  unsigned GPRCS2Offset = GPRCS1Offset - GPRCS2Size;
+  unsigned GPRCS3Offset = GPRCS2Offset - GPRCS3Size;
+  unsigned DPRAlign = DPRCSSize ? std::min(8U, Align) : 4U;
+  unsigned DPRGapSize = (GPRCS1Size + GPRCS2Size + GPRCS3Size) % DPRAlign;
+  unsigned DPRCSOffset = GPRCS3Offset - DPRGapSize - DPRCSSize;
+  int FramePtrOffsetInPush = 0;
+  int FPOffset = MFI.getObjectOffset(FramePtrSpillFI);
+  assert(getMaxFPOffset(MF.getFunction(), *AFI) <= FPOffset &&
+         "Max FP estimation is wrong");
+  FramePtrOffsetInPush = FPOffset;
+  AFI->setFramePtrSpillOffset(MFI.getObjectOffset(FramePtrSpillFI) + NumBytes);
+  AFI->setGPRCalleeSavedArea1Offset(GPRCS1Offset);
+  AFI->setGPRCalleeSavedArea2Offset(GPRCS2Offset);
+  AFI->setDPRCalleeSavedAreaOffset(DPRCSOffset);
+
+  // Move past area 2.
+  if (GPRCS2Size > 0) {
+    GPRCS2Push = LastPush = MBBI++;
+    DefCFAOffsetCandidates.addInst(LastPush, GPRCS2Size);
+  }
+
+  if (GPRCS3Size > 0) {
+    GPRCS3Push = LastPush = MBBI++;
+    DefCFAOffsetCandidates.addInst(LastPush, GPRCS3Size);
+  }
+
+  // Prolog/epilog inserter assumes we correctly align DPRs on the stack, so our
+  // .cfi_offset operations will reflect that.
+  if (DPRGapSize) {
+    assert(DPRGapSize == 4 && "unexpected alignment requirements for DPRs");
+    emitSPUpdate(isARM, MBB, MBBI, dl, TII, -DPRGapSize,
+                 MachineInstr::FrameSetup);
+    DefCFAOffsetCandidates.addInst(std::prev(MBBI), DPRGapSize);
+  }
+
+  // Move past area 3.
+  if (DPRCSSize > 0) {
+    // Since vpush register list cannot have gaps, there may be multiple vpush
+    // instructions in the prologue.
+    while (MBBI != MBB.end() && MBBI->getOpcode() == ARM::VSTMDDB_UPD) {
+      DefCFAOffsetCandidates.addInst(MBBI, sizeOfSPAdjustment(*MBBI));
+      LastPush = MBBI++;
+    }
+  }
+
+  // Move past the aligned DPRCS2 area.
+  if (AFI->getNumAlignedDPRCS2Regs() > 0) {
+    MBBI = skipAlignedDPRCS2Spills(MBBI, AFI->getNumAlignedDPRCS2Regs());
+    // The code inserted by emitAlignedDPRCS2Spills realigns the stack, and
+    // leaves the stack pointer pointing to the DPRCS2 area.
+    //
+    // Adjust NumBytes to represent the stack slots below the DPRCS2 area.
+    NumBytes += MFI.getObjectOffset(D8SpillFI);
+  } else
+    NumBytes = DPRCSOffset;
+
+  if (NumBytes) {
+    emitSPUpdate(isARM, MBB, MBBI, dl, TII, -NumBytes,
+                 MachineInstr::FrameSetup);
+    DefCFAOffsetCandidates.addInst(std::prev(MBBI), NumBytes);
+
+    if (HasFP && isARM)
+      // Restore from fp only in ARM mode: e.g. sub sp, r7, #24
+      // Note it's not safe to do this in Thumb2 mode because it would have
+      // taken two instructions:
+      // mov sp, r7
+      // sub sp, #24
+      // If an interrupt is taken between the two instructions, then sp is in
+      // an inconsistent state (pointing to the middle of callee-saved area).
+      // The interrupt handler can end up clobbering the registers.
+      AFI->setShouldRestoreSPFromFP(true);
+  }
+
+  // Set FP to point to the stack slot that contains the previous FP.
+  // For iOS, FP is R7, which has now been stored in spill area 1.
+  // Otherwise, if this is not iOS, all the callee-saved registers go
+  // into spill area 1, including the FP in R11.  In either case, it
+  // is in area one and the adjustment needs to take place just after
+  // that push.
+  MachineBasicBlock::iterator AfterPush = std::next(GPRCS1Push);
+  unsigned PushSize = sizeOfSPAdjustment(*GPRCS1Push);
+  emitRegPlusImmediate(!AFI->isThumbFunction(), MBB, AfterPush, dl, TII,
+                       FramePtr, ARM::SP, PushSize + FramePtrOffsetInPush,
+                       MachineInstr::FrameSetup);
+  if (FramePtrOffsetInPush + PushSize != 0) {
+    unsigned CFIIndex = MF.addFrameInst(MCCFIInstruction::createDefCfa(
+        nullptr, MRI->getDwarfRegNum(FramePtr, true), FramePtrOffsetInPush));
+    BuildMI(MBB, AfterPush, dl, TII.get(TargetOpcode::CFI_INSTRUCTION))
+        .addCFIIndex(CFIIndex)
+        .setMIFlags(MachineInstr::FrameSetup);
+  } else {
+    unsigned CFIIndex = MF.addFrameInst(MCCFIInstruction::createDefCfaRegister(
+        nullptr, MRI->getDwarfRegNum(FramePtr, true)));
+    BuildMI(MBB, AfterPush, dl, TII.get(TargetOpcode::CFI_INSTRUCTION))
+        .addCFIIndex(CFIIndex)
+        .setMIFlags(MachineInstr::FrameSetup);
+  }
+  // Push marker
+  if (AFI->isJSStub()) {
+    int Marker;
+    MF.getFunction()
+        .getFnAttribute("js-stub-call")
+        .getValueAsString()
+        .getAsInteger(10, Marker);
+    BuildMI(MBB, AfterPush, dl, TII.get(ARM::MOVi), ARM::LR)
+        .addImm(Marker)
+        .add(predOps(ARMCC::AL))
+        .add(condCodeOp());
+    BuildMI(MBB, AfterPush, dl, TII.get(ARM::STR_PRE_IMM), ARM::SP)
+        .addReg(ARM::LR, RegState::Kill)
+        .addReg(ARM::SP)
+        .setMIFlags(MachineInstr::NoFlags)
+        .addImm(-4)
+        .add(predOps(ARMCC::AL));
+  }
+
+  // Now that the prologue's actual instructions are finalised, we can insert
+  // the necessary DWARF cf instructions to describe the situation. Start by
+  // recording where each register ended up:
+  if (GPRCS1Size > 0) {
+    MachineBasicBlock::iterator Pos = std::next(GPRCS1Push);
+    int CFIIndex;
+    for (const auto &Entry : CSI) {
+      unsigned Reg = Entry.getReg();
+      int FI = Entry.getFrameIdx();
+      switch (Reg) {
+      case ARM::R0:
+      case ARM::R1:
+      case ARM::R7:
+      case ARM::LR:
+        CFIIndex = MF.addFrameInst(MCCFIInstruction::createOffset(
+            nullptr, MRI->getDwarfRegNum(Reg, true), MFI.getObjectOffset(FI)));
+        BuildMI(MBB, Pos, dl, TII.get(TargetOpcode::CFI_INSTRUCTION))
+            .addCFIIndex(CFIIndex)
+            .setMIFlags(MachineInstr::FrameSetup);
+        break;
+      }
+    }
+  }
+
+  if (GPRCS2Size > 0) {
+    MachineBasicBlock::iterator Pos = std::next(GPRCS2Push);
+    for (const auto &Entry : CSI) {
+      unsigned Reg = Entry.getReg();
+      int FI = Entry.getFrameIdx();
+      switch (Reg) {
+      case ARM::R3:
+        unsigned DwarfReg = MRI->getDwarfRegNum(Reg, true);
+        unsigned Offset = MFI.getObjectOffset(FI);
+        unsigned CFIIndex = MF.addFrameInst(
+            MCCFIInstruction::createOffset(nullptr, DwarfReg, Offset));
+        BuildMI(MBB, Pos, dl, TII.get(TargetOpcode::CFI_INSTRUCTION))
+            .addCFIIndex(CFIIndex)
+            .setMIFlags(MachineInstr::FrameSetup);
+        break;
+      }
+    }
+  }
+
+  if (GPRCS3Size > 0) {
+    MachineBasicBlock::iterator Pos = std::next(GPRCS3Push);
+    for (const auto &Entry : CSI) {
+      unsigned Reg = Entry.getReg();
+      int FI = Entry.getFrameIdx();
+      switch (Reg) {
+      case ARM::R5:
+      case ARM::R6:
+      case ARM::R8:
+      case ARM::R9:
+        unsigned DwarfReg = MRI->getDwarfRegNum(Reg, true);
+        unsigned Offset = MFI.getObjectOffset(FI);
+        unsigned CFIIndex = MF.addFrameInst(
+            MCCFIInstruction::createOffset(nullptr, DwarfReg, Offset));
+        BuildMI(MBB, Pos, dl, TII.get(TargetOpcode::CFI_INSTRUCTION))
+            .addCFIIndex(CFIIndex)
+            .setMIFlags(MachineInstr::FrameSetup);
+        break;
+      }
+    }
+  }
+
+  if (DPRCSSize > 0) {
+    // Since vpush register list cannot have gaps, there may be multiple vpush
+    // instructions in the prologue.
+    MachineBasicBlock::iterator Pos = std::next(LastPush);
+    for (const auto &Entry : CSI) {
+      unsigned Reg = Entry.getReg();
+      int FI = Entry.getFrameIdx();
+      if ((Reg >= ARM::D0 && Reg <= ARM::D31) &&
+          (Reg < ARM::D8 || Reg >= ARM::D8 + AFI->getNumAlignedDPRCS2Regs())) {
+        unsigned DwarfReg = MRI->getDwarfRegNum(Reg, true);
+        unsigned Offset = MFI.getObjectOffset(FI);
+        unsigned CFIIndex = MF.addFrameInst(
+            MCCFIInstruction::createOffset(nullptr, DwarfReg, Offset));
+        BuildMI(MBB, Pos, dl, TII.get(TargetOpcode::CFI_INSTRUCTION))
+            .addCFIIndex(CFIIndex)
+            .setMIFlags(MachineInstr::FrameSetup);
+      }
+    }
+  }
+
+  // Now we can emit descriptions of where the canonical frame address was
+  // throughout the process. If we have a frame pointer, it takes over the job
+  // half-way through, so only the first few .cfi_def_cfa_offset instructions
+  // actually get emitted.
+  DefCFAOffsetCandidates.emitDefCFAOffsets(MBB, dl, TII, HasFP);
+
+  if (STI.isTargetELF() && hasFP(MF))
+    MFI.setOffsetAdjustment(MFI.getOffsetAdjustment() -
+                            AFI->getFramePtrSpillOffset());
+
+  AFI->setGPRCalleeSavedArea1Size(GPRCS1Size);
+  AFI->setGPRCalleeSavedArea2Size(GPRCS2Size);
+  AFI->setGPRCalleeSavedArea3Size(GPRCS3Size);
+  AFI->setDPRCalleeSavedGapSize(DPRGapSize);
+  AFI->setDPRCalleeSavedAreaSize(DPRCSSize);
+
+  assert(!MFI.hasVarSizedObjects());
+}
+
+void ARMFrameLowering::emitJSEpilogue(MachineFunction &MF,
+                                      MachineBasicBlock &MBB) const {
+  ARMFunctionInfo *AFI = MF.getInfo<ARMFunctionInfo>();
+  MachineFrameInfo &MFI = MF.getFrameInfo();
+  const ARMBaseInstrInfo &TII =
+      *static_cast<const ARMBaseInstrInfo *>(MF.getSubtarget().getInstrInfo());
+  assert(!AFI->isThumb1OnlyFunction() &&
+         "This emitEpilogue does not support Thumb1!");
+  bool isARM = !AFI->isThumbFunction();
+  assert(isARM);
+
+  int NumBytes = (int)MFI.getStackSize();
+  unsigned FramePtr = ARM::R11;
+
+  // First put ourselves on the first (from top) terminator instructions.
+  MachineBasicBlock::iterator MBBI = MBB.getFirstTerminator();
+  DebugLoc dl = MBBI != MBB.end() ? MBBI->getDebugLoc() : DebugLoc();
+
+  if (!AFI->hasStackFrame()) {
+    if (NumBytes != 0)
+      emitSPUpdate(isARM, MBB, MBBI, dl, TII, NumBytes);
+  } else {
+    // Unwind MBBI to point to first LDR / VLDRD.
+    if (MBBI != MBB.begin()) {
+      do {
+        --MBBI;
+      } while (MBBI != MBB.begin() && isPopOpcode(MBBI->getOpcode()));
+      if (!isPopOpcode(MBBI->getOpcode()))
+        ++MBBI;
+    }
+
+    // Move SP to start of FP callee save spill area.
+    NumBytes -=
+        (AFI->getGPRCalleeSavedArea1Size() + AFI->getGPRCalleeSavedArea2Size() +
+         AFI->getGPRCalleeSavedArea3Size() + AFI->getDPRCalleeSavedGapSize() +
+         AFI->getDPRCalleeSavedAreaSize());
+
+    if (AFI->getDPRCalleeSavedAreaSize() + AFI->getGPRCalleeSavedArea3Size()) {
+      // Reset SP based on frame pointer only if the stack frame extends beyond
+      // frame pointer stack slot or target is ELF and the function has FP.
+      if (AFI->shouldRestoreSPFromFP()) {
+        NumBytes = AFI->getFramePtrSpillOffset() - NumBytes;
+        if (NumBytes) {
+          emitARMRegPlusImmediate(MBB, MBBI, dl, ARM::SP, FramePtr, -NumBytes,
+                                  ARMCC::AL, 0, TII);
+        } else {
+          // Thumb2 or ARM.
+          BuildMI(MBB, MBBI, dl, TII.get(ARM::MOVr), ARM::SP)
+              .addReg(FramePtr)
+              .add(predOps(ARMCC::AL))
+              .add(condCodeOp());
+        }
+      } else if (NumBytes)
+        emitSPUpdate(isARM, MBB, MBBI, dl, TII, NumBytes);
+    }
+
+    // Increment past our save areas.
+    if (MBBI != MBB.end() && AFI->getDPRCalleeSavedAreaSize()) {
+      MBBI++;
+      // Since vpop register list cannot have gaps, there may be multiple vpop
+      // instructions in the epilogue.
+      while (MBBI != MBB.end() && MBBI->getOpcode() == ARM::VLDMDIA_UPD)
+        MBBI++;
+    }
+    // Skip one more MI when area 3 exists.
+    if (AFI->getGPRCalleeSavedArea3Size()) {
+      MBBI++;
+    }
+
+    BuildMI(MBB, MBBI, dl, TII.get(ARM::MOVr), ARM::SP)
+        .addReg(FramePtr)
+        .add(predOps(ARMCC::AL))
+        .add(condCodeOp());
+    MachineInstrBuilder MIB =
+        BuildMI(MBB, MBBI, dl, TII.get(ARM::LDMIA_UPD), ARM::SP)
+            .addReg(ARM::SP)
+            .add(predOps(ARMCC::AL));
+    MIB.addReg(ARM::R11, getDefRegState(true));
+    MIB.addReg(ARM::LR, getDefRegState(true));
+    MBBI->eraseFromParent();
+  }
+}
Index: lib/Target/ARM/ARMFrameLowering.h
===================================================================
--- lib/Target/ARM/ARMFrameLowering.h	(revision 359070)
+++ lib/Target/ARM/ARMFrameLowering.h	(working copy)
@@ -31,6 +31,8 @@
   /// the function.
   void emitPrologue(MachineFunction &MF, MachineBasicBlock &MBB) const override;
   void emitEpilogue(MachineFunction &MF, MachineBasicBlock &MBB) const override;
+  void emitJSPrologue(MachineFunction &MF, MachineBasicBlock &MBB) const;
+  void emitJSEpilogue(MachineFunction &MF, MachineBasicBlock &MBB) const;
 
   bool spillCalleeSavedRegisters(MachineBasicBlock &MBB,
                                  MachineBasicBlock::iterator MI,
@@ -65,6 +67,11 @@
     return true;
   }
 
+  bool
+  assignCalleeSavedSpillSlots(MachineFunction &MF,
+                              const TargetRegisterInfo *TRI,
+                              std::vector<CalleeSavedInfo> &CSI) const override;
+
 private:
   void emitPushInst(MachineBasicBlock &MBB, MachineBasicBlock::iterator MI,
                     const std::vector<CalleeSavedInfo> &CSI, unsigned StmOpc,
Index: lib/Target/ARM/ARMISelDAGToDAG.cpp
===================================================================
--- lib/Target/ARM/ARMISelDAGToDAG.cpp	(revision 359070)
+++ lib/Target/ARM/ARMISelDAGToDAG.cpp	(working copy)
@@ -13,6 +13,7 @@
 
 #include "ARM.h"
 #include "ARMBaseInstrInfo.h"
+#include "ARMMachineFunctionInfo.h"
 #include "ARMTargetMachine.h"
 #include "MCTargetDesc/ARMAddressingModes.h"
 #include "Utils/ARMBaseInfo.h"
@@ -60,6 +61,15 @@
       : SelectionDAGISel(tm, OptLevel) {}
 
   bool runOnMachineFunction(MachineFunction &MF) override {
+    const Function &F = MF.getFunction();
+    ARMFunctionInfo *AFI = MF.getInfo<ARMFunctionInfo>();
+    if (F.hasFnAttribute("js-function-call"))
+      AFI->setJSFunction(true);
+    else if (F.hasFnAttribute("js-stub-call"))
+      AFI->setJSStub(true);
+    if (F.hasFnAttribute("js-wasm-call"))
+      AFI->setWASM(true);
+
     // Reset the subtarget each time through.
     Subtarget = &MF.getSubtarget<ARMSubtarget>();
     SelectionDAGISel::runOnMachineFunction(MF);
Index: lib/Target/ARM/ARMISelLowering.cpp
===================================================================
--- lib/Target/ARM/ARMISelLowering.cpp	(revision 359070)
+++ lib/Target/ARM/ARMISelLowering.cpp	(working copy)
@@ -1388,6 +1388,7 @@
   case ARMISD::VBICIMM:       return "ARMISD::VBICIMM";
   case ARMISD::VBSL:          return "ARMISD::VBSL";
   case ARMISD::MEMCPY:        return "ARMISD::MEMCPY";
+  case ARMISD::RESTORESP:     return "ARMISD::RESTORESP";
   case ARMISD::VLD1DUP:       return "ARMISD::VLD1DUP";
   case ARMISD::VLD2DUP:       return "ARMISD::VLD2DUP";
   case ARMISD::VLD3DUP:       return "ARMISD::VLD3DUP";
@@ -1601,6 +1602,10 @@
   switch (CC) {
   default:
     report_fatal_error("Unsupported calling convention");
+  case CallingConv::V8CC:
+    return CallingConv::V8CC;
+  case CallingConv::V8SBCC:
+    return CallingConv::V8SBCC;
   case CallingConv::ARM_AAPCS:
   case CallingConv::ARM_APCS:
   case CallingConv::GHC:
@@ -1650,6 +1655,10 @@
   switch (getEffectiveCallingConv(CC, isVarArg)) {
   default:
     report_fatal_error("Unsupported calling convention");
+  case CallingConv::V8SBCC:
+    return (Return ? RetCC_ARM_V8SB : CC_ARM_V8SB);
+  case CallingConv::V8CC:
+    return (Return ? RetCC_ARM_V8 : CC_ARM_V8);
   case CallingConv::ARM_APCS:
     return (Return ? RetCC_ARM_APCS : CC_ARM_APCS);
   case CallingConv::ARM_AAPCS:
@@ -1810,6 +1819,10 @@
   bool isStructRet    = (Outs.empty()) ? false : Outs[0].Flags.isSRet();
   bool isThisReturn   = false;
   bool isSibCall      = false;
+  bool hasJSCCall     = false;
+  CallingConv::ID CallerCC = MF.getFunction().getCallingConv();
+  bool CallerIsJS =
+      ((CallerCC == CallingConv::V8CC) || (CallerCC == CallingConv::V8SBCC));
   auto Attr = MF.getFunction().getFnAttribute("disable-tail-calls");
 
   // Disable tail calls if they're not supported.
@@ -1818,9 +1831,10 @@
 
   if (isTailCall) {
     // Check if it's really possible to do a tail call.
-    isTailCall = IsEligibleForTailCallOptimization(Callee, CallConv,
-                    isVarArg, isStructRet, MF.getFunction().hasStructRetAttr(),
-                                                   Outs, OutVals, Ins, DAG);
+    isTailCall = CallerIsJS || IsEligibleForTailCallOptimization(
+                                   Callee, CallConv, isVarArg, isStructRet,
+                                   MF.getFunction().hasStructRetAttr(), Outs,
+                                   OutVals, Ins, DAG);
     if (!isTailCall && CLI.CS && CLI.CS.isMustTailCall())
       report_fatal_error("failed to perform tail call elimination on a call "
                          "site marked musttail");
@@ -1849,8 +1863,29 @@
   // These operations are automatically eliminated by the prolog/epilog pass
   if (!isSibCall)
     Chain = DAG.getCALLSEQ_START(Chain, NumBytes, 0, dl);
+  if (CLI.CS.isCall()) {
+    if (CallerIsJS && (CLI.CallConv == CallingConv::C)) {
+      hasJSCCall = true;
+    }
+  }
 
-  SDValue StackPtr =
+  SDValue StackPtr;
+  if (hasJSCCall) {
+    SDValue OldStackPtr = DAG.getCopyFromReg(Chain, dl, ARM::SP,
+                                             getPointerTy(DAG.getDataLayout()));
+    Chain = OldStackPtr.getValue(1);
+    SDValue InFlag;
+    SDValue NewStackPtr = DAG.getNode(
+        ISD::AND, dl, OldStackPtr.getValueType(), OldStackPtr.getValue(0),
+        DAG.getConstant(-8, dl, OldStackPtr.getValueType()));
+    Chain =
+        DAG.getCopyToReg(Chain, dl, ARM::SP, NewStackPtr.getValue(0), InFlag);
+    MachineFunction &MF = DAG.getMachineFunction();
+    MachineFrameInfo &MFI = MF.getFrameInfo();
+    MFI.setFrameAddressIsTaken(true);
+  }
+
+  StackPtr =
       DAG.getCopyFromReg(Chain, dl, ARM::SP, getPointerTy(DAG.getDataLayout()));
 
   RegsToPassVector RegsToPass;
@@ -2191,6 +2226,12 @@
 
   // Returns a chain and a flag for retval copy to use.
   Chain = DAG.getNode(CallOpc, dl, NodeTys, Ops);
+
+  if (hasJSCCall) {
+    Chain = DAG.getNode(ARMISD::RESTORESP, dl,
+                        DAG.getVTList(MVT::Other, MVT::Glue), Chain);
+  }
+
   InFlag = Chain.getValue(1);
 
   Chain = DAG.getCALLSEQ_END(Chain, DAG.getIntPtrConstant(NumBytes, dl, true),
@@ -9254,6 +9295,17 @@
     llvm_unreachable("Unexpected instr type to insert");
   }
 
+  case TargetOpcode::STATEPOINT:
+    // As an implementation detail, STATEPOINT shares the STACKMAP format at
+    // this point in the process.  We diverge later.
+    return emitPatchPoint(MI, BB);
+
+  case TargetOpcode::STACKMAP:
+  case TargetOpcode::PATCHPOINT:
+    return emitPatchPoint(MI, BB);
+  case TargetOpcode::TCPATCHPOINT:
+    return TargetLoweringBase::emitPatchPoint(MI, BB);
+
   // Thumb1 post-indexed loads are really just single-register LDMs.
   case ARM::tLDR_postidx: {
     MachineOperand Def(MI.getOperand(1));
@@ -10961,6 +11013,10 @@
   unsigned Mask = MaskC->getZExtValue();
   if (Mask == 0xffff)
     return SDValue();
+  // Memory Operator can address constant, save one register.
+  auto UI = dyn_cast<MemSDNode>(*(N->use_begin()));
+  if (UI)
+    return SDValue();
   SDValue Res;
   // Case (1): or (and A, mask), val => ARMbfi A, val, mask
   ConstantSDNode *N1C = dyn_cast<ConstantSDNode>(N1);
@@ -13136,6 +13192,23 @@
   return -1;
 }
 
+const MCPhysReg *
+ARMTargetLowering::getScratchRegisters(CallingConv::ID CC) const {
+  if (CC == CallingConv::V8SBCC)
+    return nullptr;
+  static const MCPhysReg ScratchRegs[] = {ARM::R12, 0};
+  return ScratchRegs;
+}
+
+MachineBasicBlock *
+ARMTargetLowering::emitPatchPoint(MachineInstr &MI,
+                                  MachineBasicBlock *MBB) const {
+  MachineBasicBlock *MBB2 = TargetLoweringBase::emitPatchPoint(MI, MBB);
+  MachineFunction &MF = *MI.getMF();
+  MI.addOperand(MF, MachineOperand::CreateReg(ARM::LR, true, true));
+  return MBB2;
+}
+
 static bool isLegalT1AddressImmediate(int64_t V, EVT VT) {
   if (V < 0)
     return false;
Index: lib/Target/ARM/ARMISelLowering.h
===================================================================
--- lib/Target/ARM/ARMISelLowering.h	(revision 359070)
+++ lib/Target/ARM/ARMISelLowering.h	(working copy)
@@ -233,6 +233,8 @@
       // Pseudo-instruction representing a memory copy using ldm/stm
       // instructions.
       MEMCPY,
+      // Pseudo-instruction representing restoring sp from fp.
+      RESTORESP,
 
       // Vector load N-element structure to all lanes:
       VLD1DUP = ISD::FIRST_TARGET_MEMORY_OPCODE,
@@ -501,6 +503,11 @@
     bool functionArgumentNeedsConsecutiveRegisters(
         Type *Ty, CallingConv::ID CallConv, bool isVarArg) const override;
 
+    const MCPhysReg *getScratchRegisters(CallingConv::ID CC) const override;
+
+    MachineBasicBlock *emitPatchPoint(MachineInstr &MI,
+                                      MachineBasicBlock *MBB) const;
+
     /// If a physical register, this returns the register that receives the
     /// exception address on entry to an EH pad.
     unsigned
Index: lib/Target/ARM/ARMInstrInfo.td
===================================================================
--- lib/Target/ARM/ARMInstrInfo.td	(revision 359070)
+++ lib/Target/ARM/ARMInstrInfo.td	(working copy)
@@ -79,6 +79,7 @@
 def SDT_ARMMEMCPY  : SDTypeProfile<2, 3, [SDTCisVT<0, i32>, SDTCisVT<1, i32>,
                                           SDTCisVT<2, i32>, SDTCisVT<3, i32>,
                                           SDTCisVT<4, i32>]>;
+def SDT_ARMRESTORESP: SDTypeProfile<0, 0, []>;
 
 def SDTBinaryArithWithFlags : SDTypeProfile<2, 2,
                                             [SDTCisSameAs<0, 2>,
@@ -207,6 +208,10 @@
                         [SDNPHasChain, SDNPInGlue, SDNPOutGlue,
                          SDNPMayStore, SDNPMayLoad]>;
 
+def ARMrestoresp : SDNode<"ARMISD::RESTORESP", SDT_ARMRESTORESP,
+                        [SDNPHasChain, SDNPOutGlue,
+                         SDNPMayStore, SDNPMayLoad]>;
+
 def ARMsmulwb       : SDNode<"ARMISD::SMULWB", SDTIntBinOp, []>;
 def ARMsmulwt       : SDNode<"ARMISD::SMULWT", SDTIntBinOp, []>;
 def ARMsmlalbb      : SDNode<"ARMISD::SMLALBB", SDT_LongMac, []>;
@@ -4931,6 +4936,14 @@
             (ARMmemcopy GPR:$dst, GPR:$src, imm:$nreg))]>;
 }
 
+let hasPostISelHook = 1, hasNoSchedulingInfo = 1 in {
+    def RESTORESP : PseudoInst<
+      (outs),
+      (ins),
+      NoItinerary,
+      [(ARMrestoresp)]>;
+}
+
 def ldrex_1 : PatFrag<(ops node:$ptr), (int_arm_ldrex node:$ptr), [{
   return cast<MemIntrinsicSDNode>(N)->getMemoryVT() == MVT::i8;
 }]>;
Index: lib/Target/ARM/ARMMachineFunctionInfo.h
===================================================================
--- lib/Target/ARM/ARMMachineFunctionInfo.h	(revision 359070)
+++ lib/Target/ARM/ARMMachineFunctionInfo.h	(working copy)
@@ -85,6 +85,7 @@
   /// areas.
   unsigned GPRCS1Size = 0;
   unsigned GPRCS2Size = 0;
+  unsigned GPRCS3Size = 0;
   unsigned DPRCSAlignGapSize = 0;
   unsigned DPRCSSize = 0;
 
@@ -127,6 +128,10 @@
   /// The amount the literal pool has been increasedby due to promoted globals.
   int PromotedGlobalsIncrease = 0;
 
+  bool IsJSFunction = false;
+  bool IsJSStub = false;
+  bool IsWASM = false;
+
 public:
   ARMFunctionInfo() = default;
 
@@ -170,11 +175,13 @@
 
   unsigned getGPRCalleeSavedArea1Size() const { return GPRCS1Size; }
   unsigned getGPRCalleeSavedArea2Size() const { return GPRCS2Size; }
+  unsigned getGPRCalleeSavedArea3Size() const { return GPRCS3Size; }
   unsigned getDPRCalleeSavedGapSize() const   { return DPRCSAlignGapSize; }
   unsigned getDPRCalleeSavedAreaSize()  const { return DPRCSSize; }
 
   void setGPRCalleeSavedArea1Size(unsigned s) { GPRCS1Size = s; }
   void setGPRCalleeSavedArea2Size(unsigned s) { GPRCS2Size = s; }
+  void setGPRCalleeSavedArea3Size(unsigned s) { GPRCS3Size = s; }
   void setDPRCalleeSavedGapSize(unsigned s)   { DPRCSAlignGapSize = s; }
   void setDPRCalleeSavedAreaSize(unsigned s)  { DPRCSSize = s; }
 
@@ -239,6 +246,13 @@
   void setPromotedConstpoolIncrease(int Sz) {
     PromotedGlobalsIncrease = Sz;
   }
+  bool isJSFunction() const { return IsJSFunction; }
+  bool isJSStub() const { return IsJSStub; }
+  bool isWASM() const { return IsWASM; }
+
+  void setJSFunction(bool s) { IsJSFunction = s; }
+  void setJSStub(bool s) { IsJSStub = s; }
+  void setWASM(bool s) { IsWASM = s; }
 };
 
 } // end namespace llvm
Index: lib/Target/Target.cpp
===================================================================
--- lib/Target/Target.cpp	(revision 359070)
+++ lib/Target/Target.cpp	(working copy)
@@ -66,6 +66,12 @@
   unwrap(PM)->add(new TargetLibraryInfoWrapperPass(*unwrap(TLI)));
 }
 
+LLVMTargetLibraryInfoRef LLVMCreateEmptyTargetLibraryInfo() {
+  TargetLibraryInfoImpl *TLII = new TargetLibraryInfoImpl();
+  TLII->disableAllFunctions();
+  return wrap(TLII);
+}
+
 char *LLVMCopyStringRepOfTargetData(LLVMTargetDataRef TD) {
   std::string StringRep = unwrap(TD)->getStringRepresentation();
   return strdup(StringRep.c_str());
Index: lib/Transforms/IPO/PassManagerBuilder.cpp
===================================================================
--- lib/Transforms/IPO/PassManagerBuilder.cpp	(revision 359070)
+++ lib/Transforms/IPO/PassManagerBuilder.cpp	(working copy)
@@ -34,6 +34,7 @@
 #include "llvm/Transforms/IPO/ForceFunctionAttrs.h"
 #include "llvm/Transforms/IPO/FunctionAttrs.h"
 #include "llvm/Transforms/IPO/InferFunctionAttrs.h"
+#include "llvm/Transforms/IPO/SampleProfile.h"
 #include "llvm/Transforms/InstCombine/InstCombine.h"
 #include "llvm/Transforms/Instrumentation.h"
 #include "llvm/Transforms/Scalar.h"
@@ -273,6 +274,8 @@
 
 // Do PGO instrumentation generation or use pass as the option specified.
 void PassManagerBuilder::addPGOInstrPasses(legacy::PassManagerBase &MPM) {
+  if (PGOSampleUse.empty())
+    PGOSampleUse = SampleProfileLoaderPass::SampleProfileFileFromOption();
   if (!EnablePGOInstrGen && PGOInstrUse.empty() && PGOSampleUse.empty())
     return;
   // Perform the preinline and cleanup passes for O1 and above.
@@ -425,6 +428,8 @@
 
 void PassManagerBuilder::populateModulePassManager(
     legacy::PassManagerBase &MPM) {
+  if (PGOSampleUse.empty())
+    PGOSampleUse = SampleProfileLoaderPass::SampleProfileFileFromOption();
   if (!PGOSampleUse.empty()) {
     MPM.add(createPruneEHPass());
     MPM.add(createSampleProfileLoaderPass(PGOSampleUse));
@@ -755,6 +760,8 @@
 }
 
 void PassManagerBuilder::addLTOOptimizationPasses(legacy::PassManagerBase &PM) {
+  if (PGOSampleUse.empty())
+    PGOSampleUse = SampleProfileLoaderPass::SampleProfileFileFromOption();
   // Load sample profile before running the LTO optimization pipeline.
   if (!PGOSampleUse.empty()) {
     PM.add(createPruneEHPass());
Index: lib/Transforms/IPO/SampleProfile.cpp
===================================================================
--- lib/Transforms/IPO/SampleProfile.cpp	(revision 359070)
+++ lib/Transforms/IPO/SampleProfile.cpp	(working copy)
@@ -1669,3 +1669,7 @@
 
   return PreservedAnalyses::none();
 }
+
+std::string SampleProfileLoaderPass::SampleProfileFileFromOption() {
+  return SampleProfileFile;
+}
Index: tools/llvm-shlib/CMakeLists.txt
===================================================================
--- tools/llvm-shlib/CMakeLists.txt	(revision 359070)
+++ tools/llvm-shlib/CMakeLists.txt	(working copy)
@@ -64,6 +64,7 @@
   endif()
 
   target_link_libraries(LLVM PRIVATE ${LIB_NAMES})
+  set_target_properties(LLVM PROPERTIES LINK_FLAGS "-static-libstdc++")
 
   if (APPLE)
     set_property(TARGET LLVM APPEND_STRING PROPERTY
Index: tools/llvm-shlib/simple_version_script.map.in
===================================================================
--- tools/llvm-shlib/simple_version_script.map.in	(revision 359070)
+++ tools/llvm-shlib/simple_version_script.map.in	(working copy)
@@ -1 +1 @@
-LLVM_@LLVM_VERSION_MAJOR@ { global: *; };
+LLVM_@LLVM_VERSION_MAJOR@ { global: LLVM*; __jit_debug_descriptor; local:*; };
